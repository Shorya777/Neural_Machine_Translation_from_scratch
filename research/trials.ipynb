{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d parvmodi/english-to-hindi-machine-translation-dataset"
      ],
      "metadata": {
        "id": "wg510zXyTcgU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8100ccaf-dae4-4e0a-dff7-152cc5b5ae03"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/parvmodi/english-to-hindi-machine-translation-dataset\n",
            "License(s): CC0-1.0\n",
            "Downloading english-to-hindi-machine-translation-dataset.zip to /content\n",
            "100% 1.04G/1.04G [00:29<00:00, 42.4MB/s]\n",
            "100% 1.04G/1.04G [00:29<00:00, 37.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip english-to-hindi-machine-translation-dataset.zip"
      ],
      "metadata": {
        "id": "5d8aXkDvTwHb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c745be7b-eca1-4a47-d6b8-0e56c4880446"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  english-to-hindi-machine-translation-dataset.zip\n",
            "  inflating: train.en                \n",
            "  inflating: train.hi                \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-lrXpDF5kgx",
        "outputId": "799fdad8-1b19-4e3d-a23e-a914cd1bbb9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10125706\n",
            "[\"However, Paes, who was partnering Australia's Paul Hanley, could only go as far as the quarterfinals where they lost to Bhupathi and Knowles\\n\", 'Whosoever desires the reward of the world, with Allah is the reward of the world and of the Everlasting Life. Allah is the Hearer, the Seer.\\n', 'The value of insects in the biosphere is enormous because they outnumber all other living groups in measure of species richness.\\n', 'Mithali To Anchor Indian Team Against Australia in ODIs\\n', 'After the assent of the Honble President on 8thSeptember, 2016, the 101thConstitutional Amendment Act, 2016 came into existence\\n']\n",
            "10125706\n",
            "['आस्ट्रेलिया के पाल हेनली के साथ जोड़ी बनाने वाले पेस मियामी में क्वार्टरफाइनल तक ही पहुंच सके क्योंकि इस दौर में उन्हें भूपति और नोल्स ने हराया था।\\n', 'और जो शख्स (अपने आमाल का) बदला दुनिया ही में चाहता है तो ख़ुदा के पास दुनिया व आख़िरत दोनों का अज्र मौजूद है और ख़ुदा तो हर शख्स की सुनता और सबको देखता है\\n', 'जैव-मंडल में कीड़ों का मूल्य बहुत है, क्योंकि प्रजातियों की समृद्धि के मामले में उनकी संख्या अन्य जीव समूहों से ज़्यादा है।\\n', 'आस्ट्रेलिया के खिलाफ वनडे टीम की कमान मिताली को\\n', '8 सितम्\\u200dबर, 2016 को माननीय राष्\\u200dट्रपति की स्\\u200dवीकृति मिलने के बाद 101वां संविधान संशोधन अधिनियम, 2016 अस्तित्\\u200dव में आया\\n']\n"
          ]
        }
      ],
      "source": [
        "filepath_en = 'train.en'\n",
        "filepath_hi = 'train.hi'\n",
        "\n",
        "with open(filepath_en, encoding='utf-8') as file:\n",
        "    lines_en = file.readlines()\n",
        "print(len(lines_en))\n",
        "print(lines_en[:5])\n",
        "\n",
        "with open(filepath_hi, encoding='utf-8') as file:\n",
        "    lines_hi = file.readlines()\n",
        "\n",
        "print(len(lines_hi))\n",
        "print(lines_hi[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUOig1EV5kgx",
        "outputId": "95c6d392-cd6a-493c-d25e-59bc0076c700"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"however, paes, who was partnering australia's paul hanley, could only go as far as the quarterfinals where they lost to bhupathi and knowles\", 'whosoever desires the reward of the world, with allah is the reward of the world and of the everlasting life. allah is the hearer, the seer.', 'the value of insects in the biosphere is enormous because they outnumber all other living groups in measure of species richness.', 'mithali to anchor indian team against australia in odis', 'after the assent of the honble president on 8thseptember, 2016, the 101thconstitutional amendment act, 2016 came into existence']\n",
            "================================\n",
            "['आस्ट्रेलिया के पाल हेनली के साथ जोड़ी बनाने वाले पेस मियामी में क्वार्टरफाइनल तक ही पहुंच सके क्योंकि इस दौर में उन्हें भूपति और नोल्स ने हराया था।', 'और जो शख्स (अपने आमाल का) बदला दुनिया ही में चाहता है तो ख़ुदा के पास दुनिया व आख़िरत दोनों का अज्र मौजूद है और ख़ुदा तो हर शख्स की सुनता और सबको देखता है', 'जैव-मंडल में कीड़ों का मूल्य बहुत है, क्योंकि प्रजातियों की समृद्धि के मामले में उनकी संख्या अन्य जीव समूहों से ज़्यादा है।', 'आस्ट्रेलिया के खिलाफ वनडे टीम की कमान मिताली को', '8 सितम्\\u200dबर, 2016 को माननीय राष्\\u200dट्रपति की स्\\u200dवीकृति मिलने के बाद 101वां संविधान संशोधन अधिनियम, 2016 अस्तित्\\u200dव में आया']\n"
          ]
        }
      ],
      "source": [
        "TOTAL_SENTENCES = 200000\n",
        "english_sentences = lines_en[:TOTAL_SENTENCES]\n",
        "hindi_sentences = lines_hi[:TOTAL_SENTENCES]\n",
        "english_sentences = [sentence.rstrip('\\n').lower() for sentence in english_sentences]\n",
        "hindi_sentences = [sentence.rstrip('\\n') for sentence in hindi_sentences]\n",
        "\n",
        "print(english_sentences[:5])\n",
        "print(\"================================\")\n",
        "print(hindi_sentences[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OOIXt1O_BAv",
        "outputId": "e0d0f9d2-c0f6-485b-caf8-bafb6c77647a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "97th percentile length Kannada: 258.0\n",
            "97th percentile length English: 267.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "PERCENTILE = 97\n",
        "print( f\"{PERCENTILE}th percentile length Kannada: {np.percentile([len(x) for x in hindi_sentences], PERCENTILE)}\" )\n",
        "print( f\"{PERCENTILE}th percentile length English: {np.percentile([len(x) for x in english_sentences], PERCENTILE)}\" )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "drEPgCVEDAMJ"
      },
      "outputs": [],
      "source": [
        "START_TOKEN = ''\n",
        "PADDING_TOKEN = ''\n",
        "END_TOKEN = ''\n",
        "\n",
        "english_vocabulary = [START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/',\n",
        "                        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "                        ':', '<', '=', '>', '?', '@',\n",
        "                        '[', '\\' ', ']', '^', '_', '`',\n",
        "                        'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
        "                        'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x',\n",
        "                        'y', 'z',\n",
        "                        '{', '|', '}', '~', PADDING_TOKEN, END_TOKEN]\n",
        "\n",
        "hindi_vocabulary = [START_TOKEN, 'अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ए', 'ऐ', 'ओ', 'औ',\n",
        "    'क', 'ख', 'ग', 'घ', 'ङ','ड़', 'च', 'छ', 'ज', 'झ', 'ञ',\n",
        "    'ट', 'ठ', 'ड', 'ढ', 'ण', 'त', 'थ', 'द', 'ध', 'न',\n",
        "    'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ल', 'व', 'श',\n",
        "    'ष', 'स', 'ह', '़', 'ा', 'ि', 'ी', 'ु', 'ू', 'े', 'ै', 'ो', 'ौ',\n",
        "    'ं', 'ः', 'ँ', '्', '०', '१', '२', '३', '४', '५', '६', '७', '८', '९', '|',\n",
        "    ':', '<', '=', '>', '?', '@',\n",
        "    '[', '\\' ', ']', '^', '_', '`',\n",
        "    ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/',\n",
        "    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9','\\u200d','।', 'ृ',\n",
        "    PADDING_TOKEN, END_TOKEN\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qpZWhY7AA5K",
        "outputId": "07a5afc1-1f2a-4bbd-9fa2-dd787d6ddb4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences: 200000\n",
            "Number of valid sentences: 151260\n"
          ]
        }
      ],
      "source": [
        "max_sequence_length = 250\n",
        "\n",
        "def is_valid_tokens(sentence, vocab):\n",
        "    for token in list(set(sentence)):\n",
        "        if token not in vocab:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def is_valid_length(sentence, max_sequence_length):\n",
        "    return len(list(sentence)) < (max_sequence_length - 1) # need to re-add the end token so leaving 1 space\n",
        "\n",
        "valid_sentence_indicies = []\n",
        "for index in range(len(hindi_sentences)):\n",
        "    hindi_sentence, english_sentence = hindi_sentences[index], english_sentences[index]\n",
        "    if is_valid_length(hindi_sentence, max_sequence_length) \\\n",
        "      and is_valid_length(english_sentence, max_sequence_length) \\\n",
        "      and is_valid_tokens(hindi_sentence, hindi_vocabulary):\n",
        "        valid_sentence_indicies.append(index)\n",
        "\n",
        "print(f\"Number of sentences: {len(hindi_sentences)}\")\n",
        "print(f\"Number of valid sentences: {len(valid_sentence_indicies)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMN2l2qPHqq6",
        "outputId": "16adbe59-1db6-40e1-8e26-c1f0a5506ec7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Characters in the sentences not in the vocabulary: {'ត', 'B', 'Р', '°', 'F', 'Z', '○', '\\ue205', '章', 'ғ', '春', 'к', 'М', 'า', '੍', '夕', 'ড', 'ු', '׃', '“', '§', 'ல', 'Ⓡ', 'ె', 'ē', 'À', 'ன', 'E', 'Ň', 'ɑ', 'ر', '»', 'ಯ', '‘', 'ল', 'ॐ', 'ν', '਼', 'd', 'п', 'е', '́', 'Ę', 'ז', 's', 'ï', 'ֹ', 'χ', 'ล', 'แ', 'ர', '→', 'я', 'ס', 'デ', 'ං', 'Ō', 'p', 'ৱ', '”', 'ุ', 'n', '\\uf0d8', 'ế', 'g', '\\xa0', 'ා', '現', 'b', 'コ', 'ː', 'ස', 'ज़', 'レ', '‚', '▪', 'τ', 'ำ', 'ុ', '郑', 'ی', 'ช', 'л', '≈', 'α', 'ಳ', 'ع', 'য', 'כ', '女', 'J', ';', 'ි', 'ŋ', 'É', 'ל', 'ท', '日', '鲁', 'ಮ', 'স', 'Я', 'ַ', 'ঔ', 'i', 'ల', 'מ', '¨', 'K', 'м', 'ී', '\\\\', '॥', 'ئ', '\\xad', 'Ṭ', '庭', 'א', '\\uf0a7', 'פ', 'm', 'Э', 'с', 'ĺ', '‑', '▫', 'ট', '林', 'ய', 'ऩ', '¼', 'ॊ', 'េ', 'ス', '班', 'ு', 'ɪ', 'క', 'ก', '世', 'ə', 'ᠠ', 'Ż', 'ā', 'ؤ', 'ɵ', '七', 'Ž', 'إ', '☞', '唐', '自', '{', 'С', '動', 'ऽ', 'গ', 'ง', '車', 'ৰ', 'U', 'ে', '♪', 'ූ', '◆', 'ಚ', 'æ', 'প', 'ص', 'ֵ', 'ව', 'ィ', 'λ', '’', '\\u200b', 'Þ', '长', 'ক', 'ా', 'ী', 'ң', 'ো', 'υ', 'ऑ', 'ī', 'R', 'ہ', 'ᡠ', 'ę', 'Ю', 'ä', 'ᠩ', 'ದ', 'র', 'শ', '£', 'ල', 'ச', 'r', '明', 'ா', 'þ', 'ָ', 'ම', '․', 'ˌ', 'D', 'ඩ', 'パ', 'य़', 'נ', '\\uf514', 'ನ', 'ோ', 'â', 'ห', 'ප', 'ί', 'ţ', 'ਵ', 'ऱ', 'ර', 'উ', 'Â', 'ऎ', 'ও', 'ळ', 'గ', '్', 'ಕ', 'р', 'ు', 'మ', 'ִ', 'أ', 'ॢ', '̈', 'த', 'ú', '̯', '\\uf146', 'ּ', '☺', 'Ô', 'ऍ', 'ǔ', 'u', 'q', '\\ue5a0', 'চ', 'ி', 'ε', '骨', 'A', '₹', '\\ue208', 'Ò', '\\u200f', '⊆', 'আ', 'M', 'ª', '\\uf0b7', 'Å', 'ᐃ', '域', 'ר', '휴', '′', 'ග', 'Ω', '記', 'ম', '්', '„', 'க', 'ظ', 'k', 'ḷ', 'ש', '魯', 'ح', '관', 'फ़', 'К', 'ě', 'ᠨ', 'Τ', '€', 'ਆ', '়', 'ט', 'Q', 'ˈ', 'o', '死', '}', 'ミ', 'ឃ', 'ό', 'e', 'é', 'ب', 'อ', 'ᒡ', 'ḍ', '−', 'v', 'ι', 'й', 'д', '일', 'ਰ', 'Ê', '經', 'κ', 'ʁ', '្', 'ה', 'ق', '×', '–', 'ø', 'È', 'ς', 'ل', 'G', 'y', 'ñ', 'ை', 'X', 'ত', '\\u200c', '❏', 'ය', '州', 'ॄ', '·', 'ב', 'ʔ', 'и', '⋅', 'ต', '\\u2061', 'a', 'í', 'ّ', 'ا', '\\ufeff', '้', 'ೆ', 'ض', 'ة', 'Ó', 'l', 'រ', 'د', 'ן', 'ن', '\\uf642', 'ঘ', 'ي', 'z', 'َ', '்', '经', '■', 'ਤ', 'т', 'S', '►', 'ρ', '⁰', '‐', 'ه', 'ਿ', '—', 'ô', '¬', 'ং', 'ᐅ', 'া', '공', 'c', '□', 'ម', 'μ', 'N', 'ش', 'ò', '의', 'ו', 'à', 'ਦ', 'پ', 'ك', '›', 'о', '…', 'j', 'ξ', 'త', 'ऒ', 'ᠴ', 'ක', 'Ć', 'Y', 'ء', 'ॅ', 'ন', 'W', '◇', 'η', '℃', '®', '高', 'T', 'م', 'ย', 'ො', 'メ', 'ʹ', 'н', 'ក', 'Ч', 'ч', '¡', 'I', 'x', 'á', 'י', 'ว', 'ط', 'வ', 'क़', 'P', '্', 'ó', 'ॆ', '˜', 'Δ', 'و', '♦', 'ம', 'H', '❑', 'è', 'হ', 'ê', '่', 'ă', 'ই', 'Ι', 'න', '♫', 'ロ', 'ᓗ', 'š', 'ढ़', 'ग़', '洞', 'ু', 'ş', 'ి', 'ਸ', 'ත', 'ш', 'ॠ', 'ʊ', 't', '½', '\\u2008', 'ɨ', 'آ', 'V', 'в', 'ស', '¸', '˝', 'ン', 'Г', 'ٓ', '國', 'Ş', 'а', 'พ', 'f', '²', '´', 'ॉ', 'δ', '~', 'জ', 'ঙ', '±', '್', 'দ', 'ُ', '\\u200e', 'у', 'β', 'ְ', '大', 'ន', 'ٕ', '性', '॰', '☻', '中', '\\ue88c', 'ऌ', '¹', '॒', '™', '¶', '•', 'ಿ', '⁻', 'ֶ', '에', 'ʃ', '↔', '木', '⚡', 'ண', 'ف', 'θ', 'h', 'ব', 'ច', 'ِ', '한', 'ö', 'น', 'ப', '©', 'ت', 'ٰ', 'Ÿ', 'ি', 'ק', 'ー', 'ز', 'O', 'L', 'б', '西', 'ο', 'ы', 'ิ', 'w', 'C', 'ł', 'ō', '湖', '代', 'Θ', 'з', 'س', 'ख़'}\n"
          ]
        }
      ],
      "source": [
        "# for i in 'शख्स':\n",
        "  # print(i)\n",
        "\n",
        "\n",
        "# hindi_sentences = [\n",
        "#     'आस्ट्रेलिया के पाल हेनली के साथ जोड़ी बनाने वाले पेस मियामी में क्वार्टरफाइनल तक ही पहुंच सके क्योंकि इस दौर में उन्हें भूपति और नोल्स ने हराया था।',\n",
        "#     'और जो शख्स (अपने आमाल का) बदला दुनिया ही में चाहता है तो ख़ुदा के पास दुनिया व आख़िरत दोनों का अज्र मौजूद है और ख़ुदा तो हर शख्स की सुनता और सबको देखता है',\n",
        "#     'जैव-मंडल में कीड़ों का मूल्य बहुत है, क्योंकि प्रजातियों की समृद्धि के मामले में उनकी संख्या अन्य जीव समूहों से ज़्यादा है।',\n",
        "#     'आस्ट्रेलिया के खिलाफ वनडे टीम की कमान मिताली को',\n",
        "#     '8 सितम्‍बर, 2016 को माननीय राष्ट्रपति की स्‍वीकृति मिलने के बाद 101वां संविधान संशोधन अधिनियम, 2016 अस्तित्व में आया'\n",
        "# ]\n",
        "\n",
        "# Define the provided vocabulary set\n",
        "hindi_vocabulary_set = set(hindi_vocabulary)\n",
        "\n",
        "# Combine all sentences into one string\n",
        "combined_sentences = ''.join(hindi_sentences)\n",
        "\n",
        "# Find all unique characters in the combined sentences\n",
        "unique_chars_in_sentences = set(combined_sentences)\n",
        "\n",
        "# Find characters in the sentences that are not in the vocabulary\n",
        "missing_chars = unique_chars_in_sentences - hindi_vocabulary_set\n",
        "\n",
        "print(\"Characters in the sentences not in the vocabulary:\", missing_chars)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "dXmG9YJcTN3d"
      },
      "outputs": [],
      "source": [
        "index_to_hindi = {k:v for k,v in enumerate(hindi_vocabulary)}\n",
        "hindi_to_index = {v:k for k,v in enumerate(hindi_vocabulary)}\n",
        "index_to_english = {k:v for k,v in enumerate(english_vocabulary)}\n",
        "english_to_index = {v:k for k,v in enumerate(english_vocabulary)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "rdyjQgdKTN3d"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, Dropout\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, max_seq_len):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "    def call(self, inputs):\n",
        "        pos = np.arange(self.max_seq_len)[:, np.newaxis]\n",
        "        i = np.arange(self.d_model)[np.newaxis, :]\n",
        "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(self.d_model))\n",
        "        angle_rads = pos * angle_rates\n",
        "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "        pos_encoding = angle_rads[np.newaxis, ...]\n",
        "        return tf.constant(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "class SentenceEmbedding(tf.keras.Model):\n",
        "    def __init__(self, max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n",
        "        super(SentenceEmbedding, self).__init__()\n",
        "        self.vocab_size = len(language_to_index)\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.embedding = Embedding(input_dim=self.vocab_size, output_dim=d_model)\n",
        "        self.language_to_index = language_to_index\n",
        "        self.position_encoder = PositionalEncoding(d_model, max_sequence_length)\n",
        "        self.dropout = Dropout(0.1)\n",
        "        self.START_TOKEN = START_TOKEN\n",
        "        self.END_TOKEN = END_TOKEN\n",
        "        self.PADDING_TOKEN = PADDING_TOKEN\n",
        "\n",
        "    def batch_tokenize(self, batch, start_token, end_token):\n",
        "        def tokenize(sentence, start_token, end_token):\n",
        "            sentence = sentence.numpy().decode('utf-8').split()  # Convert tensor to list of strings\n",
        "            sentence_word_indices = [self.language_to_index.get(token, self.language_to_index.get('<UNK>')) for token in sentence]\n",
        "            # sentence_word_indices = [self.language_to_index[token] for token in list(sentence)]\n",
        "            if start_token:\n",
        "                sentence_word_indices.insert(0, self.language_to_index[self.START_TOKEN])\n",
        "            if end_token:\n",
        "                sentence_word_indices.append(self.language_to_index[self.END_TOKEN])\n",
        "            sentence_word_indices.extend([self.language_to_index[self.PADDING_TOKEN]] * (self.max_sequence_length - len(sentence_word_indices)))\n",
        "            return tf.convert_to_tensor(sentence_word_indices, dtype=tf.int32)\n",
        "\n",
        "        tokenized = [tokenize(sentence, start_token, end_token) for sentence in batch]\n",
        "        return tf.stack(tokenized)\n",
        "\n",
        "    def call(self, x, start_token, end_token):\n",
        "        x = self.batch_tokenize(x, start_token, end_token)\n",
        "        x = self.embedding(x)\n",
        "        pos = self.position_encoder(x)\n",
        "        x = self.dropout(x + pos)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def is_valid_tokens(sentence, vocab):\n",
        "    for token in list(set(sentence)):\n",
        "        if token not in vocab:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def is_valid_length(sentence, max_sequence_length):\n",
        "    return len(list(sentence)) < (max_sequence_length - 1) # need to re-add the end token so leaving 1 space\n",
        "\n",
        "valid_english_sentences = []\n",
        "for index in range(len(english_sentences)):\n",
        "    english_sentence =  english_sentences[index]\n",
        "    if  is_valid_length(english_sentence, max_sequence_length) \\\n",
        "      and is_valid_tokens(english_sentence, english_vocabulary):\n",
        "        valid_english_sentences.append(english_sentence)\n",
        "\n",
        "\n",
        "def batch_tokenize(language_to_index, batch, start_token, end_token, START_TOKEN, END_TOKEN, PADDING_TOKEN, max_sequence_length):\n",
        "    def tokenize(sentence, start_token, end_token):\n",
        "        sentence_word_indices = [language_to_index[token] for token in list(sentence)]\n",
        "        if start_token:\n",
        "            sentence_word_indices.insert(0, language_to_index[START_TOKEN])\n",
        "        if end_token:\n",
        "            sentence_word_indices.append(language_to_index[END_TOKEN])\n",
        "        sentence_word_indices.extend([language_to_index[PADDING_TOKEN]] * (max_sequence_length - len(sentence_word_indices)))\n",
        "        return tf.convert_to_tensor(sentence_word_indices, dtype=tf.int32)\n",
        "\n",
        "    tokenized = [tokenize(sentence, start_token, end_token) for sentence in batch]\n",
        "    return tf.stack(tokenized)\n",
        "\n",
        "x = batch_tokenize(language_to_index = english_to_index, batch = valid_english_sentences[:10], start_token = True, end_token = True,\n",
        "                     START_TOKEN= START_TOKEN, END_TOKEN= END_TOKEN, PADDING_TOKEN=PADDING_TOKEN, max_sequence_length= max_sequence_length)\n",
        "\n",
        "print(x)\n",
        "# print(valid_english_sentences)\n",
        "# count_70 = tf.math.count_nonzero(res[0] == 70)\n",
        "\n",
        "# print(count_70)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJAxtWozZ5Hq",
        "outputId": "d1465deb-2836-449d-e4a1-d2cb07057622"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[70 46 53 ... 70 70 70]\n",
            " [70 61 46 ... 70 70 70]\n",
            " [70 58 46 ... 70 70 70]\n",
            " ...\n",
            " [70 39 57 ... 70 70 70]\n",
            " [70 48 46 ... 70 70 70]\n",
            " [70 39 56 ... 70 70 70]], shape=(10, 250), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "6cOLH9UaTN3e",
        "outputId": "12bd3250-e7f0-4557-940f-fe9f7d6cee71"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "Exception encountered when calling layer 'sentence_embedding_8' (type SentenceEmbedding).\n\n'str' object has no attribute 'numpy'\n\nCall arguments received by layer 'sentence_embedding_8' (type SentenceEmbedding):\n  • x=['\"however, paes, who was partnering australia\\'s paul hanley, could only go as far as the quarterfinals where they lost to bhupathi and knowles\"', \"'whosoever desires the reward of the world, with allah is the reward of the world and of the everlasting life. allah is the hearer, the seer.'\", \"'the value of insects in the biosphere is enormous because they outnumber all other living groups in measure of species richness.'\", \"'mithali to anchor indian team against australia in odis'\", \"'after the assent of the honble president on 8thseptember, 2016, the 101thconstitutional amendment act, 2016 came into existence'\", \"'the court has fixed a hearing for february 12'\", \"'please select the position where the track should be split.'\", \"'as per police, armys 22rr, special operation group (sog) of police and the central reserve police force (crpf) cordoned the village and launched search operation in the area.'\", \"'jharkhand chief minister hemant soren'\", \"'arvind kumar, sho of the sector 55/56 police station, said a case has been registered under section 376-d (gang rape) of the indian penal code.'\"]\n  • start_token=True\n  • end_token=True",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-29a8bfdd3ae5>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_english_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-b5d10b8cd746>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, start_token, end_token)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-b5d10b8cd746>\u001b[0m in \u001b[0;36mbatch_tokenize\u001b[0;34m(self, batch, start_token, end_token)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_word_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_token\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-b5d10b8cd746>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_word_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_token\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-b5d10b8cd746>\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(sentence, start_token, end_token)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbatch_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Convert tensor to list of strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0msentence_word_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage_to_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage_to_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<UNK>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# sentence_word_indices = [self.language_to_index[token] for token in list(sentence)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: Exception encountered when calling layer 'sentence_embedding_8' (type SentenceEmbedding).\n\n'str' object has no attribute 'numpy'\n\nCall arguments received by layer 'sentence_embedding_8' (type SentenceEmbedding):\n  • x=['\"however, paes, who was partnering australia\\'s paul hanley, could only go as far as the quarterfinals where they lost to bhupathi and knowles\"', \"'whosoever desires the reward of the world, with allah is the reward of the world and of the everlasting life. allah is the hearer, the seer.'\", \"'the value of insects in the biosphere is enormous because they outnumber all other living groups in measure of species richness.'\", \"'mithali to anchor indian team against australia in odis'\", \"'after the assent of the honble president on 8thseptember, 2016, the 101thconstitutional amendment act, 2016 came into existence'\", \"'the court has fixed a hearing for february 12'\", \"'please select the position where the track should be split.'\", \"'as per police, armys 22rr, special operation group (sog) of police and the central reserve police force (crpf) cordoned the village and launched search operation in the area.'\", \"'jharkhand chief minister hemant soren'\", \"'arvind kumar, sho of the sector 55/56 police station, said a case has been registered under section 376-d (gang rape) of the indian penal code.'\"]\n  • start_token=True\n  • end_token=True"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "d_model = 512\n",
        "\n",
        "sentence_embedding = SentenceEmbedding(\n",
        "    max_sequence_length=max_sequence_length,\n",
        "    d_model=d_model,\n",
        "    language_to_index=english_to_index,\n",
        "    START_TOKEN=START_TOKEN,\n",
        "    END_TOKEN=END_TOKEN,\n",
        "    PADDING_TOKEN=PADDING_TOKEN\n",
        ")\n",
        "\n",
        "# output = sentence_embedding(valid_english_sentences[:10], start_token=True, end_token=True)\n",
        "\n",
        "# print(output)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Layer\n",
        "import math\n",
        "\n",
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    d_k = tf.shape(q)[-1]\n",
        "    scaled = tf.matmul(q, k, transpose_b=True) / tf.math.sqrt(tf.cast(d_k, tf.float32))\n",
        "    if mask is not None:\n",
        "        scaled += mask\n",
        "    attention = tf.nn.softmax(scaled, axis=-1)\n",
        "    values = tf.matmul(attention, v)\n",
        "    return values, attention\n",
        "\n",
        "class MultiHeadAttention(Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.qkv_layer = Dense(3 * d_model)\n",
        "        self.linear_layer = Dense(d_model)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        sequence_length = tf.shape(x)[1]\n",
        "        qkv = self.qkv_layer(x)\n",
        "        qkv = tf.reshape(qkv, (batch_size, sequence_length, self.num_heads, 3 * self.head_dim))\n",
        "        qkv = tf.transpose(qkv, perm=[0, 2, 1, 3])\n",
        "        q, k, v = tf.split(qkv, num_or_size_splits=3, axis=-1)\n",
        "        values, attention = scaled_dot_product(q, k, v, mask)\n",
        "        values = tf.transpose(values, perm=[0, 2, 1, 3])\n",
        "        values = tf.reshape(values, (batch_size, sequence_length, self.num_heads * self.head_dim))\n",
        "        out = self.linear_layer(values)\n",
        "        return out\n",
        "\n",
        "multihead_attention = MultiHeadAttention(d_model= d_model, num_heads= 8)\n",
        "\n",
        "x = multihead_attention(x= output)\n",
        "x"
      ],
      "metadata": {
        "id": "6HJV6kdiT3XN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalization(tf.keras.layers.Layer):\n",
        "    def __init__(self, parameters_shape, eps=1e-5):\n",
        "        super(LayerNormalization, self).__init__()\n",
        "        self.parameters_shape = parameters_shape\n",
        "        self.eps = eps\n",
        "        self.gamma = self.add_weight(\n",
        "            shape=parameters_shape,\n",
        "            initializer=\"ones\",\n",
        "            trainable=True,\n",
        "            name=\"gamma\"\n",
        "        )\n",
        "        self.beta = self.add_weight(\n",
        "            shape=parameters_shape,\n",
        "            initializer=\"zeros\",\n",
        "            trainable=True,\n",
        "            name=\"beta\"\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        dims = [-i for i in range(1, len(self.parameters_shape) + 1)]\n",
        "        mean = tf.reduce_mean(inputs, axis=dims, keepdims=True)\n",
        "        var = tf.reduce_mean(tf.square(inputs - mean), axis=dims, keepdims=True)\n",
        "        std = tf.sqrt(var + self.eps)\n",
        "        y = (inputs - mean) / std\n",
        "        out = self.gamma * y + self.beta\n",
        "        return out\n",
        "\n",
        "\n",
        "class PositionwiseFeedForward(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.linear1 = tf.keras.layers.Dense(hidden)\n",
        "        self.linear2 = tf.keras.layers.Dense(d_model)\n",
        "        self.relu = tf.keras.layers.ReLU()\n",
        "        self.dropout = tf.keras.layers.Dropout(rate=drop_prob)\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x, training=training)\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate=drop_prob)\n",
        "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate=drop_prob)\n",
        "\n",
        "    def call(self, x, self_attention_mask, training=False):\n",
        "        residual_x = x\n",
        "        x = self.attention(x, mask=self_attention_mask)\n",
        "        x = self.dropout1(x, training=training)\n",
        "        x = self.norm1(x + residual_x)\n",
        "        residual_x = x\n",
        "        x = self.ffn(x, training=training)\n",
        "        x = self.dropout2(x, training=training)\n",
        "        x = self.norm2(x + residual_x)\n",
        "        return x\n",
        "\n",
        "num_heads =8\n",
        "ffn_hidden = 2048\n",
        "encoder_layer = EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob =0.1)\n",
        "\n",
        "x = encoder_layer(x,self_attention_mask= None)\n",
        "x"
      ],
      "metadata": {
        "id": "VyuN9ZL8xXyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices(valid_english_sentences)\n",
        "batch_size = 32\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "iterator = iter(train_dataset)\n",
        "first_batch = next(iterator)\n",
        "first_batch = tf.reshape(first_batch, (1,batch_size))\n",
        "first_batch"
      ],
      "metadata": {
        "id": "UuupeYXLrn_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SequentialEncoder(tf.keras.Sequential):\n",
        "    def call(self, inputs, self_attention_mask, training=False):\n",
        "        x = inputs\n",
        "        for module in self.layers:\n",
        "            x = module(x, self_attention_mask, training=training)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 ffn_hidden,\n",
        "                 num_heads,\n",
        "                 drop_prob,\n",
        "                 num_layers,\n",
        "                 max_sequence_length,\n",
        "                 language_to_index,\n",
        "                 START_TOKEN,\n",
        "                 END_TOKEN,\n",
        "                 PADDING_TOKEN):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        self.layers = SequentialEncoder([EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n",
        "                                      for _ in range(num_layers)])\n",
        "\n",
        "    def call(self, x, self_attention_mask, start_token, end_token, training=False):\n",
        "        x = self.sentence_embedding(x, start_token, end_token)\n",
        "        x = self.layers(x, self_attention_mask, training=training)\n",
        "        return x\n",
        "drop_prob =0.1\n",
        "encoder = Encoder(d_model =d_model,ffn_hidden=  ffn_hidden,num_heads= num_heads, drop_prob= 0.1, num_layers= 1, max_sequence_length= max_sequence_length,\n",
        "                  language_to_index= english_to_index, START_TOKEN= START_TOKEN, END_TOKEN= END_TOKEN, PADDING_TOKEN= PADDING_TOKEN)\n",
        "x = encoder(x= first_batch, self_attention_mask= None, start_token= True, end_token= True)\n",
        "x"
      ],
      "metadata": {
        "id": "l3dyVm3ejkU2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
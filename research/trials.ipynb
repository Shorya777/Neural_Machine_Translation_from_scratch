{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !kaggle datasets download -d parvmodi/english-to-hindi-machine-translation-dataset"
      ],
      "metadata": {
        "id": "wg510zXyTcgU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip english-to-hindi-machine-translation-dataset.zip"
      ],
      "metadata": {
        "id": "5d8aXkDvTwHb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-lrXpDF5kgx",
        "outputId": "0a650023-3975-4c39-fba3-7542d9b5e490"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10125706\n",
            "[\"However, Paes, who was partnering Australia's Paul Hanley, could only go as far as the quarterfinals where they lost to Bhupathi and Knowles\\n\", 'Whosoever desires the reward of the world, with Allah is the reward of the world and of the Everlasting Life. Allah is the Hearer, the Seer.\\n', 'The value of insects in the biosphere is enormous because they outnumber all other living groups in measure of species richness.\\n', 'Mithali To Anchor Indian Team Against Australia in ODIs\\n', 'After the assent of the Honble President on 8thSeptember, 2016, the 101thConstitutional Amendment Act, 2016 came into existence\\n']\n",
            "10125706\n",
            "['आस्ट्रेलिया के पाल हेनली के साथ जोड़ी बनाने वाले पेस मियामी में क्वार्टरफाइनल तक ही पहुंच सके क्योंकि इस दौर में उन्हें भूपति और नोल्स ने हराया था।\\n', 'और जो शख्स (अपने आमाल का) बदला दुनिया ही में चाहता है तो ख़ुदा के पास दुनिया व आख़िरत दोनों का अज्र मौजूद है और ख़ुदा तो हर शख्स की सुनता और सबको देखता है\\n', 'जैव-मंडल में कीड़ों का मूल्य बहुत है, क्योंकि प्रजातियों की समृद्धि के मामले में उनकी संख्या अन्य जीव समूहों से ज़्यादा है।\\n', 'आस्ट्रेलिया के खिलाफ वनडे टीम की कमान मिताली को\\n', '8 सितम्\\u200dबर, 2016 को माननीय राष्\\u200dट्रपति की स्\\u200dवीकृति मिलने के बाद 101वां संविधान संशोधन अधिनियम, 2016 अस्तित्\\u200dव में आया\\n']\n"
          ]
        }
      ],
      "source": [
        "filepath_en = 'train.en'\n",
        "filepath_hi = 'train.hi'\n",
        "\n",
        "with open(filepath_en, encoding='utf-8') as file:\n",
        "    lines_en = file.readlines()\n",
        "print(len(lines_en))\n",
        "print(lines_en[:5])\n",
        "\n",
        "with open(filepath_hi, encoding='utf-8') as file:\n",
        "    lines_hi = file.readlines()\n",
        "\n",
        "print(len(lines_hi))\n",
        "print(lines_hi[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUOig1EV5kgx",
        "outputId": "1e44ef96-4087-4ef3-f23a-733886c35a1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"however, paes, who was partnering australia's paul hanley, could only go as far as the quarterfinals where they lost to bhupathi and knowles\", 'whosoever desires the reward of the world, with allah is the reward of the world and of the everlasting life. allah is the hearer, the seer.', 'the value of insects in the biosphere is enormous because they outnumber all other living groups in measure of species richness.', 'mithali to anchor indian team against australia in odis', 'after the assent of the honble president on 8thseptember, 2016, the 101thconstitutional amendment act, 2016 came into existence']\n",
            "================================\n",
            "['आस्ट्रेलिया के पाल हेनली के साथ जोड़ी बनाने वाले पेस मियामी में क्वार्टरफाइनल तक ही पहुंच सके क्योंकि इस दौर में उन्हें भूपति और नोल्स ने हराया था।', 'और जो शख्स (अपने आमाल का) बदला दुनिया ही में चाहता है तो ख़ुदा के पास दुनिया व आख़िरत दोनों का अज्र मौजूद है और ख़ुदा तो हर शख्स की सुनता और सबको देखता है', 'जैव-मंडल में कीड़ों का मूल्य बहुत है, क्योंकि प्रजातियों की समृद्धि के मामले में उनकी संख्या अन्य जीव समूहों से ज़्यादा है।', 'आस्ट्रेलिया के खिलाफ वनडे टीम की कमान मिताली को', '8 सितम्\\u200dबर, 2016 को माननीय राष्\\u200dट्रपति की स्\\u200dवीकृति मिलने के बाद 101वां संविधान संशोधन अधिनियम, 2016 अस्तित्\\u200dव में आया']\n"
          ]
        }
      ],
      "source": [
        "TOTAL_SENTENCES = 200000\n",
        "english_sentences = lines_en[:TOTAL_SENTENCES]\n",
        "hindi_sentences = lines_hi[:TOTAL_SENTENCES]\n",
        "english_sentences = [sentence.rstrip('\\n').lower() for sentence in english_sentences]\n",
        "hindi_sentences = [sentence.rstrip('\\n') for sentence in hindi_sentences]\n",
        "\n",
        "print(english_sentences[:5])\n",
        "print(\"================================\")\n",
        "print(hindi_sentences[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OOIXt1O_BAv",
        "outputId": "16959bc9-54c3-4b3a-c04e-4902f5534312"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "97th percentile length Kannada: 258.0\n",
            "97th percentile length English: 267.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "PERCENTILE = 97\n",
        "print( f\"{PERCENTILE}th percentile length Kannada: {np.percentile([len(x) for x in hindi_sentences], PERCENTILE)}\" )\n",
        "print( f\"{PERCENTILE}th percentile length English: {np.percentile([len(x) for x in english_sentences], PERCENTILE)}\" )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "drEPgCVEDAMJ"
      },
      "outputs": [],
      "source": [
        "START_TOKEN = ''\n",
        "PADDING_TOKEN = ''\n",
        "END_TOKEN = ''\n",
        "\n",
        "english_vocabulary = [START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/',\n",
        "                        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "                        ':', '<', '=', '>', '?', '@',\n",
        "                        '[', '\\' ', ']', '^', '_', '`',\n",
        "                        'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
        "                        'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x',\n",
        "                        'y', 'z',\n",
        "                        '{', '|', '}', '~', PADDING_TOKEN, END_TOKEN]\n",
        "\n",
        "hindi_vocabulary = [START_TOKEN, 'अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ए', 'ऐ', 'ओ', 'औ',\n",
        "    'क', 'ख', 'ग', 'घ', 'ङ','ड़', 'च', 'छ', 'ज', 'झ', 'ञ',\n",
        "    'ट', 'ठ', 'ड', 'ढ', 'ण', 'त', 'थ', 'द', 'ध', 'न',\n",
        "    'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ल', 'व', 'श',\n",
        "    'ष', 'स', 'ह', '़', 'ा', 'ि', 'ी', 'ु', 'ू', 'े', 'ै', 'ो', 'ौ',\n",
        "    'ं', 'ः', 'ँ', '्', '०', '१', '२', '३', '४', '५', '६', '७', '८', '९', '|',\n",
        "    ':', '<', '=', '>', '?', '@',\n",
        "    '[', '\\' ', ']', '^', '_', '`',\n",
        "    ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/',\n",
        "    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9','\\u200d','।', 'ृ',\n",
        "    PADDING_TOKEN, END_TOKEN\n",
        "]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qpZWhY7AA5K",
        "outputId": "61110bd2-5247-4846-ae90-485a6656b96e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences: 200000\n",
            "Number of valid sentences: 151260\n"
          ]
        }
      ],
      "source": [
        "max_sequence_length = 250\n",
        "\n",
        "def is_valid_tokens(sentence, vocab):\n",
        "    for token in list(set(sentence)):\n",
        "        if token not in vocab:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def is_valid_length(sentence, max_sequence_length):\n",
        "    return len(list(sentence)) < (max_sequence_length - 1) # need to re-add the end token so leaving 1 space\n",
        "\n",
        "valid_sentence_indicies = []\n",
        "for index in range(len(hindi_sentences)):\n",
        "    hindi_sentence, english_sentence = hindi_sentences[index], english_sentences[index]\n",
        "    if is_valid_length(hindi_sentence, max_sequence_length) \\\n",
        "      and is_valid_length(english_sentence, max_sequence_length) \\\n",
        "      and is_valid_tokens(hindi_sentence, hindi_vocabulary):\n",
        "        valid_sentence_indicies.append(index)\n",
        "\n",
        "print(f\"Number of sentences: {len(hindi_sentences)}\")\n",
        "print(f\"Number of valid sentences: {len(valid_sentence_indicies)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMN2l2qPHqq6",
        "outputId": "176a3931-9f11-4c35-ae0a-bd84d1dcf98a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Characters in the sentences not in the vocabulary: {'±', 'ᒡ', 'ਸ', 'Я', '班', 'コ', 'ज़', '้', 'ঔ', '℃', 'ط', 'ú', 'z', '\\u200e', '\\uf0d8', 'ল', 'గ', 'ී', 'ಚ', 'נ', 'ग़', 'Ň', '²', 'ו', '現', 'ص', '≈', 'শ', 'រ', 'Q', 'S', '॒', 'د', '©', 'h', 'I', 'ច', 'ō', 'ා', '่', 'ô', 'ᡠ', 'A', 'ా', 'ב', '॥', '\\xad', '“', 'в', 'ز', 'K', 'ʔ', 'ॐ', '\\uf146', 'Ι', 'l', 'β', '˜', 'ê', '♦', '→', 'з', 'េ', '‚', '经', '車', 'ě', 'ล', '唐', '☺', 'উ', 'т', 'ˌ', 'ᠩ', 'ン', 'ḍ', 'พ', 'H', 'ר', 'ʊ', 'デ', 'T', 'é', '̈', '‘', '¨', '記', 'Â', 'آ', '⋅', 'y', 'ූ', 'ְ', '⚡', '大', 'ţ', 'ы', 'න', '\\\\', 'о', '\\u200f', 'ʃ', 'ॊ', 'ऽ', 'า', 'W', 'ன', 'Р', 'i', '−', '»', '₹', '\\uf514', 'ό', 'Y', '春', 'ු', 'फ़', 'd', 'B', '州', 'Z', 'ী', 'আ', 'ー', 'М', '♫', 'و', 'ؤ', '្', 'ɑ', 'أ', 'מ', 'п', 'ឃ', '̯', 'k', 'ঙ', 'י', 'Ê', 'Ć', '长', 'ख़', '洞', 'ξ', 'É', 'V', 'ء', '్', 'Ó', '®', 'ප', '›', 'ס', 'ς', '明', 'r', 'م', 'ہ', 'ৱ', 'ң', 'ऩ', 'ī', 'C', 'ల', 'е', 'ห', 'ス', 'c', '의', 'ق', 'š', '世', 'ٰ', '¸', 'ळ', '’', '¶', '夕', 'ර', 'ï', '‑', 'ச', 'ŋ', '▪', 'и', 'Ž', '七', 'ғ', 'ن', 'إ', 'ַ', 'ត', 'మ', '¡', 'ш', 'ண', 'κ', '\\u200c', 't', 'f', '►', 'Ş', 'ස', 'ல', 'α', 'n', 'μ', 'ی', 'D', 'ē', 'ه', 'Ⓡ', 'ো', 'ض', 'ə', 'ٓ', '்', '}', 'Å', 'ᠴ', 'À', '女', 'w', 'E', 'ā', 'Ÿ', 'e', '~', 'ك', '木', 'ং', 'ِ', 'น', 'ಕ', 'д', 'ऎ', 'క', 'ט', '日', 'ิ', '”', '한', 'ּ', 'ᠠ', 'Ō', 'வ', 'g', 'ḷ', '×', 'ر', 'כ', 'ಮ', '•', '্', '↔', 'ऌ', 'ُ', '್', 'O', 'ظ', 'ย', 'υ', 'ז', 'ක', 'ɨ', '°', '高', 'ν', '\\uf0b7', 'ா', '\\ue205', 'â', '動', 'ಯ', 'o', 'ֵ', 'ক', 'ਰ', '○', 'ದ', '․', 'ę', '章', 'ί', '½', 'Δ', '일', 'ढ़', 'ɵ', 'я', 'ਤ', 'к', 'ි', 'ง', '공', 'ĺ', 'ய', 'ॄ', '魯', 'È', '鲁', 'ä', 'ช', 'ñ', 'ع', 'ว', 'ö', 'ি', '代', 'а', '▫', 'δ', 'ק', '❏', '\\xa0', 'ৰ', '中', 'ч', 'ᓗ', 'ත', 'ਆ', 'ح', 'м', 'ਦ', '੍', '■', 'L', 'य़', 'ف', 'á', 'с', 'u', 'ਿ', 'ত', 'v', 'ي', '™', '়', 'க', '′', '॰', 'ο', 'ে', '自', '□', '\\ue208', '¬', 'ය', '❑', '휴', 'ই', 'ড', 'Θ', 'ම', '́', '域', 'ស', '—', '郑', '西', 'χ', 'ট', 'ి', 'メ', 'G', 'у', 'ೆ', 'ɪ', 'ٕ', '׃', 'U', 'ִ', '\\uf0a7', 'ಿ', '€', 'Ṭ', 'н', 'Ω', 'Ò', 'ල', '林', 'Э', '¼', 'த', 'ನ', 'ు', 'ऑ', 'ை', 'ம', 'ต', 'ل', '–', 'К', 'л', '‐', '관', 'อ', 'ං', '經', 'ำ', 'm', 'R', 'ও', '에', '死', '਼', 'ロ', 'ó', 'ऒ', '◆', 'ᐃ', '§', 'à', '\\u2061', 'С', 'ॉ', '¹', '骨', 'Þ', 'Ż', '\\ue88c', 'ǔ', '☻', 'ோ', 'Τ', '♪', 'Ч', 'ন', 'ئ', '庭', 'ব', 'ب', '·', '´', 'P', ';', 'ّ', 'ا', 'ミ', 'क़', 'ุ', 'ָ', 'ॢ', '湖', 'θ', 'ィ', '„', 'ᠨ', 'ಳ', 'త', 'ة', 'F', 'ො', 'ش', 'Ю', 'a', 'x', '…', '⊆', 'ர', 'দ', 'è', 'ֶ', 'ऱ', 'ł', 'í', 'ॅ', 'þ', '\\u2008', 'פ', 'แ', 'א', 'া', 'ਵ', '˝', 'Г', 'ก', 'b', '\\u200b', 'ˈ', 'پ', '☞', '\\ue5a0', 'চ', 'ᐅ', 'ท', 'ె', 'q', 'গ', 'р', 'হ', '⁻', 'ග', 'ש', 'ி', 'ॠ', 'ן', 'レ', '£', 'ò', 'ֹ', 'η', 'ε', 'র', 'ל', 'τ', 'M', 'X', 'Ô', 'প', 'ت', 'ន', '\\uf642', 'س', 'ª', 'ុ', 'ऍ', '◇', 's', 'λ', '性', 'パ', 'ু', 'ඩ', 'ʁ', 'ম', 'ម', 'জ', 'ה', 'ă', 'æ', 'ு', 'Ę', 'J', 'ø', '්', 'ʹ', 'й', '國', 'য', 'ப', 'ក', 'ι', 'ρ', 'ව', 'ş', 'б', 'N', '{', 'j', 'p', 'ế', 'َ', 'স', 'ঘ', '\\ufeff', 'ː', '⁰', 'ॆ'}\n"
          ]
        }
      ],
      "source": [
        "# for i in 'शख्स':\n",
        "  # print(i)\n",
        "\n",
        "\n",
        "# hindi_sentences = [\n",
        "#     'आस्ट्रेलिया के पाल हेनली के साथ जोड़ी बनाने वाले पेस मियामी में क्वार्टरफाइनल तक ही पहुंच सके क्योंकि इस दौर में उन्हें भूपति और नोल्स ने हराया था।',\n",
        "#     'और जो शख्स (अपने आमाल का) बदला दुनिया ही में चाहता है तो ख़ुदा के पास दुनिया व आख़िरत दोनों का अज्र मौजूद है और ख़ुदा तो हर शख्स की सुनता और सबको देखता है',\n",
        "#     'जैव-मंडल में कीड़ों का मूल्य बहुत है, क्योंकि प्रजातियों की समृद्धि के मामले में उनकी संख्या अन्य जीव समूहों से ज़्यादा है।',\n",
        "#     'आस्ट्रेलिया के खिलाफ वनडे टीम की कमान मिताली को',\n",
        "#     '8 सितम्‍बर, 2016 को माननीय राष्ट्रपति की स्‍वीकृति मिलने के बाद 101वां संविधान संशोधन अधिनियम, 2016 अस्तित्व में आया'\n",
        "# ]\n",
        "\n",
        "# Define the provided vocabulary set\n",
        "hindi_vocabulary_set = set(hindi_vocabulary)\n",
        "\n",
        "# Combine all sentences into one string\n",
        "combined_sentences = ''.join(hindi_sentences)\n",
        "\n",
        "# Find all unique characters in the combined sentences\n",
        "unique_chars_in_sentences = set(combined_sentences)\n",
        "\n",
        "# Find characters in the sentences that are not in the vocabulary\n",
        "missing_chars = unique_chars_in_sentences - hindi_vocabulary_set\n",
        "\n",
        "print(\"Characters in the sentences not in the vocabulary:\", missing_chars)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "dXmG9YJcTN3d"
      },
      "outputs": [],
      "source": [
        "index_to_hindi = {k:v for k,v in enumerate(hindi_vocabulary)}\n",
        "hindi_to_index = {v:k for k,v in enumerate(hindi_vocabulary)}\n",
        "index_to_english = {k:v for k,v in enumerate(english_vocabulary)}\n",
        "english_to_index = {v:k for k,v in enumerate(english_vocabulary)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "rdyjQgdKTN3d"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, Dropout\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, max_seq_len):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "    def call(self, inputs):\n",
        "        pos = np.arange(self.max_seq_len)[:, np.newaxis]\n",
        "        i = np.arange(self.d_model)[np.newaxis, :]\n",
        "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(self.d_model))\n",
        "        angle_rads = pos * angle_rates\n",
        "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "        pos_encoding = angle_rads[np.newaxis, ...]\n",
        "        return tf.constant(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "class SentenceEmbedding(tf.keras.Model):\n",
        "    def __init__(self, max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n",
        "        super(SentenceEmbedding, self).__init__()\n",
        "        self.vocab_size = len(language_to_index)\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.embedding = Embedding(input_dim=self.vocab_size, output_dim=d_model)\n",
        "        self.language_to_index = language_to_index\n",
        "        self.position_encoder = PositionalEncoding(d_model, max_sequence_length)\n",
        "        self.dropout = Dropout(0.1)\n",
        "        self.START_TOKEN = START_TOKEN\n",
        "        self.END_TOKEN = END_TOKEN\n",
        "        self.PADDING_TOKEN = PADDING_TOKEN\n",
        "\n",
        "    def batch_tokenize(self, batch, start_token, end_token):\n",
        "        def tokenize(sentence, start_token, end_token):\n",
        "            sentence_word_indices = [self.language_to_index[token] for token in list(sentence)]\n",
        "            if start_token:\n",
        "                sentence_word_indices.insert(0, self.language_to_index[self.START_TOKEN])\n",
        "            if end_token:\n",
        "                sentence_word_indices.append(self.language_to_index[self.END_TOKEN])\n",
        "            sentence_word_indices.extend([self.language_to_index[self.PADDING_TOKEN]] * (self.max_sequence_length - len(sentence_word_indices)))\n",
        "            return tf.convert_to_tensor(sentence_word_indices, dtype=tf.int32)\n",
        "\n",
        "        tokenized = [tokenize(sentence, start_token, end_token) for sentence in batch]\n",
        "        return tf.stack(tokenized)\n",
        "\n",
        "    def call(self, x, start_token, end_token):\n",
        "        x = self.batch_tokenize(x, start_token, end_token)\n",
        "        x = self.embedding(x)\n",
        "        pos = self.position_encoder(x)\n",
        "        x = self.dropout(x + pos)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "6cOLH9UaTN3e",
        "outputId": "9b544db2-dcd4-4a6e-db19-a6e2c4ae3170",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[ 0.          1.          0.         ...  1.          0.\n",
            "    1.        ]\n",
            "  [ 0.8142139   0.518672    0.8392038  ...  1.0089104   0.01558238\n",
            "    0.98024493]\n",
            "  [ 0.9281368  -0.42546865  0.91599506 ...  1.0037731  -0.01198027\n",
            "    1.024127  ]\n",
            "  ...\n",
            "  [ 0.92680717 -0.37553754 -0.47029194 ...  0.99964774  0.02560204\n",
            "    0.99967223]\n",
            "  [ 0.18475212 -0.98278517  0.45737407 ...  0.9996449   0.02570567\n",
            "    0.99966955]\n",
            "  [-0.7271632  -0.6864646   0.9914194  ...  0.999642    0.02580929\n",
            "    0.99966687]]\n",
            "\n",
            " [[ 0.          1.          0.         ...  1.          0.\n",
            "    1.        ]\n",
            "  [ 0.88086265  0.5756271   0.7747787  ...  0.9555404   0.02847967\n",
            "    0.9758246 ]\n",
            "  [ 0.8820403  -0.43777713  0.9537623  ...  1.0089104   0.01568605\n",
            "    0.98024493]\n",
            "  ...\n",
            "  [ 0.92680717 -0.37553754 -0.47029194 ...  0.99964774  0.02560204\n",
            "    0.99967223]\n",
            "  [ 0.18475212 -0.98278517  0.45737407 ...  0.9996449   0.02570567\n",
            "    0.99966955]\n",
            "  [-0.7271632  -0.6864646   0.9914194  ...  0.999642    0.02580929\n",
            "    0.99966687]]\n",
            "\n",
            " [[ 0.          1.          0.         ...  1.          0.\n",
            "    1.        ]\n",
            "  [ 0.80188936  0.50394773  0.79644006 ...  1.0327843   0.01530991\n",
            "    1.0178452 ]\n",
            "  [ 0.8820403  -0.43777713  0.9537623  ...  1.0089104   0.01568605\n",
            "    0.98024493]\n",
            "  ...\n",
            "  [ 0.92680717 -0.37553754 -0.47029194 ...  0.99964774  0.02560204\n",
            "    0.99967223]\n",
            "  [ 0.18475212 -0.98278517  0.45737407 ...  0.9996449   0.02570567\n",
            "    0.99966955]\n",
            "  [-0.7271632  -0.6864646   0.9914194  ...  0.999642    0.02580929\n",
            "    0.99966687]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 0.          1.          0.         ...  1.          0.\n",
            "    1.        ]\n",
            "  [ 0.88594407  0.50776756  0.78197527 ...  1.007124    0.03937839\n",
            "    0.9865881 ]\n",
            "  [ 0.9306925  -0.42393526  0.963225   ...  0.98833376  0.01014771\n",
            "    1.0210314 ]\n",
            "  ...\n",
            "  [ 0.92680717 -0.37553754 -0.47029194 ...  0.99964774  0.02560204\n",
            "    0.99967223]\n",
            "  [ 0.18475212 -0.98278517  0.45737407 ...  0.9996449   0.02570567\n",
            "    0.99966955]\n",
            "  [-0.7271632  -0.6864646   0.9914194  ...  0.999642    0.02580929\n",
            "    0.99966687]]\n",
            "\n",
            " [[ 0.          1.          0.         ...  1.          0.\n",
            "    1.        ]\n",
            "  [ 0.8751176   0.52647644  0.8296214  ...  0.96544856  0.02437607\n",
            "    1.019704  ]\n",
            "  [ 0.8820403  -0.43777713  0.9537623  ...  1.0089104   0.01568605\n",
            "    0.98024493]\n",
            "  ...\n",
            "  [ 0.92680717 -0.37553754 -0.47029194 ...  0.99964774  0.02560204\n",
            "    0.99967223]\n",
            "  [ 0.18475212 -0.98278517  0.45737407 ...  0.9996449   0.02570567\n",
            "    0.99966955]\n",
            "  [-0.7271632  -0.6864646   0.9914194  ...  0.999642    0.02580929\n",
            "    0.99966687]]\n",
            "\n",
            " [[ 0.          1.          0.         ...  1.          0.\n",
            "    1.        ]\n",
            "  [ 0.88594407  0.50776756  0.78197527 ...  1.007124    0.03937839\n",
            "    0.9865881 ]\n",
            "  [ 0.93376094 -0.41464856  0.972794   ...  1.0358279   0.03187194\n",
            "    0.98204243]\n",
            "  ...\n",
            "  [ 0.92680717 -0.37553754 -0.47029194 ...  0.99964774  0.02560204\n",
            "    0.99967223]\n",
            "  [ 0.18475212 -0.98278517  0.45737407 ...  0.9996449   0.02570567\n",
            "    0.99966955]\n",
            "  [-0.7271632  -0.6864646   0.9914194  ...  0.999642    0.02580929\n",
            "    0.99966687]]], shape=(10, 250, 512), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "d_model = 512\n",
        "\n",
        "sentence_embedding = SentenceEmbedding(\n",
        "    max_sequence_length=max_sequence_length,\n",
        "    d_model=d_model,\n",
        "    language_to_index=english_to_index,\n",
        "    START_TOKEN=START_TOKEN,\n",
        "    END_TOKEN=END_TOKEN,\n",
        "    PADDING_TOKEN=PADDING_TOKEN\n",
        ")\n",
        "\n",
        "output = sentence_embedding(english_sentences[:10], start_token=True, end_token=True)\n",
        "\n",
        "print(output)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def is_valid_tokens(sentence, vocab):\n",
        "    for token in list(set(sentence)):\n",
        "        if token not in vocab:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def is_valid_length(sentence, max_sequence_length):\n",
        "    return len(list(sentence)) < (max_sequence_length - 1) # need to re-add the end token so leaving 1 space\n",
        "\n",
        "valid_english_sentences = []\n",
        "for index in range(len(english_sentences)):\n",
        "    english_sentence =  english_sentences[index]\n",
        "    if  is_valid_length(english_sentence, max_sequence_length) \\\n",
        "      and is_valid_tokens(english_sentence, english_vocabulary):\n",
        "        valid_english_sentences.append(english_sentence)\n",
        "\n",
        "\n",
        "def batch_tokenize(language_to_index, batch, start_token, end_token, START_TOKEN, END_TOKEN, PADDING_TOKEN, max_sequence_length):\n",
        "    def tokenize(sentence, start_token, end_token):\n",
        "        sentence_word_indices = [language_to_index[token] for token in list(sentence)]\n",
        "        if start_token:\n",
        "            sentence_word_indices.insert(0, language_to_index[START_TOKEN])\n",
        "        if end_token:\n",
        "            sentence_word_indices.append(language_to_index[END_TOKEN])\n",
        "        sentence_word_indices.extend([language_to_index[PADDING_TOKEN]] * (max_sequence_length - len(sentence_word_indices)))\n",
        "        return tf.convert_to_tensor(sentence_word_indices, dtype=tf.int32)\n",
        "\n",
        "    tokenized = [tokenize(sentence, start_token, end_token) for sentence in batch]\n",
        "    return tf.stack(tokenized)\n",
        "\n",
        "res = batch_tokenize(language_to_index = english_to_index, batch = valid_english_sentences[:10], start_token = True, end_token = True,\n",
        "                     START_TOKEN= START_TOKEN, END_TOKEN= END_TOKEN, PADDING_TOKEN=PADDING_TOKEN, max_sequence_length= max_sequence_length)\n",
        "\n",
        "print(res)\n",
        "# print(valid_english_sentences)\n",
        "# count_70 = tf.math.count_nonzero(res[0] == 70)\n",
        "\n",
        "# print(count_70)\n"
      ],
      "metadata": {
        "id": "MJAxtWozZ5Hq",
        "outputId": "4a1632a1-9f5a-4b30-8bb7-9025254ac0e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[70 46 53 61 43 60 43 56 13  1 54 39 43 57 13  1 61 46 53  1 61 39 57  1\n",
            " 54 39 56 58 52 43 56 47 52 45  1 39 59 57 58 56 39 50 47 39  8 57  1 54\n",
            " 39 59 50  1 46 39 52 50 43 63 13  1 41 53 59 50 42  1 53 52 50 63  1 45\n",
            " 53  1 39 57  1 44 39 56  1 39 57  1 58 46 43  1 55 59 39 56 58 43 56 44\n",
            " 47 52 39 50 57  1 61 46 43 56 43  1 58 46 43 63  1 50 53 57 58  1 58 53\n",
            "  1 40 46 59 54 39 58 46 47  1 39 52 42  1 49 52 53 61 50 43 57 70 70 70\n",
            " 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70\n",
            " 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70\n",
            " 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70\n",
            " 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70\n",
            " 70 70 70 70 70 70 70 70 70 70], shape=(250,), dtype=int32)\n",
            "tf.Tensor(110, shape=(), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6HJV6kdiT3XN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
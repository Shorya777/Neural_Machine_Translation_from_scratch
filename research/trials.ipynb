{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d parvmodi/english-to-hindi-machine-translation-dataset"
      ],
      "metadata": {
        "id": "wg510zXyTcgU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43c1a5c3-b911-4ca7-93fa-e929e09dfdd2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/parvmodi/english-to-hindi-machine-translation-dataset\n",
            "License(s): CC0-1.0\n",
            "Downloading english-to-hindi-machine-translation-dataset.zip to /content\n",
            "100% 1.04G/1.04G [00:15<00:00, 88.2MB/s]\n",
            "100% 1.04G/1.04G [00:15<00:00, 73.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip english-to-hindi-machine-translation-dataset.zip"
      ],
      "metadata": {
        "id": "5d8aXkDvTwHb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea4187a6-ae36-4302-e73c-ed82d4813289"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  english-to-hindi-machine-translation-dataset.zip\n",
            "  inflating: train.en                \n",
            "  inflating: train.hi                \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-lrXpDF5kgx",
        "outputId": "c51f0e9f-0c63-4573-e9ad-c3d6472197ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10125706\n",
            "[\"However, Paes, who was partnering Australia's Paul Hanley, could only go as far as the quarterfinals where they lost to Bhupathi and Knowles\\n\", 'Whosoever desires the reward of the world, with Allah is the reward of the world and of the Everlasting Life. Allah is the Hearer, the Seer.\\n', 'The value of insects in the biosphere is enormous because they outnumber all other living groups in measure of species richness.\\n', 'Mithali To Anchor Indian Team Against Australia in ODIs\\n', 'After the assent of the Honble President on 8thSeptember, 2016, the 101thConstitutional Amendment Act, 2016 came into existence\\n']\n",
            "10125706\n",
            "['आस्ट्रेलिया के पाल हेनली के साथ जोड़ी बनाने वाले पेस मियामी में क्वार्टरफाइनल तक ही पहुंच सके क्योंकि इस दौर में उन्हें भूपति और नोल्स ने हराया था।\\n', 'और जो शख्स (अपने आमाल का) बदला दुनिया ही में चाहता है तो ख़ुदा के पास दुनिया व आख़िरत दोनों का अज्र मौजूद है और ख़ुदा तो हर शख्स की सुनता और सबको देखता है\\n', 'जैव-मंडल में कीड़ों का मूल्य बहुत है, क्योंकि प्रजातियों की समृद्धि के मामले में उनकी संख्या अन्य जीव समूहों से ज़्यादा है।\\n', 'आस्ट्रेलिया के खिलाफ वनडे टीम की कमान मिताली को\\n', '8 सितम्\\u200dबर, 2016 को माननीय राष्\\u200dट्रपति की स्\\u200dवीकृति मिलने के बाद 101वां संविधान संशोधन अधिनियम, 2016 अस्तित्\\u200dव में आया\\n']\n"
          ]
        }
      ],
      "source": [
        "filepath_en = 'train.en'\n",
        "filepath_hi = 'train.hi'\n",
        "\n",
        "with open(filepath_en, encoding='utf-8') as file:\n",
        "    lines_en = file.readlines()\n",
        "print(len(lines_en))\n",
        "print(lines_en[:5])\n",
        "\n",
        "with open(filepath_hi, encoding='utf-8') as file:\n",
        "    lines_hi = file.readlines()\n",
        "\n",
        "print(len(lines_hi))\n",
        "print(lines_hi[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUOig1EV5kgx",
        "outputId": "8a5520ea-4744-40ae-989c-26c76529aa51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"however, paes, who was partnering australia's paul hanley, could only go as far as the quarterfinals where they lost to bhupathi and knowles\", 'whosoever desires the reward of the world, with allah is the reward of the world and of the everlasting life. allah is the hearer, the seer.', 'the value of insects in the biosphere is enormous because they outnumber all other living groups in measure of species richness.', 'mithali to anchor indian team against australia in odis', 'after the assent of the honble president on 8thseptember, 2016, the 101thconstitutional amendment act, 2016 came into existence']\n",
            "================================\n",
            "['आस्ट्रेलिया के पाल हेनली के साथ जोड़ी बनाने वाले पेस मियामी में क्वार्टरफाइनल तक ही पहुंच सके क्योंकि इस दौर में उन्हें भूपति और नोल्स ने हराया था।', 'और जो शख्स (अपने आमाल का) बदला दुनिया ही में चाहता है तो ख़ुदा के पास दुनिया व आख़िरत दोनों का अज्र मौजूद है और ख़ुदा तो हर शख्स की सुनता और सबको देखता है', 'जैव-मंडल में कीड़ों का मूल्य बहुत है, क्योंकि प्रजातियों की समृद्धि के मामले में उनकी संख्या अन्य जीव समूहों से ज़्यादा है।', 'आस्ट्रेलिया के खिलाफ वनडे टीम की कमान मिताली को', '8 सितम्\\u200dबर, 2016 को माननीय राष्\\u200dट्रपति की स्\\u200dवीकृति मिलने के बाद 101वां संविधान संशोधन अधिनियम, 2016 अस्तित्\\u200dव में आया']\n"
          ]
        }
      ],
      "source": [
        "TOTAL_SENTENCES = 200000\n",
        "english_sentences = lines_en[:TOTAL_SENTENCES]\n",
        "hindi_sentences = lines_hi[:TOTAL_SENTENCES]\n",
        "english_sentences = [sentence.rstrip('\\n').lower() for sentence in english_sentences]\n",
        "hindi_sentences = [sentence.rstrip('\\n') for sentence in hindi_sentences]\n",
        "\n",
        "print(english_sentences[:5])\n",
        "print(\"================================\")\n",
        "print(hindi_sentences[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OOIXt1O_BAv",
        "outputId": "9d4dd81a-acd8-4674-ee81-b6c7a3fa13fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "97th percentile length Kannada: 258.0\n",
            "97th percentile length English: 267.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "PERCENTILE = 97\n",
        "print( f\"{PERCENTILE}th percentile length Kannada: {np.percentile([len(x) for x in hindi_sentences], PERCENTILE)}\" )\n",
        "print( f\"{PERCENTILE}th percentile length English: {np.percentile([len(x) for x in english_sentences], PERCENTILE)}\" )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "drEPgCVEDAMJ",
        "outputId": "6789c843-9e48-48ca-ac69-165744966891",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "71\n"
          ]
        }
      ],
      "source": [
        "START_TOKEN = '<START>'\n",
        "PADDING_TOKEN = '<PADDING>'\n",
        "END_TOKEN = '<END>'\n",
        "\n",
        "english_vocabulary = [START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/',\n",
        "                        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "                        ':', '<', '=', '>', '?', '@',\n",
        "                        '[', '\\' ', ']', '^', '_', '`',\n",
        "                        'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
        "                        'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x',\n",
        "                        'y', 'z',\n",
        "                        '{', '|', '}', '~', PADDING_TOKEN, END_TOKEN]\n",
        "\n",
        "hindi_vocabulary = [START_TOKEN, 'अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ए', 'ऐ', 'ओ', 'औ',\n",
        "    'क', 'ख', 'ग', 'घ', 'ङ','ड़', 'च', 'छ', 'ज', 'झ', 'ञ',\n",
        "    'ट', 'ठ', 'ड', 'ढ', 'ण', 'त', 'थ', 'द', 'ध', 'न',\n",
        "    'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ल', 'व', 'श',\n",
        "    'ष', 'स', 'ह', '़', 'ा', 'ि', 'ी', 'ु', 'ू', 'े', 'ै', 'ो', 'ौ',\n",
        "    'ं', 'ः', 'ँ', '्', '०', '१', '२', '३', '४', '५', '६', '७', '८', '९', '|',\n",
        "    ':', '<', '=', '>', '?', '@',\n",
        "    '[', '\\' ', ']', '^', '_', '`',\n",
        "    ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/',\n",
        "    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9','\\u200d','।', 'ृ',\n",
        "    PADDING_TOKEN, END_TOKEN\n",
        "]\n",
        "\n",
        "print(len(english_vocabulary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qpZWhY7AA5K",
        "outputId": "493617af-310a-49c9-bcfc-db5d0d90e7c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences: 200000\n",
            "Number of valid sentences: 151260\n"
          ]
        }
      ],
      "source": [
        "max_sequence_length = 250\n",
        "\n",
        "def is_valid_tokens(sentence, vocab):\n",
        "    for token in list(set(sentence)):\n",
        "        if token not in vocab:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def is_valid_length(sentence, max_sequence_length):\n",
        "    return len(list(sentence)) < (max_sequence_length - 1) # need to re-add the end token so leaving 1 space\n",
        "\n",
        "valid_sentence_indicies = []\n",
        "for index in range(len(hindi_sentences)):\n",
        "    hindi_sentence, english_sentence = hindi_sentences[index], english_sentences[index]\n",
        "    if is_valid_length(hindi_sentence, max_sequence_length) \\\n",
        "      and is_valid_length(english_sentence, max_sequence_length) \\\n",
        "      and is_valid_tokens(hindi_sentence, hindi_vocabulary):\n",
        "        valid_sentence_indicies.append(index)\n",
        "\n",
        "print(f\"Number of sentences: {len(hindi_sentences)}\")\n",
        "print(f\"Number of valid sentences: {len(valid_sentence_indicies)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMN2l2qPHqq6",
        "outputId": "0aac7632-b92c-437f-ba12-711198f02b65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Characters in the sentences not in the vocabulary: {'p', 'V', 'ֹ', 'ы', 'ऩ', '州', 'я', 'ᐃ', 'প', 's', 'a', 'é', 'כ', '้', 'ז', 'ಮ', 'ො', '\\u2008', 'ঙ', 'к', 'ख़', 'ප', '™', 'ᡠ', 'ං', 'n', '⁰', 'μ', 'স', 'ॆ', 'א', 'ó', 'ּ', 'ص', '»', '❏', 'ऌ', 'ط', '¸', 'ظ', 'ِ', 'ী', '˝', 'ර', 'ḷ', 'ٕ', 'ঘ', 'К', 'ě', '⊆', '੍', 'ុ', 'ॉ', 'W', 'ন', '্', '්', '׃', '骨', 'у', 'コ', '車', 'с', 'ٓ', 'ل', 'ு', 'â', 'ι', 'ֶ', '°', 'ව', 'Ō', 'ַ', 'š', 'ミ', '\\ue5a0', 'L', 'េ', 'R', 'ی', '鲁', 'Ю', 'Ż', 'τ', '에', 'ॐ', '℃', '\\uf514', '한', 'ਵ', 'С', 'ឃ', 'า', '日', 'á', '£', 'ع', 'α', '▫', '}', '章', 'ş', 'δ', '木', '\\ufeff', 'ο', '⋅', 'Р', '²', 'ה', 'ー', 'ę', 'O', 'ò', 'ن', 'ہ', 'ש', 'ᠠ', 'ก', 'ุ', 'y', '្', 'வ', 'м', '́', 't', 'ǔ', '女', '经', 'ิ', 'K', '್', 'พ', 'ம', 'ί', 'ऑ', 'ঔ', 'ల', 'و', '\\uf0d8', 'জ', '்', 'o', 'ح', '′', 'ও', 'চ', '☞', 'ִ', 'ң', 'р', 'ت', 'ಿ', 'ব', 'υ', '਼', '‘', '¹', '휴', 'ग़', 'ல', 'ε', '॰', 'М', '\\ue205', 'י', 'з', '’', 'h', '唐', '☺', 'm', 'ᠴ', 'ᓗ', 'م', 'ŋ', 'Z', 'J', 'ऍ', 'ʔ', 'ς', 'ا', '¡', 'র', 'Ó', 'и', 'κ', 'ч', 'w', 'න', 'д', 'z', 'b', 'M', 'ض', '中', 'َ', 'ר', 'ਆ', '“', 'þ', 'শ', 'ো', 'Ş', 'ᠨ', 'ॊ', 'ă', 'd', '域', 'ィ', 'I', 'แ', 'ス', '̯', '\\uf146', 'X', 'ţ', 'ু', '€', 'উ', '½', 'ñ', 'B', '湖', '魯', 'ב', 'ำ', 'ʹ', 'í', 'ਰ', 'ම', 'ਿ', 'ॢ', 'ऱ', 'ೆ', '現', 'Τ', 'ه', '明', 'ස', '班', 'ʃ', 'è', 'נ', 'H', '…', 'ξ', '■', 'χ', 'ದ', 'ය', 'ν', 'อ', '庭', 'S', 'ˈ', '§', '−', '̈', 'ห', '代', 'ు', 'À', 'গ', 'ಯ', 'Я', 'п', 'ច', 'น', 'Ň', '\\ue208', 'פ', 'ק', 'ī', '일', 'U', 'б', 'g', 'ਦ', 'ロ', '\\u200c', 'ö', 'N', 'ᐅ', 'ಳ', 'β', 'ऎ', 'ស', 'य़', 'λ', 'ك', 'ɵ', 'ಕ', 'ُ', '\\ue88c', 'r', 'Ω', '长', 'أ', ';', '夕', 'о', 'Ž', '¨', 'レ', 'آ', '„', 'ش', 'v', 'ł', '性', 'k', 'ر', 'ə', '—', 'н', 'P', '¼', 'ต', 'ই', 'ɨ', '記', '్', '動', '॥', '⚡', 'メ', '►', 'u', 'ක', 'Θ', '\\u200e', '©', 'ง', 'ô', 'ế', '❑', 'η', 'E', 'ි', 'ᒡ', 'j', 'ৱ', 'ē', 'Ч', 'ా', 'ச', 'ල', '관', 'ン', 'ె', 'ш', 'ත', 'ו', '¬', 'а', '○', 'ৰ', 'ؤ', '়', '≈', '의', '西', 'ত', 'È', '郑', 'ऒ', 'e', '死', 'Y', 'ָ', 'л', 'ä', 'ి', 'ន', 'ḍ', 'θ', 'క', 'হ', '\\xa0', '\\u200b', 'ॅ', 'ড', '世', 'Å', 'i', 'l', 'గ', '洞', 'দ', 'G', 'Ò', 'Ć', 'й', 'మ', '”', 'ס', 'ล', 'আ', '˜', 'ನ', 'Ÿ', '₹', 'ज़', 'ऽ', 'ท', 'ॄ', '\\uf642', 'ف', 'ி', 'ל', 'ු', 'ό', '\\xad', 'ː', 'फ़', 'ā', 'D', 'т', 'ා', 'إ', '{', 'デ', 'е', '›', 'ಚ', 'ை', 'Ṭ', 'ق', 'Δ', 'Ⓡ', '\\uf0a7', 'ú', 'Þ', 'س', 'ø', '×', 'ª', '·', '®', '國', 'क़', 'ா', 'ғ', '◆', 'ਤ', 'ූ', '◇', '공', 'ត', 'à', '經', 'Ô', 'ក', 'ٰ', '☻', 'ט', '□', 'త', '॒', '\\u2061', 'ோ', 'ة', 'Ι', '→', '–', 'パ', 'ّ', 'ย', 'ে', 'ช', 'ক', 'ê', 'য', 'F', '่', '♫', 'x', '\\\\', 'ب', '\\uf0b7', 'ळ', 'A', 'ء', '▪', 'ং', 'ְ', 'æ', '大', 'ئ', 'ன', 'ඩ', '\\u200f', '´', 'រ', 'ப', 'É', 'ʁ', '↔', '自', '七', 'ர', 'ĺ', 'ֵ', '~', 'ট', 'f', '⁻', 'ō', 'ʊ', 'T', '¶', 'ਸ', 'ز', 'ি', '♪', 'க', 'Ê', '‐', 'ග', 'в', '高', '♦', 'c', 'ɪ', 'د', 'ម', 'ম', 'پ', 'ן', '•', '․', 'ढ़', 'C', 'ল', '‚', 'ว', 'Â', 'த', 'ண', '‑', 'מ', 'ρ', 'q', 'ී', 'Э', 'ॠ', 'Г', 'ய', 'Q', 'ï', '±', 'ᠩ', '林', 'ˌ', '春', 'ɑ', 'া', 'Ę', 'ي'}\n"
          ]
        }
      ],
      "source": [
        "# for i in 'शख्स':\n",
        "  # print(i)\n",
        "\n",
        "\n",
        "# hindi_sentences = [\n",
        "#     'आस्ट्रेलिया के पाल हेनली के साथ जोड़ी बनाने वाले पेस मियामी में क्वार्टरफाइनल तक ही पहुंच सके क्योंकि इस दौर में उन्हें भूपति और नोल्स ने हराया था।',\n",
        "#     'और जो शख्स (अपने आमाल का) बदला दुनिया ही में चाहता है तो ख़ुदा के पास दुनिया व आख़िरत दोनों का अज्र मौजूद है और ख़ुदा तो हर शख्स की सुनता और सबको देखता है',\n",
        "#     'जैव-मंडल में कीड़ों का मूल्य बहुत है, क्योंकि प्रजातियों की समृद्धि के मामले में उनकी संख्या अन्य जीव समूहों से ज़्यादा है।',\n",
        "#     'आस्ट्रेलिया के खिलाफ वनडे टीम की कमान मिताली को',\n",
        "#     '8 सितम्‍बर, 2016 को माननीय राष्ट्रपति की स्‍वीकृति मिलने के बाद 101वां संविधान संशोधन अधिनियम, 2016 अस्तित्व में आया'\n",
        "# ]\n",
        "\n",
        "# Define the provided vocabulary set\n",
        "hindi_vocabulary_set = set(hindi_vocabulary)\n",
        "\n",
        "# Combine all sentences into one string\n",
        "combined_sentences = ''.join(hindi_sentences)\n",
        "\n",
        "# Find all unique characters in the combined sentences\n",
        "unique_chars_in_sentences = set(combined_sentences)\n",
        "\n",
        "# Find characters in the sentences that are not in the vocabulary\n",
        "missing_chars = unique_chars_in_sentences - hindi_vocabulary_set\n",
        "\n",
        "print(\"Characters in the sentences not in the vocabulary:\", missing_chars)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "dXmG9YJcTN3d",
        "outputId": "72cdf059-fc17-48b5-a64b-b565f0dddca9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<START>': 0, ' ': 1, '!': 2, '\"': 3, '#': 4, '$': 5, '%': 6, '&': 7, \"'\": 8, '(': 9, ')': 10, '*': 11, '+': 12, ',': 13, '-': 14, '.': 15, '/': 16, '0': 17, '1': 18, '2': 19, '3': 20, '4': 21, '5': 22, '6': 23, '7': 24, '8': 25, '9': 26, ':': 27, '<': 28, '=': 29, '>': 30, '?': 31, '@': 32, '[': 33, \"' \": 34, ']': 35, '^': 36, '_': 37, '`': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64, '{': 65, '|': 66, '}': 67, '~': 68, '<PADDING>': 69, '<END>': 70}\n"
          ]
        }
      ],
      "source": [
        "index_to_hindi = {k:v for k,v in enumerate(hindi_vocabulary)}\n",
        "hindi_to_index = {v:k for k,v in enumerate(hindi_vocabulary)}\n",
        "index_to_english = {k:v for k,v in enumerate(english_vocabulary)}\n",
        "english_to_index = {v:k for k,v in enumerate(english_vocabulary)}\n",
        "print(english_to_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "rdyjQgdKTN3d"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, Dropout\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, max_seq_len):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "    def call(self, inputs):\n",
        "        pos = np.arange(self.max_seq_len)[:, np.newaxis]\n",
        "        i = np.arange(self.d_model)[np.newaxis, :]\n",
        "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(self.d_model))\n",
        "        angle_rads = pos * angle_rates\n",
        "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "        pos_encoding = angle_rads[np.newaxis, ...]\n",
        "        return tf.constant(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "class SentenceEmbedding(tf.keras.Model):\n",
        "    def __init__(self, max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n",
        "        super(SentenceEmbedding, self).__init__()\n",
        "        self.vocab_size = len(language_to_index)\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.embedding = Embedding(input_dim=self.vocab_size, output_dim=d_model)\n",
        "        self.language_to_index = language_to_index\n",
        "        self.position_encoder = PositionalEncoding(d_model, max_sequence_length)\n",
        "        self.dropout = Dropout(0.1)\n",
        "        self.START_TOKEN = START_TOKEN\n",
        "        self.END_TOKEN = END_TOKEN\n",
        "        self.PADDING_TOKEN = PADDING_TOKEN\n",
        "\n",
        "    def batch_tokenize(self, batch, start_token, end_token):\n",
        "        def tokenize(sentence, start_token, end_token):\n",
        "            sentence_word_indices = [self.language_to_index[token] for token in list(sentence)]\n",
        "            if start_token:\n",
        "                sentence_word_indices.insert(0, self.language_to_index[self.START_TOKEN])\n",
        "            if end_token:\n",
        "                sentence_word_indices.append(self.language_to_index[self.END_TOKEN])\n",
        "            sentence_word_indices.extend([self.language_to_index[self.PADDING_TOKEN]] * (self.max_sequence_length - len(sentence_word_indices)))\n",
        "            return tf.convert_to_tensor(sentence_word_indices, dtype=tf.int32)\n",
        "\n",
        "        tokenized = [tokenize(sentence, start_token, end_token) for sentence in batch]\n",
        "        return tf.stack(tokenized)\n",
        "\n",
        "    def call(self, x, start_token, end_token):\n",
        "        x = self.batch_tokenize(x, start_token, end_token)\n",
        "        x = self.embedding(x)\n",
        "        pos = self.position_encoder(x)\n",
        "        x = self.dropout(x + pos)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def is_valid_tokens(sentence, vocab):\n",
        "    for token in list(set(sentence)):\n",
        "        if token not in vocab:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def is_valid_length(sentence, max_sequence_length):\n",
        "    return len(list(sentence)) < (max_sequence_length - 1) # need to re-add the end token so leaving 1 space\n",
        "\n",
        "valid_english_sentences = []\n",
        "valid_hindi_sentences = []\n",
        "for index in valid_sentence_indicies:\n",
        "    valid_english_sentences.append(english_sentences[index])\n",
        "    valid_hindi_sentences.append(hindi_sentences[index])\n",
        "\n",
        "def batch_tokenize(language_to_index, batch, start_token, end_token, START_TOKEN, END_TOKEN, PADDING_TOKEN, max_sequence_length):\n",
        "    def tokenize(sentence, start_token, end_token):\n",
        "        sentence_word_indices = [language_to_index[token] for token in list(sentence)]\n",
        "        if start_token:\n",
        "            sentence_word_indices.insert(0, language_to_index[START_TOKEN])\n",
        "        if end_token:\n",
        "            sentence_word_indices.append(language_to_index[END_TOKEN])\n",
        "        sentence_word_indices.extend([language_to_index[PADDING_TOKEN]] * (max_sequence_length - len(sentence_word_indices)))\n",
        "        return tf.convert_to_tensor(sentence_word_indices, dtype=tf.int32)\n",
        "\n",
        "    tokenized = [tokenize(sentence, start_token, end_token) for sentence in batch]\n",
        "    return tf.stack(tokenized)\n",
        "\n",
        "x = batch_tokenize(language_to_index = english_to_index, batch = valid_english_sentences[:10], start_token = True, end_token = True,\n",
        "                     START_TOKEN= START_TOKEN, END_TOKEN= END_TOKEN, PADDING_TOKEN=PADDING_TOKEN, max_sequence_length= max_sequence_length)\n",
        "\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJAxtWozZ5Hq",
        "outputId": "338fc6e3-fae3-4517-cdd0-bc5693f89d63"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[ 0 46 53 ... 69 69 69]\n",
            " [ 0 61 46 ... 69 69 69]\n",
            " [ 0 58 46 ... 69 69 69]\n",
            " ...\n",
            " [ 0 48 46 ... 69 69 69]\n",
            " [ 0 39 56 ... 69 69 69]\n",
            " [ 0 40 56 ... 69 69 69]], shape=(10, 250), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cOLH9UaTN3e",
        "outputId": "fe14c6bf-5212-4ac5-d4b4-b58545dc3088"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[ 0.04724257  1.0271591  -0.01443849 ...  0.9656585  -0.01987906\n",
            "    1.0488182 ]\n",
            "  [ 0.8396481   0.5532805   0.8504856  ...  1.0116874  -0.00496451\n",
            "    1.0300165 ]\n",
            "  [ 0.86095643 -0.389178    0.98013884 ...  0.9905458   0.04022251\n",
            "    0.9985762 ]\n",
            "  ...\n",
            "  [ 0.9552097  -0.38166624 -0.42940688 ...  1.034032    0.00436316\n",
            "    1.0489395 ]\n",
            "  [ 0.21315461 -0.9889139   0.49825913 ...  1.0340291   0.00446679\n",
            "    1.0489368 ]\n",
            "  [-0.6987607  -0.69259334  1.0323044  ...  1.0340263   0.00457042\n",
            "    1.0489341 ]]\n",
            "\n",
            " [[ 0.04724257  1.0271591  -0.01443849 ...  0.9656585  -0.01987906\n",
            "    1.0488182 ]\n",
            "  [ 0.8472885   0.5511036   0.85813016 ...  1.0285046  -0.03072091\n",
            "    1.030985  ]\n",
            "  [ 0.9074746  -0.40316865  0.96504414 ...  1.0116874  -0.00486085\n",
            "    1.0300165 ]\n",
            "  ...\n",
            "  [ 0.9552097  -0.38166624 -0.42940688 ...  1.034032    0.00436316\n",
            "    1.0489395 ]\n",
            "  [ 0.21315461 -0.9889139   0.49825913 ...  1.0340291   0.00446679\n",
            "    1.0489368 ]\n",
            "  [-0.6987607  -0.69259334  1.0323044  ...  1.0340263   0.00457042\n",
            "    1.0489341 ]]\n",
            "\n",
            " [[ 0.04724257  1.0271591  -0.01443849 ...  0.9656585  -0.01987906\n",
            "    1.0488182 ]\n",
            "  [ 0.82736105  0.5017593   0.8596636  ...  0.9810187  -0.03783387\n",
            "    1.0323427 ]\n",
            "  [ 0.9074746  -0.40316865  0.96504414 ...  1.0116874  -0.00486085\n",
            "    1.0300165 ]\n",
            "  ...\n",
            "  [ 0.9552097  -0.38166624 -0.42940688 ...  1.034032    0.00436316\n",
            "    1.0489395 ]\n",
            "  [ 0.21315461 -0.9889139   0.49825913 ...  1.0340291   0.00446679\n",
            "    1.0489368 ]\n",
            "  [-0.6987607  -0.69259334  1.0323044  ...  1.0340263   0.00457042\n",
            "    1.0489341 ]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 0.04724257  1.0271591  -0.01443849 ...  0.9656585  -0.01987906\n",
            "    1.0488182 ]\n",
            "  [ 0.8044614   0.52847785  0.8237283  ...  1.0038927   0.03310205\n",
            "    1.0207486 ]\n",
            "  [ 0.9074746  -0.40316865  0.96504414 ...  1.0116874  -0.00486085\n",
            "    1.0300165 ]\n",
            "  ...\n",
            "  [ 0.9552097  -0.38166624 -0.42940688 ...  1.034032    0.00436316\n",
            "    1.0489395 ]\n",
            "  [ 0.21315461 -0.9889139   0.49825913 ...  1.0340291   0.00446679\n",
            "    1.0489368 ]\n",
            "  [-0.6987607  -0.69259334  1.0323044  ...  1.0340263   0.00457042\n",
            "    1.0489341 ]]\n",
            "\n",
            " [[ 0.04724257  1.0271591  -0.01443849 ...  0.9656585  -0.01987906\n",
            "    1.0488182 ]\n",
            "  [ 0.8853671   0.49166435  0.8077857  ...  0.99889284  0.03362932\n",
            "    0.98131514]\n",
            "  [ 0.9564932  -0.3738908   0.89295167 ...  0.9751629  -0.01457378\n",
            "    1.0081122 ]\n",
            "  ...\n",
            "  [ 0.9552097  -0.38166624 -0.42940688 ...  1.034032    0.00436316\n",
            "    1.0489395 ]\n",
            "  [ 0.21315461 -0.9889139   0.49825913 ...  1.0340291   0.00446679\n",
            "    1.0489368 ]\n",
            "  [-0.6987607  -0.69259334  1.0323044  ...  1.0340263   0.00457042\n",
            "    1.0489341 ]]\n",
            "\n",
            " [[ 0.04724257  1.0271591  -0.01443849 ...  0.9656585  -0.01987906\n",
            "    1.0488182 ]\n",
            "  [ 0.87696904  0.5289043   0.7721093  ...  0.97326225  0.02481435\n",
            "    1.0197035 ]\n",
            "  [ 0.9564932  -0.3738908   0.89295167 ...  0.9751629  -0.01457378\n",
            "    1.0081122 ]\n",
            "  ...\n",
            "  [ 0.9552097  -0.38166624 -0.42940688 ...  1.034032    0.00436316\n",
            "    1.0489395 ]\n",
            "  [ 0.21315461 -0.9889139   0.49825913 ...  1.0340291   0.00446679\n",
            "    1.0489368 ]\n",
            "  [-0.6987607  -0.69259334  1.0323044  ...  1.0340263   0.00457042\n",
            "    1.0489341 ]]], shape=(10, 250, 512), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "d_model = 512\n",
        "\n",
        "sentence_embedding = SentenceEmbedding(\n",
        "    max_sequence_length=max_sequence_length,\n",
        "    d_model=d_model,\n",
        "    language_to_index=english_to_index,\n",
        "    START_TOKEN=START_TOKEN,\n",
        "    END_TOKEN=END_TOKEN,\n",
        "    PADDING_TOKEN=PADDING_TOKEN\n",
        ")\n",
        "\n",
        "output = sentence_embedding(valid_english_sentences[:10], start_token=True, end_token=True)\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Layer\n",
        "import math\n",
        "\n",
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    d_k = tf.shape(q)[-1]\n",
        "    scaled = tf.matmul(q, k, transpose_b=True) / tf.math.sqrt(tf.cast(d_k, tf.float32))\n",
        "    if mask is not None:\n",
        "        scaled += mask\n",
        "    attention = tf.nn.softmax(scaled, axis=-1)\n",
        "    values = tf.matmul(attention, v)\n",
        "    return values, attention\n",
        "\n",
        "class MultiHeadAttention(Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.qkv_layer = Dense(3 * d_model)\n",
        "        self.linear_layer = Dense(d_model)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        sequence_length = tf.shape(x)[1]\n",
        "        qkv = self.qkv_layer(x)\n",
        "        qkv = tf.reshape(qkv, (batch_size, sequence_length, self.num_heads, 3 * self.head_dim))\n",
        "        qkv = tf.transpose(qkv, perm=[0, 2, 1, 3])\n",
        "        q, k, v = tf.split(qkv, num_or_size_splits=3, axis=-1)\n",
        "        values, attention = scaled_dot_product(q, k, v, mask)\n",
        "        values = tf.transpose(values, perm=[0, 2, 1, 3])\n",
        "        values = tf.reshape(values, (batch_size, sequence_length, self.num_heads * self.head_dim))\n",
        "        out = self.linear_layer(values)\n",
        "        return out\n",
        "\n",
        "num_heads= 8\n",
        "multihead_attention = MultiHeadAttention(d_model= d_model, num_heads= num_heads)\n",
        "\n",
        "x = multihead_attention(x= output)\n",
        "x"
      ],
      "metadata": {
        "id": "6HJV6kdiT3XN",
        "outputId": "085be661-2b8c-422f-b755-9f87d508f843",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10, 250, 512), dtype=float32, numpy=\n",
              "array([[[ 0.10430601, -0.15512487, -0.26501533, ..., -0.36567742,\n",
              "          0.34704858,  0.5278903 ],\n",
              "        [ 0.10323407, -0.15886356, -0.26329628, ..., -0.3705477 ,\n",
              "          0.34731573,  0.52886975],\n",
              "        [ 0.09900612, -0.16176273, -0.26456174, ..., -0.372053  ,\n",
              "          0.3486197 ,  0.5293998 ],\n",
              "        ...,\n",
              "        [ 0.10559104, -0.1868048 , -0.28306225, ..., -0.37313545,\n",
              "          0.37468496,  0.53041506],\n",
              "        [ 0.10522281, -0.18412626, -0.28762388, ..., -0.3707921 ,\n",
              "          0.3798082 ,  0.5360537 ],\n",
              "        [ 0.10601634, -0.18269604, -0.29249632, ..., -0.37099957,\n",
              "          0.3845331 ,  0.53912956]],\n",
              "\n",
              "       [[ 0.10401697, -0.15576619, -0.2661923 , ..., -0.36693528,\n",
              "          0.34442347,  0.5298821 ],\n",
              "        [ 0.10243075, -0.15941566, -0.26435962, ..., -0.37132376,\n",
              "          0.3444742 ,  0.53077674],\n",
              "        [ 0.09924141, -0.16227666, -0.26582012, ..., -0.37360886,\n",
              "          0.3454441 ,  0.53071606],\n",
              "        ...,\n",
              "        [ 0.10533787, -0.18764277, -0.28421906, ..., -0.37455037,\n",
              "          0.37180626,  0.5321535 ],\n",
              "        [ 0.10498069, -0.18493798, -0.28883988, ..., -0.37216523,\n",
              "          0.37689233,  0.53771096],\n",
              "        [ 0.10581264, -0.18347597, -0.293805  , ..., -0.37230638,\n",
              "          0.38160437,  0.5407988 ]],\n",
              "\n",
              "       [[ 0.10363084, -0.15674648, -0.26392817, ..., -0.36747307,\n",
              "          0.34790528,  0.52489364],\n",
              "        [ 0.10261169, -0.16081099, -0.26259902, ..., -0.37167293,\n",
              "          0.3489993 ,  0.52554375],\n",
              "        [ 0.09913668, -0.16313043, -0.2634685 , ..., -0.3741876 ,\n",
              "          0.3488084 ,  0.52572846],\n",
              "        ...,\n",
              "        [ 0.10484523, -0.188479  , -0.28194454, ..., -0.375182  ,\n",
              "          0.3751869 ,  0.5278347 ],\n",
              "        [ 0.10451768, -0.18577825, -0.28641918, ..., -0.37280768,\n",
              "          0.3802688 ,  0.5334586 ],\n",
              "        [ 0.10537492, -0.1842471 , -0.29128763, ..., -0.37294772,\n",
              "          0.38501334,  0.5365292 ]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[ 0.09824832, -0.15041803, -0.25460166, ..., -0.36911044,\n",
              "          0.34668455,  0.52536535],\n",
              "        [ 0.0981165 , -0.15516002, -0.25351694, ..., -0.3741308 ,\n",
              "          0.34558272,  0.5253241 ],\n",
              "        [ 0.0933841 , -0.15640633, -0.25397107, ..., -0.37542236,\n",
              "          0.34773716,  0.52648   ],\n",
              "        ...,\n",
              "        [ 0.10059144, -0.18280992, -0.27169132, ..., -0.3768907 ,\n",
              "          0.37362328,  0.52919066],\n",
              "        [ 0.10064402, -0.1802318 , -0.27580622, ..., -0.37440538,\n",
              "          0.3786435 ,  0.5346604 ],\n",
              "        [ 0.10185239, -0.17884493, -0.2803287 , ..., -0.37456813,\n",
              "          0.38332868,  0.5376541 ]],\n",
              "\n",
              "       [[ 0.10439259, -0.15626703, -0.2646164 , ..., -0.36621523,\n",
              "          0.34683728,  0.5280617 ],\n",
              "        [ 0.10187294, -0.16085212, -0.26433238, ..., -0.37072855,\n",
              "          0.3486461 ,  0.5290472 ],\n",
              "        [ 0.09939919, -0.16392763, -0.26320386, ..., -0.3728755 ,\n",
              "          0.3479259 ,  0.52839303],\n",
              "        ...,\n",
              "        [ 0.10578378, -0.18877262, -0.28256264, ..., -0.37394804,\n",
              "          0.3738321 ,  0.5306009 ],\n",
              "        [ 0.10545158, -0.18597648, -0.28706503, ..., -0.3715775 ,\n",
              "          0.37881336,  0.5361493 ],\n",
              "        [ 0.10626543, -0.18438172, -0.29197398, ..., -0.37169218,\n",
              "          0.38351756,  0.5391696 ]],\n",
              "\n",
              "       [[ 0.10650609, -0.1589536 , -0.27175057, ..., -0.36768934,\n",
              "          0.34884167,  0.5297485 ],\n",
              "        [ 0.10617776, -0.16389397, -0.27024302, ..., -0.3719255 ,\n",
              "          0.34822172,  0.5293418 ],\n",
              "        [ 0.10176976, -0.1663657 , -0.2705121 , ..., -0.37391287,\n",
              "          0.35013694,  0.53040224],\n",
              "        ...,\n",
              "        [ 0.10824196, -0.19113791, -0.28929752, ..., -0.37594804,\n",
              "          0.3753947 ,  0.5323899 ],\n",
              "        [ 0.10802709, -0.18841141, -0.29360154, ..., -0.37356916,\n",
              "          0.38029528,  0.5377738 ],\n",
              "        [ 0.10894606, -0.18686068, -0.2983756 , ..., -0.3736897 ,\n",
              "          0.38494852,  0.54066217]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalization(tf.keras.layers.Layer):\n",
        "    def __init__(self, parameters_shape, eps=1e-5):\n",
        "        super(LayerNormalization, self).__init__()\n",
        "        self.parameters_shape = parameters_shape\n",
        "        self.eps = eps\n",
        "        self.gamma = self.add_weight(\n",
        "            shape=parameters_shape,\n",
        "            initializer=\"ones\",\n",
        "            trainable=True,\n",
        "            name=\"gamma\"\n",
        "        )\n",
        "        self.beta = self.add_weight(\n",
        "            shape=parameters_shape,\n",
        "            initializer=\"zeros\",\n",
        "            trainable=True,\n",
        "            name=\"beta\"\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        dims = [-i for i in range(1, len(self.parameters_shape) + 1)]\n",
        "        mean = tf.reduce_mean(inputs, axis=dims, keepdims=True)\n",
        "        var = tf.reduce_mean(tf.square(inputs - mean), axis=dims, keepdims=True)\n",
        "        std = tf.sqrt(var + self.eps)\n",
        "        y = (inputs - mean) / std\n",
        "        out = self.gamma * y + self.beta\n",
        "        return out\n",
        "\n",
        "\n",
        "class PositionwiseFeedForward(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.linear1 = tf.keras.layers.Dense(hidden)\n",
        "        self.linear2 = tf.keras.layers.Dense(d_model)\n",
        "        self.relu = tf.keras.layers.ReLU()\n",
        "        self.dropout = tf.keras.layers.Dropout(rate=drop_prob)\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x, training=training)\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate=drop_prob)\n",
        "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate=drop_prob)\n",
        "\n",
        "    def call(self, x, self_attention_mask, training=False):\n",
        "        residual_x = x\n",
        "        x = self.attention(x, mask=self_attention_mask)\n",
        "        x = self.dropout1(x, training=training)\n",
        "        x = self.norm1(x + residual_x)\n",
        "        residual_x = x\n",
        "        x = self.ffn(x, training=training)\n",
        "        x = self.dropout2(x, training=training)\n",
        "        x = self.norm2(x + residual_x)\n",
        "        return x\n",
        "\n",
        "num_heads =8\n",
        "ffn_hidden = 2048\n",
        "encoder_layer = EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob =0.1)\n",
        "\n",
        "x = encoder_layer(x,self_attention_mask= None)\n",
        "x"
      ],
      "metadata": {
        "id": "VyuN9ZL8xXyo",
        "outputId": "2845f51f-6050-472a-8e1d-7f477418202b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10, 250, 512), dtype=float32, numpy=\n",
              "array([[[ 0.5141616 ,  0.44739604,  0.95789105, ..., -0.38501436,\n",
              "         -0.29433343,  1.0783138 ],\n",
              "        [ 0.51609814,  0.44157794,  0.9583447 , ..., -0.39854723,\n",
              "         -0.28630322,  1.0799603 ],\n",
              "        [ 0.5111541 ,  0.43939304,  0.9558624 , ..., -0.40295455,\n",
              "         -0.27511466,  1.0814762 ],\n",
              "        ...,\n",
              "        [ 0.5281283 ,  0.39772686,  0.92455465, ..., -0.3841518 ,\n",
              "         -0.2068237 ,  1.0867946 ],\n",
              "        [ 0.52891725,  0.40803048,  0.9106763 , ..., -0.38007614,\n",
              "         -0.19982453,  1.0962558 ],\n",
              "        [ 0.5351895 ,  0.41350174,  0.8982499 , ..., -0.3815637 ,\n",
              "         -0.19220492,  1.1017783 ]],\n",
              "\n",
              "       [[ 0.5161666 ,  0.44461542,  0.95409596, ..., -0.3861025 ,\n",
              "         -0.30081156,  1.081996  ],\n",
              "        [ 0.51614887,  0.43954962,  0.9559135 , ..., -0.39906582,\n",
              "         -0.29413605,  1.0840089 ],\n",
              "        [ 0.5146126 ,  0.43613815,  0.95089453, ..., -0.40537608,\n",
              "         -0.2837721 ,  1.0829492 ],\n",
              "        ...,\n",
              "        [ 0.53076476,  0.3946734 ,  0.9204986 , ..., -0.38650224,\n",
              "         -0.21495907,  1.0898343 ],\n",
              "        [ 0.5313743 ,  0.4051041 ,  0.90675217, ..., -0.38237575,\n",
              "         -0.20796727,  1.0990525 ],\n",
              "        [ 0.53755957,  0.41074327,  0.8942204 , ..., -0.38371658,\n",
              "         -0.20028606,  1.1044575 ]],\n",
              "\n",
              "       [[ 0.5190984 ,  0.44798055,  0.95815074, ..., -0.3868166 ,\n",
              "         -0.29482445,  1.0667782 ],\n",
              "        [ 0.52066255,  0.44086048,  0.9582891 , ..., -0.39760658,\n",
              "         -0.2847818 ,  1.069947  ],\n",
              "        [ 0.51846844,  0.43925646,  0.9554249 , ..., -0.40615   ,\n",
              "         -0.2775682 ,  1.0675983 ],\n",
              "        ...,\n",
              "        [ 0.5330561 ,  0.39791963,  0.9254916 , ..., -0.38664928,\n",
              "         -0.20805201,  1.0758841 ],\n",
              "        [ 0.5338453 ,  0.4084051 ,  0.91180116, ..., -0.38266   ,\n",
              "         -0.20139118,  1.085243  ],\n",
              "        [ 0.540054  ,  0.41429463,  0.89911246, ..., -0.38417965,\n",
              "         -0.19386356,  1.0906974 ]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[ 0.5255432 ,  0.44895977,  0.9709114 , ..., -0.38462615,\n",
              "         -0.30492857,  1.0794181 ],\n",
              "        [ 0.52818084,  0.44240853,  0.97037435, ..., -0.39929733,\n",
              "         -0.30006003,  1.0792521 ],\n",
              "        [ 0.52369636,  0.44179428,  0.9671041 , ..., -0.4036146 ,\n",
              "         -0.28808913,  1.0809925 ],\n",
              "        ...,\n",
              "        [ 0.5416406 ,  0.3998148 ,  0.93746066, ..., -0.38340938,\n",
              "         -0.21864982,  1.090878  ],\n",
              "        [ 0.54312134,  0.4099053 ,  0.9239216 , ..., -0.37893355,\n",
              "         -0.21182083,  1.1002926 ],\n",
              "        [ 0.54971707,  0.41567007,  0.91173285, ..., -0.38010743,\n",
              "         -0.20418368,  1.1055748 ]],\n",
              "\n",
              "       [[ 0.51447463,  0.44701493,  0.9579887 , ..., -0.3900077 ,\n",
              "         -0.29422137,  1.0768329 ],\n",
              "        [ 0.5141927 ,  0.43904653,  0.95643574, ..., -0.40195188,\n",
              "         -0.28266326,  1.0798559 ],\n",
              "        [ 0.5122067 ,  0.43694818,  0.95930594, ..., -0.40879565,\n",
              "         -0.2752048 ,  1.0771533 ],\n",
              "        ...,\n",
              "        [ 0.52887756,  0.39603615,  0.9245007 , ..., -0.39029446,\n",
              "         -0.20849895,  1.0849502 ],\n",
              "        [ 0.5295718 ,  0.40656978,  0.910676  , ..., -0.3862483 ,\n",
              "         -0.20202835,  1.0942972 ],\n",
              "        [ 0.53582084,  0.4123562 ,  0.8981607 , ..., -0.38753444,\n",
              "         -0.19440858,  1.0997218 ]],\n",
              "\n",
              "       [[ 0.51234955,  0.4443745 ,  0.94490033, ..., -0.39671427,\n",
              "         -0.28848898,  1.0773318 ],\n",
              "        [ 0.51392555,  0.43564245,  0.94633377, ..., -0.40839228,\n",
              "         -0.28224543,  1.0772707 ],\n",
              "        [ 0.5101956 ,  0.4340671 ,  0.9448724 , ..., -0.41467687,\n",
              "         -0.2694616 ,  1.0783902 ],\n",
              "        ...,\n",
              "        [ 0.5283894 ,  0.39546207,  0.9110894 , ..., -0.3972261 ,\n",
              "         -0.20326464,  1.0845041 ],\n",
              "        [ 0.52936625,  0.40566573,  0.89746714, ..., -0.39286104,\n",
              "         -0.19653395,  1.0933716 ],\n",
              "        [ 0.53596747,  0.4115876 ,  0.88507926, ..., -0.39398888,\n",
              "         -0.18915406,  1.0986395 ]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices(valid_english_sentences)\n",
        "batch_size = 32\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "iterator = iter(train_dataset)\n",
        "first_batch = next(iterator)\n",
        "# first_batch = tf.reshape(first_batch, (1,batch_size))\n",
        "first_batch = [sentence.numpy().decode('utf-8') for sentence in first_batch]\n",
        "print(type(first_batch))"
      ],
      "metadata": {
        "id": "UuupeYXLrn_3",
        "outputId": "9a920e7e-0b4a-44af-e12c-ae69bb41868d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SequentialEncoder(tf.keras.Sequential):\n",
        "    def call(self, inputs, self_attention_mask, training=False):\n",
        "        x = inputs\n",
        "        for module in self.layers:\n",
        "            x = module(x, self_attention_mask, training=training)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 ffn_hidden,\n",
        "                 num_heads,\n",
        "                 drop_prob,\n",
        "                 num_layers,\n",
        "                 max_sequence_length,\n",
        "                 language_to_index,\n",
        "                 START_TOKEN,\n",
        "                 END_TOKEN,\n",
        "                 PADDING_TOKEN):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        self.layers = SequentialEncoder([EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n",
        "                                      for _ in range(num_layers)])\n",
        "\n",
        "    def call(self, x, self_attention_mask, start_token, end_token, training=False):\n",
        "        x = self.sentence_embedding(x, start_token, end_token)\n",
        "        x = self.layers(x, self_attention_mask, training=training)\n",
        "        return x\n",
        "drop_prob =0.1\n",
        "encoder = Encoder(d_model =d_model,ffn_hidden=  ffn_hidden,num_heads= num_heads, drop_prob= drop_prob, num_layers= 1, max_sequence_length= max_sequence_length,\n",
        "                  language_to_index= english_to_index, START_TOKEN= START_TOKEN, END_TOKEN= END_TOKEN, PADDING_TOKEN= PADDING_TOKEN)\n",
        "x = encoder(x= first_batch, self_attention_mask= None, start_token= True, end_token= True)\n",
        "x"
      ],
      "metadata": {
        "id": "l3dyVm3ejkU2",
        "outputId": "f136528d-00b1-49da-9188-2c4cbd5b492f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(32, 250, 512), dtype=float32, numpy=\n",
              "array([[[-0.6177005 ,  0.74857634, -0.9057464 , ...,  0.82537603,\n",
              "         -0.9960935 ,  0.5560274 ],\n",
              "        [ 0.85070515, -0.13362451,  0.32034773, ...,  0.5841383 ,\n",
              "         -0.97704524,  0.4606448 ],\n",
              "        [ 1.1444985 , -1.8249949 ,  0.5708078 , ...,  0.413912  ,\n",
              "         -0.98397046,  0.5712344 ],\n",
              "        ...,\n",
              "        [ 1.6469132 , -0.6939828 , -0.6188047 , ...,  0.5604304 ,\n",
              "         -0.9067436 ,  0.5719796 ],\n",
              "        [ 0.8337428 , -1.3771899 ,  0.3314194 , ...,  0.42468733,\n",
              "         -0.7729402 ,  0.74965775],\n",
              "        [-0.36150673, -1.0594146 ,  0.9373097 , ...,  0.35746846,\n",
              "         -0.5790491 ,  0.97050303]],\n",
              "\n",
              "       [[-0.62151307,  0.74753827, -0.9074398 , ...,  0.82512015,\n",
              "         -0.9949043 ,  0.55768436],\n",
              "        [ 0.8235764 , -0.24716254,  0.4228808 , ...,  0.6315357 ,\n",
              "         -1.0490125 ,  0.5826833 ],\n",
              "        [ 1.0981297 , -1.7522687 ,  0.4499316 , ...,  0.2865526 ,\n",
              "         -0.9363413 ,  0.44683152],\n",
              "        ...,\n",
              "        [ 1.6439115 , -0.6952328 , -0.62069994, ...,  0.561348  ,\n",
              "         -0.90625316,  0.57356495],\n",
              "        [ 0.83100414, -1.3782984 ,  0.32945448, ...,  0.425676  ,\n",
              "         -0.772617  ,  0.75069046],\n",
              "        [-0.36424893, -1.0602605 ,  0.93504655, ...,  0.35828212,\n",
              "         -0.57865405,  0.9719551 ]],\n",
              "\n",
              "       [[-0.61717796,  0.7483116 , -0.9046489 , ...,  0.82466614,\n",
              "         -0.9938362 ,  0.55519366],\n",
              "        [ 0.86404574, -0.25747865,  0.43954036, ...,  0.6429651 ,\n",
              "         -1.0088493 ,  0.5769955 ],\n",
              "        [ 1.1014005 , -1.7506021 ,  0.45178542, ...,  0.2850281 ,\n",
              "         -0.9346504 ,  0.44539   ],\n",
              "        ...,\n",
              "        [ 1.6468438 , -0.6955017 , -0.617988  , ...,  0.5603893 ,\n",
              "         -0.9054977 ,  0.5722273 ],\n",
              "        [ 0.8337712 , -1.3783406 ,  0.3327439 , ...,  0.42521456,\n",
              "         -0.7720459 ,  0.74906987],\n",
              "        [-0.361234  , -1.0608186 ,  0.93813854, ...,  0.35769257,\n",
              "         -0.5772541 ,  0.9692833 ]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[-0.6154444 ,  0.7437049 , -0.9122571 , ...,  0.8232053 ,\n",
              "         -0.9915998 ,  0.5510202 ],\n",
              "        [ 0.8667707 , -0.2626563 ,  0.43134055, ...,  0.64535975,\n",
              "         -1.0077957 ,  0.56695044],\n",
              "        [ 1.1049578 , -1.7540674 ,  0.44245425, ...,  0.2850455 ,\n",
              "         -0.9356053 ,  0.43586162],\n",
              "        ...,\n",
              "        [ 1.6472489 , -0.6925589 , -0.6256207 , ...,  0.55925757,\n",
              "         -0.9030924 ,  0.562191  ],\n",
              "        [ 0.83342236, -1.3759159 ,  0.32483876, ...,  0.42430648,\n",
              "         -0.76939636,  0.7391185 ],\n",
              "        [-0.36151946, -1.0578302 ,  0.93181235, ...,  0.3588493 ,\n",
              "         -0.5758511 ,  0.9613046 ]],\n",
              "\n",
              "       [[-0.61991274,  0.7574223 , -0.9014136 , ...,  0.82335365,\n",
              "         -0.9882904 ,  0.5611566 ],\n",
              "        [ 0.84028673, -0.0549822 ,  0.38954863, ...,  0.6933117 ,\n",
              "         -1.0312419 ,  0.5239767 ],\n",
              "        [ 1.0528673 , -1.7679296 ,  0.46072173, ...,  0.42965963,\n",
              "         -0.95245725,  0.5112341 ],\n",
              "        ...,\n",
              "        [ 1.6497608 , -0.6994542 , -0.61558473, ...,  0.55876786,\n",
              "         -0.90171987,  0.5836948 ],\n",
              "        [ 0.8381665 , -1.3829368 ,  0.33633855, ...,  0.42298868,\n",
              "         -0.76745003,  0.7623872 ],\n",
              "        [-0.35834774, -1.0665107 ,  0.9400434 , ...,  0.3524163 ,\n",
              "         -0.5716277 ,  0.98155254]],\n",
              "\n",
              "       [[-0.620402  ,  0.75769746, -0.9006817 , ...,  0.8248717 ,\n",
              "         -0.98879516,  0.560896  ],\n",
              "        [ 0.8974548 , -0.2486622 ,  0.3837988 , ...,  0.57588446,\n",
              "         -1.0025349 ,  0.46617723],\n",
              "        [ 1.080282  , -1.8014354 ,  0.56189066, ...,  0.35868415,\n",
              "         -0.9403585 ,  0.46591556],\n",
              "        ...,\n",
              "        [ 1.6492168 , -0.6987744 , -0.61478144, ...,  0.5588933 ,\n",
              "         -0.9020937 ,  0.58327955],\n",
              "        [ 0.8375361 , -1.3822991 ,  0.33696178, ...,  0.42318237,\n",
              "         -0.7678864 ,  0.76214576],\n",
              "        [-0.35898829, -1.0660526 ,  0.9407951 , ...,  0.35270208,\n",
              "         -0.5722876 ,  0.98125947]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadCrossAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadCrossAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.kv_layer = Dense(2 * d_model)\n",
        "        self.q_layer = Dense(d_model)\n",
        "        self.linear_layer = Dense(d_model)\n",
        "\n",
        "    def call(self, x, y, mask):\n",
        "        batch_size, sequence_length, d_model = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2]\n",
        "        kv = self.kv_layer(x)\n",
        "        q = self.q_layer(y)\n",
        "        kv = tf.reshape(kv, (batch_size, sequence_length, self.num_heads, 2 * self.head_dim))\n",
        "        q = tf.reshape(q, (batch_size, sequence_length, self.num_heads, self.head_dim))\n",
        "        kv = tf.transpose(kv, perm=[0, 2, 1, 3])\n",
        "        q = tf.transpose(q, perm=[0, 2, 1, 3])\n",
        "        k, v = tf.split(kv, 2, axis=-1)\n",
        "        values, attention = scaled_dot_product(q, k, v, mask)\n",
        "        values = tf.transpose(values, perm=[0, 2, 1, 3])\n",
        "        values = tf.reshape(values, (batch_size, sequence_length, d_model))\n",
        "        out = self.linear_layer(values)\n",
        "        return out\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attention = MultiHeadAttention(num_heads=num_heads, d_model=d_model)\n",
        "        self.layer_norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout1 = Dropout(drop_prob)\n",
        "\n",
        "        self.encoder_decoder_attention = MultiHeadCrossAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.layer_norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout2 = Dropout(drop_prob)\n",
        "\n",
        "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "        self.layer_norm3 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout3 = Dropout(drop_prob)\n",
        "\n",
        "    def call(self, x, y, self_attention_mask, cross_attention_mask):\n",
        "        _y = y\n",
        "        y = self.self_attention(y, mask=self_attention_mask)\n",
        "        y = self.dropout1(y)\n",
        "        y = self.layer_norm1(y + _y)\n",
        "\n",
        "        _y = y\n",
        "        y = self.encoder_decoder_attention(x, y, mask=cross_attention_mask)\n",
        "        y = self.dropout2(y)\n",
        "        y = self.layer_norm2(y + _y)\n",
        "\n",
        "        _y = y\n",
        "        y = self.ffn(y)\n",
        "        y = self.dropout3(y)\n",
        "        y = self.layer_norm3(y + _y)\n",
        "        return y\n",
        "\n",
        "    def get_sub_layers(self):\n",
        "        return [self.self_attention, self.encoder_decoder_attention, self.ffn, self.layer_norm1, self.layer_norm2, self.layer_norm3]\n",
        "\n",
        "\n",
        "\n",
        "class SequentialDecoder(tf.keras.Sequential):\n",
        "    def call(self, *inputs):\n",
        "        x, y, self_attention_mask, cross_attention_mask = inputs\n",
        "        for layer in self.layers:\n",
        "            y = layer(x, y, self_attention_mask, cross_attention_mask)\n",
        "        return y\n",
        "\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 ffn_hidden,\n",
        "                 num_heads,\n",
        "                 drop_prob,\n",
        "                 num_layers,\n",
        "                 max_sequence_length,\n",
        "                 language_to_index,\n",
        "                 START_TOKEN,\n",
        "                 END_TOKEN,\n",
        "                 PADDING_TOKEN):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        self.layers = SequentialDecoder([DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)])\n",
        "\n",
        "    def call(self, x, y, self_attention_mask=None, cross_attention_mask=None, start_token=True, end_token=True):\n",
        "        y = self.sentence_embedding(y, start_token, end_token)\n",
        "        y = self.layers(x, y, self_attention_mask, cross_attention_mask)\n",
        "        return y\n",
        "\n",
        "    def get_layers(self):\n",
        "        return self.layers\n",
        "\n",
        "\n",
        "class Transformer(tf.keras.layers.Layer):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 ffn_hidden,\n",
        "                 num_heads,\n",
        "                 drop_prob,\n",
        "                 num_layers,\n",
        "                 max_sequence_length,\n",
        "                 hi_vocab_size,\n",
        "                 english_to_index,\n",
        "                 hindi_to_index,\n",
        "                 START_TOKEN,\n",
        "                 END_TOKEN,\n",
        "                 PADDING_TOKEN):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, english_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        self.decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, hindi_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        self.linear = Dense(hi_vocab_size)\n",
        "        self.device = tf.config.experimental.list_physical_devices('GPU')\n",
        "\n",
        "    def call(self,\n",
        "             x,\n",
        "             y,\n",
        "             encoder_self_attention_mask=None,\n",
        "             decoder_self_attention_mask=None,\n",
        "             decoder_cross_attention_mask=None,\n",
        "             enc_start_token=False,\n",
        "             enc_end_token=False,\n",
        "             dec_start_token=False, # We should make this true\n",
        "             dec_end_token=False): # x, y are batch of sentences\n",
        "        x = self.encoder(x, encoder_self_attention_mask, start_token=enc_start_token, end_token=enc_end_token)\n",
        "        out = self.decoder(x, y, decoder_self_attention_mask, decoder_cross_attention_mask, start_token=dec_start_token, end_token=dec_end_token)\n",
        "        out = self.linear(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "ANiiVZ48Ei__"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(valid_hindi_sentences[:10])\n",
        "print(valid_english_sentences[:10])"
      ],
      "metadata": {
        "id": "gzGLHvACAqnf",
        "outputId": "58b2bc66-a083-485d-e09a-1d61122a0afc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['आस्ट्रेलिया के पाल हेनली के साथ जोड़ी बनाने वाले पेस मियामी में क्वार्टरफाइनल तक ही पहुंच सके क्योंकि इस दौर में उन्हें भूपति और नोल्स ने हराया था।', 'और जो शख्स (अपने आमाल का) बदला दुनिया ही में चाहता है तो ख़ुदा के पास दुनिया व आख़िरत दोनों का अज्र मौजूद है और ख़ुदा तो हर शख्स की सुनता और सबको देखता है', 'जैव-मंडल में कीड़ों का मूल्य बहुत है, क्योंकि प्रजातियों की समृद्धि के मामले में उनकी संख्या अन्य जीव समूहों से ज़्यादा है।', 'आस्ट्रेलिया के खिलाफ वनडे टीम की कमान मिताली को', '8 सितम्\\u200dबर, 2016 को माननीय राष्\\u200dट्रपति की स्\\u200dवीकृति मिलने के बाद 101वां संविधान संशोधन अधिनियम, 2016 अस्तित्\\u200dव में आया', 'अदालत ने इस मामले में आगे की सुनवाई के लिए एक फरवरी की तारीख़ तय की', 'जहाँ पर ट्रैक को विभाजित किया जाना है, कृपया वह स्थान चुनें.', 'झारखंड के मुख्यमंत्री हेमंत सोरेन (फोटोः पीटीआई)', 'सेक्टर 55/56 के एसएचओ अरविंद कुमार ने बताया कि इस मामले में आईपीसी की धारा 376-डी (गैंगरेप) के तहत मामला दर्ज कर लिया गया है।', 'आज नई दिल्ली में मीडिया से बातचीत में पार्टी के राज्य प्रभारी अनिल जैन ने बताया कि बैठक के बाद पार्टी, सरकार के गठन का दावा पेश करने के लिए राज्यपाल से मिलेगी।']\n",
            "[\"however, paes, who was partnering australia's paul hanley, could only go as far as the quarterfinals where they lost to bhupathi and knowles\", 'whosoever desires the reward of the world, with allah is the reward of the world and of the everlasting life. allah is the hearer, the seer.', 'the value of insects in the biosphere is enormous because they outnumber all other living groups in measure of species richness.', 'mithali to anchor indian team against australia in odis', 'after the assent of the honble president on 8thseptember, 2016, the 101thconstitutional amendment act, 2016 came into existence', 'the court has fixed a hearing for february 12', 'please select the position where the track should be split.', 'jharkhand chief minister hemant soren', 'arvind kumar, sho of the sector 55/56 police station, said a case has been registered under section 376-d (gang rape) of the indian penal code.', \"briefing media in new delhi today, party's state-in-charge, anil jain said, after the meeting the party will meet the governor to stake claim for government formation in the state.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "\n",
        "# class TextDataset(tf.data.Dataset):\n",
        "#     def _generator(english_sentences, hindi_sentences):\n",
        "#         for eng, hi in zip(english_sentences, hindi_sentences):\n",
        "#             yield eng, hi\n",
        "\n",
        "#     def __new__(cls, english_sentences, hindi_sentences):\n",
        "#         return tf.data.Dataset.from_generator(\n",
        "#             cls._generator,\n",
        "#             output_signature=(\n",
        "#                 tf.TensorSpec(shape=(), dtype=tf.string),\n",
        "#                 tf.TensorSpec(shape=(), dtype=tf.string)\n",
        "#             ),\n",
        "#             args=(english_sentences, hindi_sentences)\n",
        "#         )\n",
        "\n",
        "\n",
        "# dataset = TextDataset(valid_english_sentences, valid_hindi_sentences)\n",
        "# dataset = dataset.batch(batch_size)\n",
        "\n",
        "\n",
        "# batches_list = []\n",
        "# for batch in dataset:\n",
        "#     batch_list = []\n",
        "#     batch_list_eng = []\n",
        "#     batch_list_kan = []\n",
        "#     for eng, kan in zip(batch[0], batch[1]):\n",
        "#         batch_list_eng.append(eng.numpy().decode(\"utf-8\"))\n",
        "#         batch_list_kan.append(kan.numpy().decode(\"utf-8\"))\n",
        "#     batch_list.append(tuple(batch_list_eng))\n",
        "#     batch_list.append(tuple(batch_list_kan))\n",
        "#     batches_list.append(batch_list)\n",
        "\n",
        "# batches_list[0]"
      ],
      "metadata": {
        "id": "n9bZwJh54wbJ"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class IndexedTextDataset:\n",
        "#     def __init__(self, english_sentences, hindi_sentences):\n",
        "#         self.english_sentences = english_sentences\n",
        "#         self.hindi_sentences = hindi_sentences\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.english_sentences)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         return self.english_sentences[idx], self.hindi_sentences[idx]\n",
        "\n",
        "# def batch_data(dataset, batch_size):\n",
        "#     \"\"\"Yield batches of data from the dataset.\"\"\"\n",
        "#     num_samples = len(dataset)\n",
        "#     for start_idx in range(0, num_samples, batch_size):\n",
        "#         end_idx = min(start_idx + batch_size, num_samples)\n",
        "#         batch = [dataset[i] for i in range(start_idx, end_idx)]\n",
        "#         yield batch\n",
        "\n",
        "# dataset = IndexedTextDataset(valid_english_sentences, valid_hindi_sentences)\n",
        "# dataset = list(batch_data(dataset, batch_size))\n",
        "# # Accessing a specific item\n",
        "# print(dataset[0])\n"
      ],
      "metadata": {
        "id": "I8PDIynXG8IT"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "\n",
        "# class TextDataset(tf.data.Dataset):\n",
        "#     def _generator(english_sentences, kannada_sentences):\n",
        "#         for eng, kan in zip(english_sentences, kannada_sentences):\n",
        "#             yield eng, kan\n",
        "\n",
        "#     def __new__(cls, english_sentences, kannada_sentences):\n",
        "#         return tf.data.Dataset.from_generator(\n",
        "#             cls._generator,\n",
        "#             output_signature=(\n",
        "#                 tf.TensorSpec(shape=(), dtype=tf.string),\n",
        "#                 tf.TensorSpec(shape=(), dtype=tf.string)\n",
        "#             ),\n",
        "#             args=(english_sentences, kannada_sentences)\n",
        "#         )\n",
        "\n",
        "# # Example sentences (you can replace these with your actual sentences)\n",
        "# english_sentences = [\"Hello\", \"How are you?\", \"Good morning\", \"This is a test\", \"Hope you're well\"] * 10  # Repeat to have enough for batching\n",
        "# kannada_sentences = [\"ನಮಸ್ಕಾರ\", \"ನೀವು ಹೇಗಿದ್ದೀರಾ?\", \"ಶುಭೋದಯ\", \"ಇದು ಪರೀಕ್ಷೆ\", \"ನೀವು ಚೆನ್ನಾಗಿದ್ದೀರಾ ಎಂದು ಭಾವಿಸುತ್ತೇವೆ\"] * 10  # Repeat to have enough for batching\n",
        "\n",
        "# dataset = TextDataset(english_sentences, kannada_sentences)\n",
        "\n",
        "# # Batch the dataset\n",
        "# batch_size = 32\n",
        "# dataset = dataset.batch(batch_size)\n",
        "\n",
        "# # Example usage: Print the first batch\n",
        "# for batch in dataset.take(1):\n",
        "#     for eng, kan in zip(batch[0].numpy(), batch[1].numpy()):\n",
        "#         print(f'English: {eng.decode(\"utf-8\")}, Kannada: {kan.decode(\"utf-8\")}')\n"
      ],
      "metadata": {
        "id": "2adOkXb3ALYJ"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512\n",
        "batch_size = 30\n",
        "ffn_hidden = 2048\n",
        "num_heads = 8\n",
        "drop_prob = 0.1\n",
        "num_layers = 1\n",
        "max_sequence_length = 200\n",
        "kn_vocab_size = len(hindi_vocabulary)\n",
        "\n",
        "transformer = Transformer(d_model,\n",
        "                          ffn_hidden,\n",
        "                          num_heads,\n",
        "                          drop_prob,\n",
        "                          num_layers,\n",
        "                          max_sequence_length,\n",
        "                          kn_vocab_size,\n",
        "                          english_to_index,\n",
        "                          hindi_to_index,\n",
        "                          START_TOKEN,\n",
        "                          END_TOKEN,\n",
        "                          PADDING_TOKEN)\n",
        ""
      ],
      "metadata": {
        "id": "ko9cfDv15qk8"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss function with ignore_index equivalent\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True,\n",
        "    reduction='none'\n",
        ")\n",
        "\n",
        "# Define a custom loss function to ignore padding tokens\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.not_equal(real, hindi_to_index[PADDING_TOKEN])\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\n",
        "\n",
        "initializer = tf.keras.initializers.GlorotUniform()\n",
        "\n",
        "# Iterate over the model's layers and initialize weights with Xavier uniform initializer\n",
        "# for layer in transformer.encoder.layer.layers:\n",
        "#     if isinstance(layer, tf.keras.layers.Dense):\n",
        "#         layer.kernel_initializer = initializer\n",
        "#         # To reinitialize the kernel with the new initializer\n",
        "#         layer.build(layer.input_shape)\n",
        "\n",
        "# for layer in transformer.decoder.layer.layers:\n",
        "#     if isinstance(layer, tf.keras.layers.Dense):\n",
        "#         layer.kernel_initializer = initializer\n",
        "#         # To reinitialize the kernel with the new initializer\n",
        "#         layer.build(layer.input_shape)\n",
        "\n",
        "\n",
        "# Initialize layers in the decoder\n",
        "# for decoder_layer in transformer.decoder.layers:\n",
        "#     if isinstance(decoder_layer, DecoderLayer):\n",
        "#         for sub_layer in decoder_layer.get_sub_layers():\n",
        "#             if isinstance(sub_layer, tf.keras.layers.Dense):\n",
        "#                 sub_layer.kernel_initializer = initializer\n",
        "#                 # Rebuild the layer to apply the new initializer\n",
        "#                 sub_layer.build(sub_layer.input_shape)\n",
        "\n",
        "# Initialize layers in the encoder (assuming similar structure)\n",
        "# for layer in transformer.encoder.get_layers():\n",
        "#     if isinstance(layer, DecoderLayer):\n",
        "#         for sub_layer in layer.get_sub_layers():\n",
        "#             if isinstance(sub_layer, tf.keras.layers.Dense):\n",
        "#                 sub_layer.kernel_initializer = initializer\n",
        "#                 sub_layer.build(sub_layer.input_shape)  # Rebuild the layer to apply the new initializer\n",
        "\n",
        "type(transformer.decoder.layers)\n"
      ],
      "metadata": {
        "id": "t1B9Bg9l4Ub6",
        "outputId": "db43f6da-fecd-43c8-abf0-24dee5f03e94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        }
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "__main__.SequentialDecoder"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>SequentialDecoder</b><br/>def error_handler(*args, **kwargs)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\"></a>`Sequential` groups a linear stack of layers into a `tf.keras.Model`.\n",
              "\n",
              "`Sequential` provides training and inference features on this model.\n",
              "\n",
              "Examples:\n",
              "\n",
              "```python\n",
              "model = tf.keras.Sequential()\n",
              "model.add(tf.keras.Input(shape=(16,)))\n",
              "model.add(tf.keras.layers.Dense(8))\n",
              "\n",
              "# Note that you can also omit the initial `Input`.\n",
              "# In that case the model doesn&#x27;t have any weights until the first call\n",
              "# to a training/evaluation method (since it isn&#x27;t yet built):\n",
              "model = tf.keras.Sequential()\n",
              "model.add(tf.keras.layers.Dense(8))\n",
              "model.add(tf.keras.layers.Dense(4))\n",
              "# model.weights not created yet\n",
              "\n",
              "# Whereas if you specify an `Input`, the model gets built\n",
              "# continuously as you are adding layers:\n",
              "model = tf.keras.Sequential()\n",
              "model.add(tf.keras.Input(shape=(16,)))\n",
              "model.add(tf.keras.layers.Dense(4))\n",
              "len(model.weights)\n",
              "# Returns &quot;2&quot;\n",
              "\n",
              "# When using the delayed-build pattern (no input shape specified), you can\n",
              "# choose to manually build your model by calling\n",
              "# `build(batch_input_shape)`:\n",
              "model = tf.keras.Sequential()\n",
              "model.add(tf.keras.layers.Dense(8))\n",
              "model.add(tf.keras.layers.Dense(4))\n",
              "model.build((None, 16))\n",
              "len(model.weights)\n",
              "# Returns &quot;4&quot;\n",
              "\n",
              "# Note that when using the delayed-build pattern (no input shape specified),\n",
              "# the model gets built the first time you call `fit`, `eval`, or `predict`,\n",
              "# or the first time you call the model on some input data.\n",
              "model = tf.keras.Sequential()\n",
              "model.add(tf.keras.layers.Dense(8))\n",
              "model.add(tf.keras.layers.Dense(1))\n",
              "model.compile(optimizer=&#x27;sgd&#x27;, loss=&#x27;mse&#x27;)\n",
              "# This builds the model for the first time:\n",
              "model.fit(x, y, batch_size=32, epochs=10)\n",
              "```</pre></div>"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
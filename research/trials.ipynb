{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shorya777/Neural_Machine_Translation_from_scratch/blob/master/research/trials.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wg510zXyTcgU",
        "outputId": "d4d67036-ad0b-45b9-f107-ce3a160607fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/parvmodi/english-to-hindi-machine-translation-dataset\n",
            "License(s): CC0-1.0\n",
            "Downloading english-to-hindi-machine-translation-dataset.zip to /content\n",
            " 99% 1.03G/1.04G [00:20<00:00, 88.2MB/s]\n",
            "100% 1.04G/1.04G [00:20<00:00, 54.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d parvmodi/english-to-hindi-machine-translation-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d8aXkDvTwHb",
        "outputId": "02132e63-1eba-45bf-e042-71bd55b785f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  english-to-hindi-machine-translation-dataset.zip\n",
            "  inflating: train.en                \n",
            "  inflating: train.hi                \n"
          ]
        }
      ],
      "source": [
        "!unzip english-to-hindi-machine-translation-dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-lrXpDF5kgx",
        "outputId": "f9519a1d-4a6d-4558-dcba-9cfdfa14cbba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10125706\n",
            "[\"However, Paes, who was partnering Australia's Paul Hanley, could only go as far as the quarterfinals where they lost to Bhupathi and Knowles\\n\", 'Whosoever desires the reward of the world, with Allah is the reward of the world and of the Everlasting Life. Allah is the Hearer, the Seer.\\n', 'The value of insects in the biosphere is enormous because they outnumber all other living groups in measure of species richness.\\n', 'Mithali To Anchor Indian Team Against Australia in ODIs\\n', 'After the assent of the Honble President on 8thSeptember, 2016, the 101thConstitutional Amendment Act, 2016 came into existence\\n']\n",
            "10125706\n",
            "['आस्ट्रेलिया के पाल हेनली के साथ जोड़ी बनाने वाले पेस मियामी में क्वार्टरफाइनल तक ही पहुंच सके क्योंकि इस दौर में उन्हें भूपति और नोल्स ने हराया था।\\n', 'और जो शख्स (अपने आमाल का) बदला दुनिया ही में चाहता है तो ख़ुदा के पास दुनिया व आख़िरत दोनों का अज्र मौजूद है और ख़ुदा तो हर शख्स की सुनता और सबको देखता है\\n', 'जैव-मंडल में कीड़ों का मूल्य बहुत है, क्योंकि प्रजातियों की समृद्धि के मामले में उनकी संख्या अन्य जीव समूहों से ज़्यादा है।\\n', 'आस्ट्रेलिया के खिलाफ वनडे टीम की कमान मिताली को\\n', '8 सितम्\\u200dबर, 2016 को माननीय राष्\\u200dट्रपति की स्\\u200dवीकृति मिलने के बाद 101वां संविधान संशोधन अधिनियम, 2016 अस्तित्\\u200dव में आया\\n']\n"
          ]
        }
      ],
      "source": [
        "filepath_en = 'train.en'\n",
        "filepath_hi = 'train.hi'\n",
        "\n",
        "with open(filepath_en, encoding='utf-8') as file:\n",
        "    lines_en = file.readlines()\n",
        "print(len(lines_en))\n",
        "print(lines_en[:5])\n",
        "\n",
        "with open(filepath_hi, encoding='utf-8') as file:\n",
        "    lines_hi = file.readlines()\n",
        "\n",
        "print(len(lines_hi))\n",
        "print(lines_hi[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUOig1EV5kgx",
        "outputId": "6985871a-a31f-4764-966b-83104968b45a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"however, paes, who was partnering australia's paul hanley, could only go as far as the quarterfinals where they lost to bhupathi and knowles\", 'whosoever desires the reward of the world, with allah is the reward of the world and of the everlasting life. allah is the hearer, the seer.', 'the value of insects in the biosphere is enormous because they outnumber all other living groups in measure of species richness.', 'mithali to anchor indian team against australia in odis', 'after the assent of the honble president on 8thseptember, 2016, the 101thconstitutional amendment act, 2016 came into existence']\n",
            "================================\n",
            "['आस्ट्रेलिया के पाल हेनली के साथ जोड़ी बनाने वाले पेस मियामी में क्वार्टरफाइनल तक ही पहुंच सके क्योंकि इस दौर में उन्हें भूपति और नोल्स ने हराया था।', 'और जो शख्स (अपने आमाल का) बदला दुनिया ही में चाहता है तो ख़ुदा के पास दुनिया व आख़िरत दोनों का अज्र मौजूद है और ख़ुदा तो हर शख्स की सुनता और सबको देखता है', 'जैव-मंडल में कीड़ों का मूल्य बहुत है, क्योंकि प्रजातियों की समृद्धि के मामले में उनकी संख्या अन्य जीव समूहों से ज़्यादा है।', 'आस्ट्रेलिया के खिलाफ वनडे टीम की कमान मिताली को', '8 सितम्\\u200dबर, 2016 को माननीय राष्\\u200dट्रपति की स्\\u200dवीकृति मिलने के बाद 101वां संविधान संशोधन अधिनियम, 2016 अस्तित्\\u200dव में आया']\n"
          ]
        }
      ],
      "source": [
        "TOTAL_SENTENCES = 200000\n",
        "english_sentences = lines_en[:TOTAL_SENTENCES]\n",
        "hindi_sentences = lines_hi[:TOTAL_SENTENCES]\n",
        "english_sentences = [sentence.rstrip('\\n').lower() for sentence in english_sentences]\n",
        "hindi_sentences = [sentence.rstrip('\\n') for sentence in hindi_sentences]\n",
        "\n",
        "print(english_sentences[:5])\n",
        "print(\"================================\")\n",
        "print(hindi_sentences[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OOIXt1O_BAv",
        "outputId": "512581c2-acca-4f56-91e4-5984d210d460"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "97th percentile length Kannada: 258.0\n",
            "97th percentile length English: 267.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "PERCENTILE = 97\n",
        "print( f\"{PERCENTILE}th percentile length Kannada: {np.percentile([len(x) for x in hindi_sentences], PERCENTILE)}\" )\n",
        "print( f\"{PERCENTILE}th percentile length English: {np.percentile([len(x) for x in english_sentences], PERCENTILE)}\" )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drEPgCVEDAMJ",
        "outputId": "ecfe1c32-2b5c-4c37-dd7d-2db585d2e2ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "71\n"
          ]
        }
      ],
      "source": [
        "START_TOKEN = '<START>'\n",
        "PADDING_TOKEN = '<PADDING>'\n",
        "END_TOKEN = '<END>'\n",
        "\n",
        "english_vocabulary = [START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/',\n",
        "                        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "                        ':', '<', '=', '>', '?', '@',\n",
        "                        '[', '\\' ', ']', '^', '_', '`',\n",
        "                        'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
        "                        'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x',\n",
        "                        'y', 'z',\n",
        "                        '{', '|', '}', '~', PADDING_TOKEN, END_TOKEN]\n",
        "\n",
        "hindi_vocabulary = [START_TOKEN, 'अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ए', 'ऐ', 'ओ', 'औ',\n",
        "    'क', 'ख', 'ग', 'घ', 'ङ','ड़', 'च', 'छ', 'ज', 'झ', 'ञ',\n",
        "    'ट', 'ठ', 'ड', 'ढ', 'ण', 'त', 'थ', 'द', 'ध', 'न',\n",
        "    'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ल', 'व', 'श',\n",
        "    'ष', 'स', 'ह', '़', 'ा', 'ि', 'ी', 'ु', 'ू', 'े', 'ै', 'ो', 'ौ',\n",
        "    'ं', 'ः', 'ँ', '्', '०', '१', '२', '३', '४', '५', '६', '७', '८', '९', '|',\n",
        "    ':', '<', '=', '>', '?', '@',\n",
        "    '[', '\\' ', ']', '^', '_', '`',\n",
        "    ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/',\n",
        "    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9','\\u200d','।', 'ृ',\n",
        "    PADDING_TOKEN, END_TOKEN\n",
        "]\n",
        "\n",
        "print(len(english_vocabulary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qpZWhY7AA5K",
        "outputId": "9594a92c-9bab-4836-de6f-d3613d1e555e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences: 200000\n",
            "Number of valid sentences: 151260\n"
          ]
        }
      ],
      "source": [
        "max_sequence_length = 250\n",
        "\n",
        "def is_valid_tokens(sentence, vocab):\n",
        "    for token in list(set(sentence)):\n",
        "        if token not in vocab:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def is_valid_length(sentence, max_sequence_length):\n",
        "    return len(list(sentence)) < (max_sequence_length - 1) # need to re-add the end token so leaving 1 space\n",
        "\n",
        "valid_sentence_indicies = []\n",
        "for index in range(len(hindi_sentences)):\n",
        "    hindi_sentence, english_sentence = hindi_sentences[index], english_sentences[index]\n",
        "    if is_valid_length(hindi_sentence, max_sequence_length) \\\n",
        "      and is_valid_length(english_sentence, max_sequence_length) \\\n",
        "      and is_valid_tokens(hindi_sentence, hindi_vocabulary):\n",
        "        valid_sentence_indicies.append(index)\n",
        "\n",
        "print(f\"Number of sentences: {len(hindi_sentences)}\")\n",
        "print(f\"Number of valid sentences: {len(valid_sentence_indicies)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMN2l2qPHqq6",
        "outputId": "83607e33-adde-4d12-ba94-7569bbcf4ba3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Characters in the sentences not in the vocabulary: {'H', 'ְ', '™', '♦', 'ˌ', 'ಳ', 'ා', 'x', 'ö', 'ɵ', 'ต', 'ς', '․', 'ع', 'λ', 'ऌ', 'ॅ', 'ু', 'ច', '॥', 'è', '್', '□', '′', '章', 'M', 'و', 'コ', 'X', 'à', '≈', '\\uf0d8', '˜', 'ط', '西', 'ă', '\\ue208', 'ড', 'з', 'ف', 'ī', 'ల', 'М', '班', 'ч', '~', 'ு', 'ක', ';', 'w', 'ො', '੍', '“', 'ί', 'ದ', 's', 'জ', '\\ue5a0', 'ನ', 'б', 'ម', '£', 'র', 'j', 'ে', 'ы', '공', 'ॄ', 'ᒡ', 'ය', '්', '−', 't', 'ー', '記', '經', 'ห', 'ុ', 'q', '§', '☞', 'ว', 'כ', '한', '♪', 'ි', 'ॠ', 'Ň', 'ʔ', 'ৱ', 'パ', '女', 'ோ', 'ē', 'þ', 'ਦ', 'ุ', '°', 'ּ', 'ǔ', 'ॉ', 'Я', 'r', 'ย', 'ন', '\\uf146', 'Э', 'ำ', 'ை', 'ó', 'ਤ', 'ඩ', 'Ż', '日', 'ę', 'ු', 'ʁ', 'â', 'ស', 'ර', 'க', 'ऱ', '½', 'c', '☺', 'េ', 'Z', 'ল', 'ס', 'క', 'த', 'י', 'ಿ', '›', 'é', 'ត', 'R', '◆', '਼', '্', '의', 'ص', '்', 'υ', 'ғ', 'ऽ', 'ä', '❑', 'S', 'ල', 'ָ', '\\ue205', '郑', 'ன', 'Y', 'พ', 'æ', '–', '¶', 'শ', '夕', '́', 'ও', 'J', 'ಕ', 'o', 'ت', 'у', 'ש', 'ش', 'ি', 'F', 'ו', 'ᡠ', 'Ê', 'Ю', '្', 'ං', '©', '¡', '代', 'แ', 'م', 'р', 'ਆ', 'Ÿ', 'ō', 'ï', 'ී', '⋅', '\\u2061', '에', 'ক', '·', 'с', 'Τ', 'p', 'V', 'T', 'া', 'ज़', 'อ', 'ট', '域', '◇', 'y', '▫', '世', 'ν', 'Ş', 'வ', 'a', 'ל', '—', 'N', 'উ', 'n', 'ు', 'ό', '”', 'й', 'h', 'ā', '॒', '⁰', '일', 'ऍ', '长', 'C', 'ロ', 'я', '\\uf0a7', '‑', '動', 'ॐ', 'ĺ', 'ق', '×', 'ᐅ', 'ñ', 'ô', 'ι', 'т', 'Ę', '大', '唐', '휴', '₹', 'ក', 'τ', 'l', '州', 'আ', 'α', '⊆', 'ග', '¸', 'గ', 'v', 'ਸ', 'ᓗ', '…', 'រ', 'ス', 'ן', 'á', 'ᠩ', 'ী', 'л', 'ل', '℃', 'i', '自', 'а', 'Θ', 'ن', 'ॢ', 'A', 'Ω', 'O', 'W', '\\u200c', '‐', 'ə', 'f', 'ر', 'ª', 'נ', 'ρ', 'κ', '木', 'ং', 'َ', '•', 'ا', 'ز', 'п', 'य़', 'ḍ', '¬', 'ग़', 'ச', 'ú', 'š', 'ช', 'দ', 'L', 'ַ', 'ऒ', 'চ', '经', 'U', 'ֶ', 'Ч', '❏', '春', 'า', '˝', 'Q', 'ਵ', 'Р', '관', 'ิ', '\\u200b', 'ន', 'ң', 'ர', 'ப', 'デ', '車', 'फ़', 'Ι', 'К', 'ؤ', '¨', 'ळ', 'ೆ', 'χ', '\\xad', 'হ', 'о', 'ె', 'ঔ', 'త', 'क़', 'ɨ', 'ி', 'ة', 'ض', '॰', '\\u2008', '়', 'ه', 'Δ', 'ᠴ', '׃', '現', 'b', 'メ', 'إ', 'д', 'ম', 'น', 'أ', 'Â', 'ా', 'ה', 'B', 'ล', '性', 'ಮ', '€', 'ᠨ', 'Ō', '‘', 'ʃ', 'g', 'ב', 'G', 'Ó', 'ŋ', 'න', '่', 'в', 'গ', '中', 'ா', 'ّ', 'ख़', 'ॆ', 'e', '♫', 'ᠠ', 'ہ', 'پ', '☻', 'ك', 'ِ', 'ז', '▪', 'K', 'ø', '湖', '̯', 'k', 'ي', '\\xa0', 'ঘ', 'í', 'ง', 'ُ', 'É', 'ê', 'ਰ', '洞', 'প', '²', '○', '↔', 'ィ', 'ḷ', 'آ', 'ই', 'ٓ', 'ਿ', '»', 'ب', 'ɑ', 'ế', 'ឃ', 'Ž', '±', 'I', 'ප', 'θ', 'ٕ', 'ढ़', '高', 'ᐃ', '林', '\\ufeff', 'ε', 'ء', 'מ', '̈', 'স', 'ě', '鲁', '►', 'Ⓡ', 'ʹ', 'Ò', '\\uf514', 'η', '¹', 'ː', 'Ô', 'и', 'ק', '\\ue88c', 'ව', 'ಯ', 'س', 'е', 'ʊ', 'ح', 'ִ', '死', '\\uf0b7', 'Ć', '\\u200e', 'మ', 'С', 'ٰ', 'ண', 'ත', 'Г', '{', 'ɪ', 'D', '\\uf642', 'א', '⚡', 'P', 'Þ', 'ස', 'н', 'ئ', 'ظ', 'ऎ', 'ξ', 'ঙ', 'ి', 'ł', 'u', 'ˈ', 'Å', 'ท', 'ט', 'μ', 'ò', 'ก', '七', 'ত', 'ॊ', '}', 'ৰ', 'ऩ', 'ம', '‚', 'د', '→', 'ූ', '®', '్', 'ר', '⁻', 'ミ', 'м', 'ン', 'レ', 'ی', '明', '’', '\\u200f', '■', 'ಚ', 'ţ', 'E', 'ο', '¼', 'Ṭ', 'd', 'ֵ', '´', 'פ', 'ֹ', 'к', '魯', 'β', 'ய', 'δ', 'ব', 'È', 'À', 'য', 'ো', 'z', 'ş', 'ல', '骨', 'ш', '國', '\\\\', 'ම', 'm', '庭', 'ऑ', '้', '„'}\n"
          ]
        }
      ],
      "source": [
        "# for i in 'शख्स':\n",
        "  # print(i)\n",
        "\n",
        "\n",
        "# hindi_sentences = [\n",
        "#     'आस्ट्रेलिया के पाल हेनली के साथ जोड़ी बनाने वाले पेस मियामी में क्वार्टरफाइनल तक ही पहुंच सके क्योंकि इस दौर में उन्हें भूपति और नोल्स ने हराया था।',\n",
        "#     'और जो शख्स (अपने आमाल का) बदला दुनिया ही में चाहता है तो ख़ुदा के पास दुनिया व आख़िरत दोनों का अज्र मौजूद है और ख़ुदा तो हर शख्स की सुनता और सबको देखता है',\n",
        "#     'जैव-मंडल में कीड़ों का मूल्य बहुत है, क्योंकि प्रजातियों की समृद्धि के मामले में उनकी संख्या अन्य जीव समूहों से ज़्यादा है।',\n",
        "#     'आस्ट्रेलिया के खिलाफ वनडे टीम की कमान मिताली को',\n",
        "#     '8 सितम्‍बर, 2016 को माननीय राष्ट्रपति की स्‍वीकृति मिलने के बाद 101वां संविधान संशोधन अधिनियम, 2016 अस्तित्व में आया'\n",
        "# ]\n",
        "\n",
        "# Define the provided vocabulary set\n",
        "hindi_vocabulary_set = set(hindi_vocabulary)\n",
        "\n",
        "# Combine all sentences into one string\n",
        "combined_sentences = ''.join(hindi_sentences)\n",
        "\n",
        "# Find all unique characters in the combined sentences\n",
        "unique_chars_in_sentences = set(combined_sentences)\n",
        "\n",
        "# Find characters in the sentences that are not in the vocabulary\n",
        "missing_chars = unique_chars_in_sentences - hindi_vocabulary_set\n",
        "\n",
        "print(\"Characters in the sentences not in the vocabulary:\", missing_chars)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXmG9YJcTN3d",
        "outputId": "eb529bb2-44fa-4f68-b5b4-5d718abf36ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<START>': 0, ' ': 1, '!': 2, '\"': 3, '#': 4, '$': 5, '%': 6, '&': 7, \"'\": 8, '(': 9, ')': 10, '*': 11, '+': 12, ',': 13, '-': 14, '.': 15, '/': 16, '0': 17, '1': 18, '2': 19, '3': 20, '4': 21, '5': 22, '6': 23, '7': 24, '8': 25, '9': 26, ':': 27, '<': 28, '=': 29, '>': 30, '?': 31, '@': 32, '[': 33, \"' \": 34, ']': 35, '^': 36, '_': 37, '`': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64, '{': 65, '|': 66, '}': 67, '~': 68, '<PADDING>': 69, '<END>': 70}\n"
          ]
        }
      ],
      "source": [
        "index_to_hindi = {k:v for k,v in enumerate(hindi_vocabulary)}\n",
        "hindi_to_index = {v:k for k,v in enumerate(hindi_vocabulary)}\n",
        "index_to_english = {k:v for k,v in enumerate(english_vocabulary)}\n",
        "english_to_index = {v:k for k,v in enumerate(english_vocabulary)}\n",
        "print(english_to_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rdyjQgdKTN3d"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, Dropout\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, max_seq_len):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "    def call(self, inputs):\n",
        "        pos = np.arange(self.max_seq_len)[:, np.newaxis]\n",
        "        i = np.arange(self.d_model)[np.newaxis, :]\n",
        "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(self.d_model))\n",
        "        angle_rads = pos * angle_rates\n",
        "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "        pos_encoding = angle_rads[np.newaxis, ...]\n",
        "        return tf.constant(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "class SentenceEmbedding(tf.keras.Model):\n",
        "    def __init__(self, max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n",
        "        super(SentenceEmbedding, self).__init__()\n",
        "        self.vocab_size = len(language_to_index)\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.embedding = Embedding(input_dim=self.vocab_size, output_dim=d_model)\n",
        "        self.language_to_index = language_to_index\n",
        "        self.position_encoder = PositionalEncoding(d_model, max_sequence_length)\n",
        "        self.dropout = Dropout(0.1)\n",
        "        self.START_TOKEN = START_TOKEN\n",
        "        self.END_TOKEN = END_TOKEN\n",
        "        self.PADDING_TOKEN = PADDING_TOKEN\n",
        "\n",
        "    def batch_tokenize(self, batch, start_token, end_token):\n",
        "        def tokenize(sentence, start_token, end_token):\n",
        "            sentence_word_indices = [self.language_to_index[token] for token in list(sentence.numpy().decode('utf-8'))]\n",
        "            if start_token:\n",
        "                sentence_word_indices.insert(0, self.language_to_index[self.START_TOKEN])\n",
        "            if end_token:\n",
        "                sentence_word_indices.append(self.language_to_index[self.END_TOKEN])\n",
        "            for _ in range(len(sentence_word_indices), self.max_sequence_length):\n",
        "                sentence_word_indices.append(self.language_to_index[self.PADDING_TOKEN])\n",
        "            return tf.convert_to_tensor(sentence_word_indices, dtype=tf.int32)\n",
        "\n",
        "        # if not isinstance(batch, (list, tuple)):\n",
        "        #     batch = [batch]  # Convert to a list if it's a single tensor\n",
        "        tokenized = [tokenize(sentence, start_token, end_token) for sentence in batch]\n",
        "        return tf.stack(tokenized)\n",
        "\n",
        "    def call(self, x, start_token, end_token):\n",
        "        x = self.batch_tokenize(x, start_token, end_token)\n",
        "        x = self.embedding(x)\n",
        "        pos = self.position_encoder(x)\n",
        "        x = self.dropout(x + pos)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJAxtWozZ5Hq",
        "outputId": "09dc8242-ff5f-4df5-a28b-ef66716d4208"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[ 0 46 53 ... 69 69 69]\n",
            " [ 0 61 46 ... 69 69 69]\n",
            " [ 0 58 46 ... 69 69 69]\n",
            " ...\n",
            " [ 0 48 46 ... 69 69 69]\n",
            " [ 0 39 56 ... 69 69 69]\n",
            " [ 0 40 56 ... 69 69 69]], shape=(10, 250), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "def is_valid_tokens(sentence, vocab):\n",
        "    for token in list(set(sentence)):\n",
        "        if token not in vocab:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def is_valid_length(sentence, max_sequence_length):\n",
        "    return len(list(sentence)) < (max_sequence_length - 1) # need to re-add the end token so leaving 1 space\n",
        "\n",
        "valid_english_sentences = []\n",
        "valid_hindi_sentences = []\n",
        "for index in valid_sentence_indicies:\n",
        "    valid_english_sentences.append(english_sentences[index])\n",
        "    valid_hindi_sentences.append(hindi_sentences[index])\n",
        "\n",
        "def batch_tokenize(language_to_index, batch, start_token, end_token, START_TOKEN, END_TOKEN, PADDING_TOKEN, max_sequence_length):\n",
        "    def tokenize(sentence, start_token, end_token):\n",
        "        sentence_word_indices = [language_to_index[token] for token in list(sentence)]\n",
        "        if start_token:\n",
        "            sentence_word_indices.insert(0, language_to_index[START_TOKEN])\n",
        "        if end_token:\n",
        "            sentence_word_indices.append(language_to_index[END_TOKEN])\n",
        "        if max_sequence_length > len(sentence_word_indices):\n",
        "            sentence_word_indices.extend([language_to_index[PADDING_TOKEN]] * (max_sequence_length - len(sentence_word_indices)))\n",
        "        return tf.convert_to_tensor(sentence_word_indices, dtype=tf.int32)\n",
        "\n",
        "    tokenized = [tokenize(sentence, start_token, end_token) for sentence in batch]\n",
        "    return tf.stack(tokenized)\n",
        "\n",
        "x = batch_tokenize(language_to_index = english_to_index, batch = valid_english_sentences[:10], start_token = True, end_token = True,\n",
        "                     START_TOKEN= START_TOKEN, END_TOKEN= END_TOKEN, PADDING_TOKEN=PADDING_TOKEN, max_sequence_length= max_sequence_length)\n",
        "\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6cOLH9UaTN3e"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "d_model = 512\n",
        "\n",
        "sentence_embedding = SentenceEmbedding(\n",
        "    max_sequence_length=max_sequence_length,\n",
        "    d_model=d_model,\n",
        "    language_to_index=english_to_index,\n",
        "    START_TOKEN=START_TOKEN,\n",
        "    END_TOKEN=END_TOKEN,\n",
        "    PADDING_TOKEN=PADDING_TOKEN\n",
        ")\n",
        "\n",
        "# output = sentence_embedding(valid_english_sentences[:10], start_token=True, end_token=True)\n",
        "\n",
        "# print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "6HJV6kdiT3XN"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Layer\n",
        "import math\n",
        "\n",
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    d_k = tf.shape(q)[-1]\n",
        "    scaled = tf.matmul(q, k, transpose_b=True) / tf.math.sqrt(tf.cast(d_k, tf.float32))\n",
        "    if mask is not None:\n",
        "        scaled = tf.transpose(scaled, perm=[1, 0, 2, 3])\n",
        "\n",
        "        # Add the mask (broadcasting it over the head and batch dimensions)\n",
        "        mask = tf.expand_dims(mask, axis=0)  # Expand dims to match head dimension\n",
        "        scaled += mask\n",
        "\n",
        "        # Permute back to the original shape\n",
        "        scaled = tf.transpose(scaled, perm=[1, 0, 2, 3])\n",
        "\n",
        "    attention = tf.nn.softmax(scaled, axis=-1)\n",
        "    values = tf.matmul(attention, v)\n",
        "    return values, attention\n",
        "\n",
        "class MultiHeadAttention(Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.qkv_layer = Dense(3 * d_model)\n",
        "        self.linear_layer = Dense(d_model)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        sequence_length = tf.shape(x)[1]\n",
        "        qkv = self.qkv_layer(x)\n",
        "        qkv = tf.reshape(qkv, (batch_size, sequence_length, self.num_heads, 3 * self.head_dim))\n",
        "        qkv = tf.transpose(qkv, perm=[0, 2, 1, 3])\n",
        "        q, k, v = tf.split(qkv, num_or_size_splits=3, axis=-1)\n",
        "        values, attention = scaled_dot_product(q, k, v, mask)\n",
        "        values = tf.transpose(values, perm=[0, 2, 1, 3])\n",
        "        values = tf.reshape(values, (batch_size, sequence_length, self.num_heads * self.head_dim))\n",
        "        out = self.linear_layer(values)\n",
        "        return out\n",
        "\n",
        "num_heads= 8\n",
        "multihead_attention = MultiHeadAttention(d_model= d_model, num_heads= num_heads)\n",
        "\n",
        "# x = multihead_attention(x= output)\n",
        "# x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "VyuN9ZL8xXyo"
      },
      "outputs": [],
      "source": [
        "class LayerNormalization(tf.keras.layers.Layer):\n",
        "    def __init__(self, parameters_shape, eps=1e-5):\n",
        "        super(LayerNormalization, self).__init__()\n",
        "        self.parameters_shape = parameters_shape\n",
        "        self.eps = eps\n",
        "        self.gamma = self.add_weight(\n",
        "            shape=parameters_shape,\n",
        "            initializer=\"ones\",\n",
        "            trainable=True,\n",
        "            name=\"gamma\"\n",
        "        )\n",
        "        self.beta = self.add_weight(\n",
        "            shape=parameters_shape,\n",
        "            initializer=\"zeros\",\n",
        "            trainable=True,\n",
        "            name=\"beta\"\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        dims = [-i for i in range(1, len(self.parameters_shape) + 1)]\n",
        "        mean = tf.reduce_mean(inputs, axis=dims, keepdims=True)\n",
        "        var = tf.reduce_mean(tf.square(inputs - mean), axis=dims, keepdims=True)\n",
        "        std = tf.sqrt(var + self.eps)\n",
        "        y = (inputs - mean) / std\n",
        "        out = self.gamma * y + self.beta\n",
        "        return out\n",
        "\n",
        "\n",
        "class PositionwiseFeedForward(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.linear1 = tf.keras.layers.Dense(hidden)\n",
        "        self.linear2 = tf.keras.layers.Dense(d_model)\n",
        "        self.relu = tf.keras.layers.ReLU()\n",
        "        self.dropout = tf.keras.layers.Dropout(rate=drop_prob)\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x, training=training)\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate=drop_prob)\n",
        "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate=drop_prob)\n",
        "\n",
        "    def call(self, x, self_attention_mask, training=False):\n",
        "        residual_x = x\n",
        "        x = self.attention(x, mask=self_attention_mask)\n",
        "        x = self.dropout1(x, training=training)\n",
        "        x = self.norm1(x + residual_x)\n",
        "        residual_x = x\n",
        "        x = self.ffn(x, training=training)\n",
        "        x = self.dropout2(x, training=training)\n",
        "        x = self.norm2(x + residual_x)\n",
        "        return x\n",
        "\n",
        "num_heads =8\n",
        "ffn_hidden = 2048\n",
        "encoder_layer = EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob =0.1)\n",
        "\n",
        "# x = encoder_layer(x,self_attention_mask= None)\n",
        "# x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuupeYXLrn_3",
        "outputId": "1961db60-a6a4-4001-9ec9-be1f71162547"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"however, paes, who was partnering australia's paul hanley, could only go as far as the quarterfinals where they lost to bhupathi and knowles\", 'whosoever desires the reward of the world, with allah is the reward of the world and of the everlasting life. allah is the hearer, the seer.', 'the value of insects in the biosphere is enormous because they outnumber all other living groups in measure of species richness.', 'mithali to anchor indian team against australia in odis', 'after the assent of the honble president on 8thseptember, 2016, the 101thconstitutional amendment act, 2016 came into existence', 'the court has fixed a hearing for february 12', 'please select the position where the track should be split.', 'jharkhand chief minister hemant soren', 'arvind kumar, sho of the sector 55/56 police station, said a case has been registered under section 376-d (gang rape) of the indian penal code.', \"briefing media in new delhi today, party's state-in-charge, anil jain said, after the meeting the party will meet the governor to stake claim for government formation in the state.\", '\"jesus responded, as he taught in the temple, \"\"how is it that the scribes say that the christ is the son of david?\"', 'senior leaders of all major parties held electioneering in favour of their candidates.', 'meanwhile, three people came there on a bike.', 'katrina shared the video on instagram.', 'he does this because he is angry at the alleged desecration of the indian flag', 'attempts to contact randhawa on his mobile over the past one week have remained futile.', 'share videos', '\"\"\"wunderlich and his wife, petra, are both gardeners, who themselves graduated from a normal high school, which they attended \"\"\"\"not reluctantly,\"\"\"\" dirk said.\"\"\"', 'the shias venerate him as a member of the family of imams, while the sunni simply see him as a person of great sanctity.', 'hence, we must make sure we ourselves dont become careless, not allow anyone else do so', 'mayawati, akhilesh yadav seal pact for 2019 polls: bsp-sp not to contest in amethi, raebareli', 'they are not seen anywhere.', 'joseph dreamed a dream, and he told it to his brothers, and they hated him all the more.', 'they praise night and day, without ever tiring.', 'some intercity trains like manmad-mumbai express, gujarat express, saurashtra express, bandra-terminus surat intercity express and mumbai central-ahmedabad shatabdi express have been cancelled, officials of the central and western railways said.', 'both are safe and healthy.', 'new delhi: noted scientist sivan k was appointed chairman of the indian space research organisation on wednesday.', 'they raised slogans against the government and the administration.', 'the vice president said that india has consistently voiced the need to have a strong resolute response to terrorism and its manifestations and india is committed to working towards fighting and elimination of this menace.', 'the amount allocated and utilized for passenger safety, capital and development work, cleanliness and finance and accounting reforms in railways in the financial year 2016-17 is indicated below :-', 'im fine by the grace of god.', 'but its still a long way to go.']\n"
          ]
        }
      ],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices(valid_english_sentences)\n",
        "batch_size = 32\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "iterator = iter(train_dataset)\n",
        "first_batch = next(iterator)\n",
        "# first_batch = tf.reshape(first_batch, (1,batch_size))\n",
        "first_batch = [sentence.numpy().decode('utf-8') for sentence in first_batch]\n",
        "print(first_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "l3dyVm3ejkU2"
      },
      "outputs": [],
      "source": [
        "class SequentialEncoder(tf.keras.Sequential):\n",
        "    def call(self, inputs, self_attention_mask, training=False):\n",
        "        x = inputs\n",
        "        for module in self.layers:\n",
        "            x = module(x, self_attention_mask, training=training)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 ffn_hidden,\n",
        "                 num_heads,\n",
        "                 drop_prob,\n",
        "                 num_layers,\n",
        "                 max_sequence_length,\n",
        "                 language_to_index,\n",
        "                 START_TOKEN,\n",
        "                 END_TOKEN,\n",
        "                 PADDING_TOKEN):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        self.layers = SequentialEncoder([EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)])\n",
        "\n",
        "    def call(self, x, self_attention_mask, start_token, end_token, training=False):\n",
        "        x = self.sentence_embedding(x, start_token, end_token)\n",
        "        x = self.layers(x, self_attention_mask, training=training)\n",
        "        return x\n",
        "\n",
        "    def get_layers(self):\n",
        "        return self.layers\n",
        "\n",
        "drop_prob =0.1\n",
        "encoder = Encoder(d_model =d_model,ffn_hidden=  ffn_hidden,num_heads= num_heads, drop_prob= drop_prob, num_layers= 1, max_sequence_length= max_sequence_length,\n",
        "                  language_to_index= english_to_index, START_TOKEN= START_TOKEN, END_TOKEN= END_TOKEN, PADDING_TOKEN= PADDING_TOKEN)\n",
        "# x = encoder(x= first_batch, self_attention_mask= None, start_token= True, end_token= True)\n",
        "# x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ANiiVZ48Ei__"
      },
      "outputs": [],
      "source": [
        "class MultiHeadCrossAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadCrossAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.kv_layer = Dense(2 * d_model)\n",
        "        self.q_layer = Dense(d_model)\n",
        "        self.linear_layer = Dense(d_model)\n",
        "\n",
        "    def call(self, x, y, mask):\n",
        "        batch_size, sequence_length, d_model = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2]\n",
        "        kv = self.kv_layer(x)\n",
        "        q = self.q_layer(y)\n",
        "        kv = tf.reshape(kv, (batch_size, sequence_length, self.num_heads, 2 * self.head_dim))\n",
        "        q = tf.reshape(q, (batch_size, sequence_length, self.num_heads, self.head_dim))\n",
        "        kv = tf.transpose(kv, perm=[0, 2, 1, 3])\n",
        "        q = tf.transpose(q, perm=[0, 2, 1, 3])\n",
        "        k, v = tf.split(kv, 2, axis=-1)\n",
        "        values, attention = scaled_dot_product(q, k, v, mask)\n",
        "        values = tf.transpose(values, perm=[0, 2, 1, 3])\n",
        "        values = tf.reshape(values, (batch_size, sequence_length, d_model))\n",
        "        out = self.linear_layer(values)\n",
        "        return out\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attention = MultiHeadAttention(num_heads=num_heads, d_model=d_model)\n",
        "        self.layer_norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout1 = Dropout(drop_prob)\n",
        "\n",
        "        self.encoder_decoder_attention = MultiHeadCrossAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.layer_norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout2 = Dropout(drop_prob)\n",
        "\n",
        "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "        self.layer_norm3 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout3 = Dropout(drop_prob)\n",
        "\n",
        "    def call(self, x, y, self_attention_mask, cross_attention_mask):\n",
        "        _y = y\n",
        "        y = self.self_attention(y, mask=self_attention_mask)\n",
        "        y = self.dropout1(y)\n",
        "        y = self.layer_norm1(y + _y)\n",
        "\n",
        "        _y = y\n",
        "        y = self.encoder_decoder_attention(x, y, mask=cross_attention_mask)\n",
        "        y = self.dropout2(y)\n",
        "        y = self.layer_norm2(y + _y)\n",
        "\n",
        "        _y = y\n",
        "        y = self.ffn(y)\n",
        "        y = self.dropout3(y)\n",
        "        y = self.layer_norm3(y + _y)\n",
        "        return y\n",
        "\n",
        "    # def get_sub_layers(self):\n",
        "        # return [self.self_attention, self.encoder_decoder_attention, self.ffn, self.layer_norm1, self.layer_norm2, self.layer_norm3]\n",
        "\n",
        "\n",
        "\n",
        "class SequentialDecoder(tf.keras.Sequential):\n",
        "    def call(self, *inputs):\n",
        "        x, y, self_attention_mask, cross_attention_mask = inputs\n",
        "        for layer in self.layers:\n",
        "            y = layer(x, y, self_attention_mask, cross_attention_mask)\n",
        "        return y\n",
        "\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 ffn_hidden,\n",
        "                 num_heads,\n",
        "                 drop_prob,\n",
        "                 num_layers,\n",
        "                 max_sequence_length,\n",
        "                 language_to_index,\n",
        "                 START_TOKEN,\n",
        "                 END_TOKEN,\n",
        "                 PADDING_TOKEN):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        self.layers = SequentialDecoder([DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)])\n",
        "        # self.layers = tf.keras.Sequential([DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)])\n",
        "\n",
        "\n",
        "    def call(self, x, y, self_attention_mask=None, cross_attention_mask=None, start_token=True, end_token=True):\n",
        "        y = self.sentence_embedding(y, start_token, end_token)\n",
        "        y = self.layers(x, y, self_attention_mask, cross_attention_mask)\n",
        "        return y\n",
        "\n",
        "    def get_layers(self):\n",
        "        return self.layers\n",
        "\n",
        "\n",
        "class Transformer(tf.keras.layers.Layer):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 ffn_hidden,\n",
        "                 num_heads,\n",
        "                 drop_prob,\n",
        "                 num_layers,\n",
        "                 max_sequence_length,\n",
        "                 hi_vocab_size,\n",
        "                 english_to_index,\n",
        "                 hindi_to_index,\n",
        "                 START_TOKEN,\n",
        "                 END_TOKEN,\n",
        "                 PADDING_TOKEN):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, english_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        self.decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, hindi_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        self.linear = Dense(hi_vocab_size)\n",
        "        self.device = tf.config.experimental.list_physical_devices('GPU')\n",
        "\n",
        "    def call(self,\n",
        "             x,\n",
        "             y,\n",
        "             encoder_self_attention_mask=None,\n",
        "             decoder_self_attention_mask=None,\n",
        "             decoder_cross_attention_mask=None,\n",
        "             enc_start_token=False,\n",
        "             enc_end_token=False,\n",
        "             dec_start_token=False, # We should make this true\n",
        "             dec_end_token=False): # x, y are batch of sentences\n",
        "        x = self.encoder(x, encoder_self_attention_mask, start_token=enc_start_token, end_token=enc_end_token)\n",
        "        print(\"encoder done\")\n",
        "        out = self.decoder(x, y, decoder_self_attention_mask, decoder_cross_attention_mask, start_token=dec_start_token, end_token=dec_end_token)\n",
        "        print(\"decoder done\")\n",
        "        out = self.linear(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzGLHvACAqnf",
        "outputId": "66e35a42-f29c-403e-858f-3ab42a021bb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['आस्ट्रेलिया के पाल हेनली के साथ जोड़ी बनाने वाले पेस मियामी में क्वार्टरफाइनल तक ही पहुंच सके क्योंकि इस दौर में उन्हें भूपति और नोल्स ने हराया था।', 'और जो शख्स (अपने आमाल का) बदला दुनिया ही में चाहता है तो ख़ुदा के पास दुनिया व आख़िरत दोनों का अज्र मौजूद है और ख़ुदा तो हर शख्स की सुनता और सबको देखता है', 'जैव-मंडल में कीड़ों का मूल्य बहुत है, क्योंकि प्रजातियों की समृद्धि के मामले में उनकी संख्या अन्य जीव समूहों से ज़्यादा है।', 'आस्ट्रेलिया के खिलाफ वनडे टीम की कमान मिताली को', '8 सितम्\\u200dबर, 2016 को माननीय राष्\\u200dट्रपति की स्\\u200dवीकृति मिलने के बाद 101वां संविधान संशोधन अधिनियम, 2016 अस्तित्\\u200dव में आया', 'अदालत ने इस मामले में आगे की सुनवाई के लिए एक फरवरी की तारीख़ तय की', 'जहाँ पर ट्रैक को विभाजित किया जाना है, कृपया वह स्थान चुनें.', 'झारखंड के मुख्यमंत्री हेमंत सोरेन (फोटोः पीटीआई)', 'सेक्टर 55/56 के एसएचओ अरविंद कुमार ने बताया कि इस मामले में आईपीसी की धारा 376-डी (गैंगरेप) के तहत मामला दर्ज कर लिया गया है।', 'आज नई दिल्ली में मीडिया से बातचीत में पार्टी के राज्य प्रभारी अनिल जैन ने बताया कि बैठक के बाद पार्टी, सरकार के गठन का दावा पेश करने के लिए राज्यपाल से मिलेगी।']\n",
            "[\"however, paes, who was partnering australia's paul hanley, could only go as far as the quarterfinals where they lost to bhupathi and knowles\", 'whosoever desires the reward of the world, with allah is the reward of the world and of the everlasting life. allah is the hearer, the seer.', 'the value of insects in the biosphere is enormous because they outnumber all other living groups in measure of species richness.', 'mithali to anchor indian team against australia in odis', 'after the assent of the honble president on 8thseptember, 2016, the 101thconstitutional amendment act, 2016 came into existence', 'the court has fixed a hearing for february 12', 'please select the position where the track should be split.', 'jharkhand chief minister hemant soren', 'arvind kumar, sho of the sector 55/56 police station, said a case has been registered under section 376-d (gang rape) of the indian penal code.', \"briefing media in new delhi today, party's state-in-charge, anil jain said, after the meeting the party will meet the governor to stake claim for government formation in the state.\"]\n"
          ]
        }
      ],
      "source": [
        "print(valid_hindi_sentences[:10])\n",
        "print(valid_english_sentences[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INa_bl_U6Cvz",
        "outputId": "b5ae140f-5aa4-4eee-8db7-56224509fd30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"however, paes, who was partnering australia's paul hanley, could only go as far as the quarterfinals where they lost to bhupathi and knowles\"\n",
            " b'whosoever desires the reward of the world, with allah is the reward of the world and of the everlasting life. allah is the hearer, the seer.'\n",
            " b'the value of insects in the biosphere is enormous because they outnumber all other living groups in measure of species richness.'\n",
            " b'mithali to anchor indian team against australia in odis'\n",
            " b'after the assent of the honble president on 8thseptember, 2016, the 101thconstitutional amendment act, 2016 came into existence'\n",
            " b'the court has fixed a hearing for february 12'\n",
            " b'please select the position where the track should be split.'\n",
            " b'jharkhand chief minister hemant soren'\n",
            " b'arvind kumar, sho of the sector 55/56 police station, said a case has been registered under section 376-d (gang rape) of the indian penal code.'\n",
            " b\"briefing media in new delhi today, party's state-in-charge, anil jain said, after the meeting the party will meet the governor to stake claim for government formation in the state.\"\n",
            " b'\"jesus responded, as he taught in the temple, \"\"how is it that the scribes say that the christ is the son of david?\"'\n",
            " b'senior leaders of all major parties held electioneering in favour of their candidates.'\n",
            " b'meanwhile, three people came there on a bike.'\n",
            " b'katrina shared the video on instagram.'\n",
            " b'he does this because he is angry at the alleged desecration of the indian flag'\n",
            " b'attempts to contact randhawa on his mobile over the past one week have remained futile.'\n",
            " b'share videos'\n",
            " b'\"\"\"wunderlich and his wife, petra, are both gardeners, who themselves graduated from a normal high school, which they attended \"\"\"\"not reluctantly,\"\"\"\" dirk said.\"\"\"'\n",
            " b'the shias venerate him as a member of the family of imams, while the sunni simply see him as a person of great sanctity.'\n",
            " b'hence, we must make sure we ourselves dont become careless, not allow anyone else do so'\n",
            " b'mayawati, akhilesh yadav seal pact for 2019 polls: bsp-sp not to contest in amethi, raebareli'\n",
            " b'they are not seen anywhere.'\n",
            " b'joseph dreamed a dream, and he told it to his brothers, and they hated him all the more.'\n",
            " b'they praise night and day, without ever tiring.'\n",
            " b'some intercity trains like manmad-mumbai express, gujarat express, saurashtra express, bandra-terminus surat intercity express and mumbai central-ahmedabad shatabdi express have been cancelled, officials of the central and western railways said.'\n",
            " b'both are safe and healthy.'\n",
            " b'new delhi: noted scientist sivan k was appointed chairman of the indian space research organisation on wednesday.'\n",
            " b'they raised slogans against the government and the administration.'\n",
            " b'the vice president said that india has consistently voiced the need to have a strong resolute response to terrorism and its manifestations and india is committed to working towards fighting and elimination of this menace.'\n",
            " b'the amount allocated and utilized for passenger safety, capital and development work, cleanliness and finance and accounting reforms in railways in the financial year 2016-17 is indicated below :-'\n",
            " b'im fine by the grace of god.' b'but its still a long way to go.'], shape=(32,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class TextDataset:\n",
        "    def __init__(self, english_sentences, hindi_sentences):\n",
        "        self.english_sentences = english_sentences\n",
        "        self.hindi_sentences = hindi_sentences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.english_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.english_sentences[idx], self.hindi_sentences[idx]\n",
        "\n",
        "\n",
        "\n",
        "dataset = TextDataset(valid_english_sentences, valid_hindi_sentences)\n",
        "\n",
        "def generator():\n",
        "    for i in range(len(dataset)):\n",
        "        eng_sentence, hi_sentence = dataset[i]\n",
        "        yield eng_sentence, hi_sentence\n",
        "\n",
        "output_signature = (\n",
        "    tf.TensorSpec(shape=(), dtype=tf.string),\n",
        "    tf.TensorSpec(shape=(), dtype=tf.string)\n",
        ")\n",
        "\n",
        "tf_dataset = tf.data.Dataset.from_generator(generator, output_signature=output_signature)\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Create a batched dataset\n",
        "tf_dataset = tf_dataset.batch(batch_size)\n",
        "\n",
        "# Create an iterator\n",
        "iterator = iter(tf_dataset)\n",
        "\n",
        "# Iterate through the dataset\n",
        "\n",
        "for batch_num, batch in enumerate(iterator):\n",
        "    print(batch[0])\n",
        "    if batch_num > -1:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, english_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "sentence_embedding.batch_tokenize(next(iterator)[0], start_token= True, end_token= True)\n",
        "# def batch_tokenize(batch, start_token, end_token):\n",
        "#     def tokenize(sentence, start_token, end_token):\n",
        "#         sentence_word_indices = [english_to_index[token] for token in list(sentence.numpy().decode('utf-8'))]\n",
        "#         if start_token:\n",
        "#             sentence_word_indices.insert(0, english_to_index[start_token])\n",
        "#         if end_token:\n",
        "#             sentence_word_indices.append(english_to_index[end_token])\n",
        "#         for _ in range(len(sentence_word_indices), max_sequence_length):\n",
        "#             sentence_word_indices.append(english_to_index[PADDING_TOKEN])\n",
        "#         return tf.convert_to_tensor(sentence_word_indices, dtype=tf.int32)\n",
        "\n",
        "#     # if not isinstance(batch, (list, tuple)):\n",
        "#         # batch = [batch]  # Convert to a list if it's a single tensor\n",
        "#     tokenized = [tokenize(sentence, start_token, end_token) for sentence in batch]\n",
        "#     return tf.stack(tokenized)\n",
        "\n",
        "# for sentence in next(iterator)[0]:\n",
        "    # sentence_word_indices = [english_to_index[token] for token in list(sentence.numpy().decode('utf-8'))]\n",
        "    # print(len(sentence.numpy().decode('utf-8')))\n",
        "    # print(len(sentence_word_indices))\n",
        "    # print(sentence)\n",
        "\n",
        "# batch_tokenize(next(iterator)[0], START_TOKEN, END_TOKEN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9SUrBX1sUqK",
        "outputId": "67d15abb-b601-46b1-f07c-643bca68232a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(32, 250), dtype=int32, numpy=\n",
              "array([[ 0, 46, 47, ..., 69, 69, 69],\n",
              "       [ 0, 58, 46, ..., 69, 69, 69],\n",
              "       [ 0, 61, 47, ..., 69, 69, 69],\n",
              "       ...,\n",
              "       [ 0, 40, 59, ..., 69, 69, 69],\n",
              "       [ 0, 25, 25, ..., 69, 69, 69],\n",
              "       [ 0, 43, 60, ..., 69, 69, 69]], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9bZwJh54wbJ"
      },
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "\n",
        "# class TextDataset(tf.data.Dataset):\n",
        "#     def _generator(english_sentences, hindi_sentences):\n",
        "#         for eng, hi in zip(english_sentences, hindi_sentences):\n",
        "#             yield eng, hi\n",
        "\n",
        "#     def __new__(cls, english_sentences, hindi_sentences):\n",
        "#         return tf.data.Dataset.from_generator(\n",
        "#             cls._generator,\n",
        "#             output_signature=(\n",
        "#                 tf.TensorSpec(shape=(), dtype=tf.string),\n",
        "#                 tf.TensorSpec(shape=(), dtype=tf.string)\n",
        "#             ),\n",
        "#             args=(english_sentences, hindi_sentences)\n",
        "#         )\n",
        "\n",
        "\n",
        "# dataset = TextDataset(valid_english_sentences, valid_hindi_sentences)\n",
        "# dataset = dataset.batch(batch_size)\n",
        "\n",
        "\n",
        "# batches_list = []\n",
        "# for batch in dataset:\n",
        "#     batch_list = []\n",
        "#     batch_list_eng = []\n",
        "#     batch_list_kan = []\n",
        "#     for eng, kan in zip(batch[0], batch[1]):\n",
        "#         batch_list_eng.append(eng.numpy().decode(\"utf-8\"))\n",
        "#         batch_list_kan.append(kan.numpy().decode(\"utf-8\"))\n",
        "#     batch_list.append(tuple(batch_list_eng))\n",
        "#     batch_list.append(tuple(batch_list_kan))\n",
        "#     batches_list.append(batch_list)\n",
        "\n",
        "# batches_list[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AzUo2EgslU2"
      },
      "outputs": [],
      "source": [
        "encoder = Encoder(d_model =d_model,ffn_hidden=  ffn_hidden,num_heads= num_heads, drop_prob= drop_prob, num_layers= 1, max_sequence_length= max_sequence_length,\n",
        "                  language_to_index= english_to_index, START_TOKEN= START_TOKEN, END_TOKEN= END_TOKEN, PADDING_TOKEN= PADDING_TOKEN)\n",
        "for batch_num, batch in enumerate(iterator):\n",
        "    # x = encoder(x= batch[0], self_attention_mask= None, start_token= True, end_token= True)\n",
        "    for sentence in batch[0]:\n",
        "      print(sentence.numpy().decode('utf-8'))\n",
        "    if batch_num > -1:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ko9cfDv15qk8"
      },
      "outputs": [],
      "source": [
        "d_model = 512\n",
        "batch_size = 32\n",
        "ffn_hidden = 2048\n",
        "num_heads = 8\n",
        "drop_prob = 0.1\n",
        "num_layers = 1\n",
        "hi_vocab_size = len(hindi_vocabulary)\n",
        "\n",
        "transformer = Transformer(d_model,\n",
        "                          ffn_hidden,\n",
        "                          num_heads,\n",
        "                          drop_prob,\n",
        "                          num_layers,\n",
        "                          max_sequence_length,\n",
        "                          hi_vocab_size,\n",
        "                          english_to_index,\n",
        "                          hindi_to_index,\n",
        "                          START_TOKEN,\n",
        "                          END_TOKEN,\n",
        "                          PADDING_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "t1B9Bg9l4Ub6"
      },
      "outputs": [],
      "source": [
        "initializer = tf.keras.initializers.GlorotUniform()\n",
        "\n",
        "# Iterate over the model's layers and initialize weights with Xavier uniform initializer\n",
        "for layer in transformer.encoder.get_layers().layers:\n",
        "    if isinstance(layer, tf.keras.layers.Dense):\n",
        "        layer.kernel_initializer = initializer\n",
        "        # To reinitialize the kernel with the new initializer\n",
        "        layer.build(layer.input_shape)\n",
        "        print(layer)\n",
        "\n",
        "for layer in transformer.decoder.get_layers().layers:\n",
        "    if isinstance(layer, tf.keras.layers.Dense):\n",
        "        layer.kernel_initializer = initializer\n",
        "        # To reinitialize the kernel with the new initializer\n",
        "        layer.build(layer.input_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "U9aogDYrD7CX"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "NEG_INFTY = -1e9\n",
        "\n",
        "def create_masks(eng_batch, kn_batch, max_sequence_length):\n",
        "    # eng_batch = eng_batch.numpy()\n",
        "    # kn_batch = kn_batch.numpy()\n",
        "    num_sentences = tf.shape(eng_batch)[0]\n",
        "\n",
        "    # Look-ahead mask for decoder\n",
        "    look_ahead_mask = tf.linalg.band_part(tf.ones((max_sequence_length, max_sequence_length)), -1, 0)\n",
        "    look_ahead_mask = 1 - look_ahead_mask\n",
        "\n",
        "    encoder_padding_mask = tf.zeros((num_sentences, max_sequence_length, max_sequence_length), dtype=tf.float32)\n",
        "    decoder_padding_mask_self_attention = tf.zeros((num_sentences, max_sequence_length, max_sequence_length), dtype=tf.float32)\n",
        "    decoder_padding_mask_cross_attention = tf.zeros((num_sentences, max_sequence_length, max_sequence_length), dtype=tf.float32)\n",
        "\n",
        "    for idx in range(num_sentences):\n",
        "        eng_sentence_length = len(eng_batch[idx].numpy().decode('utf-8'))\n",
        "        kn_sentence_length = len(kn_batch[idx].numpy().decode('utf-8'))\n",
        "\n",
        "        eng_chars_to_padding_mask = tf.range(eng_sentence_length + 1, max_sequence_length)\n",
        "        # print(kn_sentence_length)\n",
        "        # print(kn_batch[idx].decode('utf-8'))\n",
        "        kn_chars_to_padding_mask = tf.range(kn_sentence_length + 1, max_sequence_length)\n",
        "\n",
        "        # Update encoder padding mask\n",
        "        for pos in eng_chars_to_padding_mask:\n",
        "            encoder_padding_mask = tf.tensor_scatter_nd_update(\n",
        "                encoder_padding_mask,\n",
        "                [[idx, pos, i] for i in range(max_sequence_length)],\n",
        "                tf.ones((max_sequence_length,), dtype=tf.float32)\n",
        "            )\n",
        "            encoder_padding_mask = tf.tensor_scatter_nd_update(\n",
        "                encoder_padding_mask,\n",
        "                [[idx, i, pos] for i in range(max_sequence_length)],\n",
        "                tf.ones((max_sequence_length,), dtype=tf.float32)\n",
        "            )\n",
        "\n",
        "        # Update decoder padding mask for self-attention\n",
        "        for pos in kn_chars_to_padding_mask:\n",
        "            decoder_padding_mask_self_attention = tf.tensor_scatter_nd_update(\n",
        "                decoder_padding_mask_self_attention,\n",
        "                [[idx, pos, i] for i in range(max_sequence_length)],\n",
        "                tf.ones((max_sequence_length,), dtype=tf.float32)\n",
        "            )\n",
        "            decoder_padding_mask_self_attention = tf.tensor_scatter_nd_update(\n",
        "                decoder_padding_mask_self_attention,\n",
        "                [[idx, i, pos] for i in range(max_sequence_length)],\n",
        "                tf.ones((max_sequence_length,), dtype=tf.float32)\n",
        "            )\n",
        "\n",
        "        # Update decoder padding mask for cross-attention\n",
        "        for pos in eng_chars_to_padding_mask:\n",
        "            decoder_padding_mask_cross_attention = tf.tensor_scatter_nd_update(\n",
        "                decoder_padding_mask_cross_attention,\n",
        "                [[idx, pos, i] for i in range(max_sequence_length)],\n",
        "                tf.ones((max_sequence_length,), dtype=tf.float32)\n",
        "            )\n",
        "        for pos in kn_chars_to_padding_mask:\n",
        "            decoder_padding_mask_cross_attention = tf.tensor_scatter_nd_update(\n",
        "                decoder_padding_mask_cross_attention,\n",
        "                [[idx, i, pos] for i in range(max_sequence_length)],\n",
        "                tf.ones((max_sequence_length,), dtype=tf.float32)\n",
        "            )\n",
        "\n",
        "    encoder_self_attention_mask = tf.where(encoder_padding_mask>0, NEG_INFTY, 0.0)\n",
        "    decoder_self_attention_mask = tf.where(tf.cast(look_ahead_mask, tf.float32) + decoder_padding_mask_self_attention > 0, NEG_INFTY, 0.0)\n",
        "    decoder_cross_attention_mask = tf.where(decoder_padding_mask_cross_attention > 0, NEG_INFTY, 0.0)\n",
        "\n",
        "    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask\n",
        "\n",
        "\n",
        "# look_ahead_mask = tf.linalg.band_part(tf.ones((max_sequence_length, max_sequence_length)), -1, 0)\n",
        "# look_ahead_mask = 1 - look_ahead_mask #normal mask taught\n",
        "\n",
        "# eng_chars_to_padding_mask = np.arange( 50+ 1, max_sequence_length)\n",
        "# encoder_padding_mask = tf.tensor_scatter_nd_update(encoder_padding_mask,\n",
        "#                                                     indices=[[idx, :, eng_chars_to_padding_mask]],\n",
        "#                                                     updates=tf.ones_like(eng_chars_to_padding_mask, dtype=tf.bool))\n",
        "\n",
        "# encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(next(iterator)[0], next(iterator)[1], max_sequence_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCG1O3s77TJJ"
      },
      "outputs": [],
      "source": [
        "print(encoder_self_attention_mask)\n",
        "print(\"==============================\")\n",
        "print(decoder_self_attention_mask)\n",
        "print(\"==============================\")\n",
        "print(decoder_cross_attention_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bwSgcIg80ie"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# from torch import nn\n",
        "# def create_masks(eng_batch, kn_batch):\n",
        "#     num_sentences = len(eng_batch)\n",
        "#     look_ahead_mask = torch.full([max_sequence_length, max_sequence_length] , True)\n",
        "#     look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)\n",
        "#     encoder_padding_mask = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
        "#     decoder_padding_mask_self_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
        "#     decoder_padding_mask_cross_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
        "\n",
        "#     for idx in range(num_sentences):\n",
        "#       eng_sentence_length, kn_sentence_length = len(eng_batch[idx]), len(kn_batch[idx])\n",
        "#       eng_chars_to_padding_mask = np.arange(eng_sentence_length + 1, max_sequence_length)\n",
        "#       kn_chars_to_padding_mask = np.arange(kn_sentence_length + 1, max_sequence_length)\n",
        "#       encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = True\n",
        "#       encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = True\n",
        "#       decoder_padding_mask_self_attention[idx, :, kn_chars_to_padding_mask] = True\n",
        "#       decoder_padding_mask_self_attention[idx, kn_chars_to_padding_mask, :] = True\n",
        "#       decoder_padding_mask_cross_attention[idx, :, eng_chars_to_padding_mask] = True\n",
        "#       decoder_padding_mask_cross_attention[idx, kn_chars_to_padding_mask, :] = True\n",
        "\n",
        "#     encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0)\n",
        "#     decoder_self_attention_mask =  torch.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n",
        "#     decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n",
        "#     return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask\n",
        "\n",
        "# encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(batch_list[0][0], batch_list[0][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCQYwmNZ9Hnp"
      },
      "outputs": [],
      "source": [
        "# print(encoder_self_attention_mask)\n",
        "# print(\"==============================\")\n",
        "# print(decoder_self_attention_mask)\n",
        "# print(\"==============================\")\n",
        "# print(decoder_cross_attention_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJ7Tc2iGGt5v",
        "outputId": "737cf7f8-0c57-43d0-d0ff-d235010260e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "device = '/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'\n",
        "num_epochs = 10\n",
        "\n",
        "# Define loss function with ignore_index equivalent\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True,\n",
        "    reduction='none'\n",
        ")\n",
        "optim= tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "# Define a custom loss function to ignore padding tokens\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.not_equal(real, hindi_to_index[PADDING_TOKEN])\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}\")\n",
        "    total_loss = 0\n",
        "    for batch_num, batch in enumerate(iterator):\n",
        "        eng_batch = batch[0]\n",
        "        hi_batch = batch[1]\n",
        "        print(batch_num)\n",
        "        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_batch, hi_batch, max_sequence_length)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            hi_predictions = transformer(eng_batch,\n",
        "                                         hi_batch,\n",
        "                                         encoder_self_attention_mask,\n",
        "                                         decoder_self_attention_mask,\n",
        "                                         decoder_cross_attention_mask,\n",
        "                                         enc_start_token=False,\n",
        "                                         enc_end_token=False,\n",
        "                                         dec_start_token=True,\n",
        "                                         dec_end_token=True)\n",
        "            labels = transformer.decoder.sentence_embedding.batch_tokenize(hi_batch, start_token=False, end_token=True)\n",
        "            loss = loss_function(labels, hi_predictions)\n",
        "\n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "        optim.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "        total_loss += loss.numpy()\n",
        "\n",
        "        if batch_num % 100 == 0:\n",
        "            print(f\"Batch {batch_num} Loss {total_loss.numpy()}\")\n",
        "            print(f\"English: {eng_batch[0]}\")\n",
        "            print(f\"Hindi Translation: {hi_batch[0]}\")\n",
        "            hi_sentence_predicted = tf.argmax(hi_predictions[0], axis=1)\n",
        "            predicted_sentence = \"\"\n",
        "            for idx in hi_sentence_predicted:\n",
        "                if idx == hindi_to_index[END_TOKEN]:\n",
        "                    break\n",
        "                predicted_sentence += index_to_hindi[idx.numpy()]\n",
        "            print(f\"Hindi Prediction: {predicted_sentence}\")\n",
        "\n",
        "            transformer.eval()\n",
        "            hi_sentence = \"\"\n",
        "            eng_sentence = \"should we go to the mall?\"\n",
        "            for word_counter in range(max_sequence_length):\n",
        "                encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks([eng_sentence], [hi_sentence], max_sequence_length)\n",
        "                predictions = transformer([eng_sentence, hi_sentence, encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask], training=False)\n",
        "                next_token_prob_distribution = predictions[0][word_counter]\n",
        "                next_token_index = tf.argmax(next_token_prob_distribution).numpy()\n",
        "                next_token = index_to_hindi[next_token_index]\n",
        "                hi_sentence += next_token\n",
        "                if next_token == END_TOKEN:\n",
        "                    break\n",
        "\n",
        "            print(f\"Evaluation translation (should we go to the mall?): {hi_sentence}\")\n",
        "            print(\"-------------------------------------------\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
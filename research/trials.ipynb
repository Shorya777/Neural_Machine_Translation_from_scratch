{
  "cells": [
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 6,
=======
      "source": [
        "!kaggle datasets download -d parvmodi/english-to-hindi-machine-translation-dataset"
      ],
>>>>>>> eefedf4ecc41ec268fad2a52ffef4af3e9d73412
      "metadata": {
        "id": "wg510zXyTcgU",
        "outputId": "d138d17a-cff4-4696-8ed3-1f52bc5fa75a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
<<<<<<< HEAD
      "outputs": [],
      "source": [
        "# !kaggle datasets download -d parvmodi/english-to-hindi-machine-translation-dataset"
=======
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/parvmodi/english-to-hindi-machine-translation-dataset\n",
            "License(s): CC0-1.0\n",
            "Downloading english-to-hindi-machine-translation-dataset.zip to /content\n",
            "100% 1.04G/1.04G [00:37<00:00, 36.3MB/s]\n",
            "100% 1.04G/1.04G [00:37<00:00, 29.6MB/s]\n"
          ]
        }
>>>>>>> eefedf4ecc41ec268fad2a52ffef4af3e9d73412
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 7,
=======
      "source": [
        "!unzip english-to-hindi-machine-translation-dataset.zip"
      ],
>>>>>>> eefedf4ecc41ec268fad2a52ffef4af3e9d73412
      "metadata": {
        "id": "5d8aXkDvTwHb",
        "outputId": "b2060681-3b7e-468b-e53a-e39dad7c0d24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
<<<<<<< HEAD
      "outputs": [],
      "source": [
        "# !unzip english-to-hindi-machine-translation-dataset.zip"
=======
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  english-to-hindi-machine-translation-dataset.zip\n",
            "  inflating: train.en                \n",
            "  inflating: train.hi                \n"
          ]
        }
>>>>>>> eefedf4ecc41ec268fad2a52ffef4af3e9d73412
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-lrXpDF5kgx",
        "outputId": "ac5928ce-cb91-45de-9acf-15404bf7fe5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10125706\n",
            "[\"However, Paes, who was partnering Australia's Paul Hanley, could only go as far as the quarterfinals where they lost to Bhupathi and Knowles\\n\", 'Whosoever desires the reward of the world, with Allah is the reward of the world and of the Everlasting Life. Allah is the Hearer, the Seer.\\n', 'The value of insects in the biosphere is enormous because they outnumber all other living groups in measure of species richness.\\n', 'Mithali To Anchor Indian Team Against Australia in ODIs\\n', 'After the assent of the Honble President on 8thSeptember, 2016, the 101thConstitutional Amendment Act, 2016 came into existence\\n']\n",
            "10125706\n",
            "['आस्ट्रेलिया के पाल हेनली के साथ जोड़ी बनाने वाले पेस मियामी में क्वार्टरफाइनल तक ही पहुंच सके क्योंकि इस दौर में उन्हें भूपति और नोल्स ने हराया था।\\n', 'और जो शख्स (अपने आमाल का) बदला दुनिया ही में चाहता है तो ख़ुदा के पास दुनिया व आख़िरत दोनों का अज्र मौजूद है और ख़ुदा तो हर शख्स की सुनता और सबको देखता है\\n', 'जैव-मंडल में कीड़ों का मूल्य बहुत है, क्योंकि प्रजातियों की समृद्धि के मामले में उनकी संख्या अन्य जीव समूहों से ज़्यादा है।\\n', 'आस्ट्रेलिया के खिलाफ वनडे टीम की कमान मिताली को\\n', '8 सितम्\\u200dबर, 2016 को माननीय राष्\\u200dट्रपति की स्\\u200dवीकृति मिलने के बाद 101वां संविधान संशोधन अधिनियम, 2016 अस्तित्\\u200dव में आया\\n']\n"
          ]
        }
      ],
      "source": [
        "filepath_en = 'train.en'\n",
        "filepath_hi = 'train.hi'\n",
        "\n",
        "with open(filepath_en, encoding='utf-8') as file:\n",
        "    lines_en = file.readlines()\n",
        "print(len(lines_en))\n",
        "print(lines_en[:5])\n",
        "\n",
        "with open(filepath_hi, encoding='utf-8') as file:\n",
        "    lines_hi = file.readlines()\n",
        "\n",
        "print(len(lines_hi))\n",
        "print(lines_hi[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUOig1EV5kgx",
        "outputId": "beeb200b-75ba-4dbb-c04d-d15aa361dd32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\"however, paes, who was partnering australia's paul hanley, could only go as far as the quarterfinals where they lost to bhupathi and knowles\", 'whosoever desires the reward of the world, with allah is the reward of the world and of the everlasting life. allah is the hearer, the seer.', 'the value of insects in the biosphere is enormous because they outnumber all other living groups in measure of species richness.', 'mithali to anchor indian team against australia in odis', 'after the assent of the honble president on 8thseptember, 2016, the 101thconstitutional amendment act, 2016 came into existence']\n",
            "================================\n",
            "['आस्ट्रेलिया के पाल हेनली के साथ जोड़ी बनाने वाले पेस मियामी में क्वार्टरफाइनल तक ही पहुंच सके क्योंकि इस दौर में उन्हें भूपति और नोल्स ने हराया था।', 'और जो शख्स (अपने आमाल का) बदला दुनिया ही में चाहता है तो ख़ुदा के पास दुनिया व आख़िरत दोनों का अज्र मौजूद है और ख़ुदा तो हर शख्स की सुनता और सबको देखता है', 'जैव-मंडल में कीड़ों का मूल्य बहुत है, क्योंकि प्रजातियों की समृद्धि के मामले में उनकी संख्या अन्य जीव समूहों से ज़्यादा है।', 'आस्ट्रेलिया के खिलाफ वनडे टीम की कमान मिताली को', '8 सितम्\\u200dबर, 2016 को माननीय राष्\\u200dट्रपति की स्\\u200dवीकृति मिलने के बाद 101वां संविधान संशोधन अधिनियम, 2016 अस्तित्\\u200dव में आया']\n"
          ]
        }
      ],
      "source": [
        "TOTAL_SENTENCES = 200000\n",
        "english_sentences = lines_en[:TOTAL_SENTENCES]\n",
        "hindi_sentences = lines_hi[:TOTAL_SENTENCES]\n",
        "english_sentences = [sentence.rstrip('\\n').lower() for sentence in english_sentences]\n",
        "hindi_sentences = [sentence.rstrip('\\n') for sentence in hindi_sentences]\n",
        "\n",
        "print(english_sentences[:5])\n",
        "print(\"================================\")\n",
        "print(hindi_sentences[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OOIXt1O_BAv",
        "outputId": "33179c1c-8055-442e-9327-62985fe4483f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "97th percentile length Kannada: 258.0\n",
            "97th percentile length English: 267.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "PERCENTILE = 97\n",
        "print( f\"{PERCENTILE}th percentile length Kannada: {np.percentile([len(x) for x in hindi_sentences], PERCENTILE)}\" )\n",
        "print( f\"{PERCENTILE}th percentile length English: {np.percentile([len(x) for x in english_sentences], PERCENTILE)}\" )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "drEPgCVEDAMJ"
      },
      "outputs": [],
      "source": [
        "START_TOKEN = ''\n",
        "PADDING_TOKEN = ''\n",
        "END_TOKEN = ''\n",
        "\n",
        "english_vocabulary = [START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/',\n",
        "                        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "                        ':', '<', '=', '>', '?', '@',\n",
        "                        '[', '\\' ', ']', '^', '_', '`',\n",
        "                        'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
        "                        'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x',\n",
        "                        'y', 'z',\n",
        "                        '{', '|', '}', '~', PADDING_TOKEN, END_TOKEN]\n",
        "\n",
        "hindi_vocabulary = [START_TOKEN, 'अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ए', 'ऐ', 'ओ', 'औ',\n",
        "    'क', 'ख', 'ग', 'घ', 'ङ','ड़', 'च', 'छ', 'ज', 'झ', 'ञ',\n",
        "    'ट', 'ठ', 'ड', 'ढ', 'ण', 'त', 'थ', 'द', 'ध', 'न',\n",
        "    'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ल', 'व', 'श',\n",
        "    'ष', 'स', 'ह', '़', 'ा', 'ि', 'ी', 'ु', 'ू', 'े', 'ै', 'ो', 'ौ',\n",
        "    'ं', 'ः', 'ँ', '्', '०', '१', '२', '३', '४', '५', '६', '७', '८', '९', '|',\n",
        "    ':', '<', '=', '>', '?', '@',\n",
        "    '[', '\\' ', ']', '^', '_', '`',\n",
        "    ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/',\n",
        "    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9','\\u200d','।', 'ृ',\n",
        "    PADDING_TOKEN, END_TOKEN\n",
        "]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qpZWhY7AA5K",
        "outputId": "bcc707f7-e309-46f6-afae-35c3dd092d7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of sentences: 200000\n",
            "Number of valid sentences: 151260\n"
          ]
        }
      ],
      "source": [
        "max_sequence_length = 250\n",
        "\n",
        "def is_valid_tokens(sentence, vocab):\n",
        "    for token in list(set(sentence)):\n",
        "        if token not in vocab:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def is_valid_length(sentence, max_sequence_length):\n",
        "    return len(list(sentence)) < (max_sequence_length - 1) # need to re-add the end token so leaving 1 space\n",
        "\n",
        "valid_sentence_indicies = []\n",
        "for index in range(len(hindi_sentences)):\n",
        "    hindi_sentence, english_sentence = hindi_sentences[index], english_sentences[index]\n",
        "    if is_valid_length(hindi_sentence, max_sequence_length) \\\n",
        "      and is_valid_length(english_sentence, max_sequence_length) \\\n",
        "      and is_valid_tokens(hindi_sentence, hindi_vocabulary):\n",
        "        valid_sentence_indicies.append(index)\n",
        "\n",
        "print(f\"Number of sentences: {len(hindi_sentences)}\")\n",
        "print(f\"Number of valid sentences: {len(valid_sentence_indicies)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMN2l2qPHqq6",
        "outputId": "0c699fb3-687f-497b-f5ee-b7ab563afb6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Characters in the sentences not in the vocabulary: {'ᒡ', 'ই', 'Â', 'ా', '\\u2061', 'М', '性', 'ज़', 'ش', 'ك', 'ّ', '記', 'ਸ', 'Þ', 'ී', 'þ', '॒', '☞', 'ர', 'ˌ', 'j', 'c', 'ਰ', '湖', 'ᠴ', 'Y', 'ч', 'ی', 'ι', '′', 'ต', '七', 'Ō', 'ש', 'ָ', 'ּ', '班', 'ہ', '□', 'C', 'ٰ', '਼', 'ॊ', 'ො', '\\u2008', 'ล', '้', 'ក', 'a', 'ы', '–', 'Ⓡ', 'p', 'ে', 'С', '世', '○', 'ص', 'फ़', 'ғ', 'আ', '×', 'κ', 'ᠩ', '\\uf514', 'È', '에', '▪', 'ο', '̈', 'm', 'α', '章', 'ن', 'Ô', '¸', 'q', 'R', 'ి', 'É', '\\uf0b7', 'ح', '\\uf642', 'ী', '♪', 'м', 'ம', 'ᡠ', '骨', 'x', 'Ч', 'ಚ', 'ย', '“', 'у', 'п', 'Ň', '\\xad', '్', '林', 'ਿ', 'ט', '‑', 'َ', '®', 'â', 'ঙ', 'ল', '˜', 'ط', 'θ', 'ำ', 'y', 'n', 'ס', '木', 'I', 'U', '្', 'ξ', 'ய', 'ऎ', 'ʁ', 'ඩ', 'ॠ', 'ॄ', '⊆', '—', 'గ', 'Ć', 'ॐ', '⚡', '일', '়', 'ऌ', 'ය', 'η', '್', 'ш', 'স', 'ত', 'ٓ', 'Ż', 'த', 'ب', 'ف', '女', 'á', 'G', '唐', 'ਵ', 'ᠠ', '经', 'ਤ', 'Δ', 'χ', '한', 'ר', 'デ', 'P', 'ミ', 'ං', 'ុ', 'ស', '經', 'א', 'ק', 'Å', '☻', 'ॢ', '自', 'ב', 'ও', 'ֵ', 'ு', '⋅', '‐', 'è', 'g', 'J', 'ម', 'ó', 'า', 'ऽ', '্', 'ன', '¼', 'ਦ', 'س', 'ম', 'レ', ';', 'Ê', 'β', 'ַ', 'é', 'à', '\\ue208', 'ل', 'ه', 'চ', '國', 'υ', 'f', 'ோ', 'ు', 'в', '}', 'أ', '´', 'ɑ', '”', 'پ', 's', 'ң', '℃', 'ḷ', 'ె', 'ದ', '⁻', 'य़', 'К', 'ό', 'r', '\\uf0d8', 'ئ', 'æ', 'ง', '¬', 'ग़', '长', 'ড', '►', 'រ', '‘', 'Z', '↔', '•', 'ª', 'À', '日', 'ா', 'e', '\\u200b', 'ಳ', '„', 'ス', 'ท', '動', 'ז', '→', '州', 'น', '現', 'я', 'ɵ', '‚', 'Q', 'M', 'ග', '°', '£', 'ត', 'Р', 'ě', 'Ę', '½', '庭', '魯', '郑', 'ٕ', 'ţ', 'ก', 'و', '車', 'ä', 'ú', 'ː', 'แ', 'ৱ', 'ρ', '▫', 'ක', 'ן', 'ු', 'ಮ', '☺', '่', 'ᠨ', 'T', 'พ', '域', 'ة', 'ظ', 'Ю', 'ॆ', 'コ', 'উ', 'ම', '¹', 'O', 'o', 'ʔ', '−', '\\u200f', 'ï', '॰', 'ł', 'ऍ', '♫', '死', 'δ', 'ॉ', 'Э', 'ʃ', 'ি', 'ර', 'š', 'ə', 'ē', 'б', 'জ', '春', 'Ω', 'v', 'כ', 'ঔ', 'দ', 'ನ', 'ö', 'ø', 'হ', 'ă', '西', 'ப', 'д', '❑', 'র', 'Я', 'ت', 'வ', '․', 'க', '¨', 'ห', 'ו', 'פ', 'מ', 'н', 'ි', 'ロ', 'ñ', '’', 'ऩ', 'อ', 'Ş', 'D', '\\ue5a0', 'ę', 'ো', 'ঘ', 'Г', '夕', '◆', 'ִ', '高', 'H', 'ق', '明', 'ऒ', 'V', '₹', 'N', 'ˈ', 'ऑ', 'ō', 'া', 'ع', '⁰', '\\xa0', 'ֹ', 'ン', '■', '²', 'パ', 'ʊ', 'ー', 'ং', '́', 'េ', 'ু', 'Ó', 'গ', '{', '§', 'z', 'ᐅ', 'е', 'ิ', 'з', 'ූ', '관', 'إ', 'р', 'ء', 'メ', 'w', 'ุ', '\\ufeff', '휴', '׃', '의', 'ُ', 'μ', 'ᓗ', 'ʹ', 'ਆ', 'í', 'K', 'ض', 'ְ', 'ख़', 'Ÿ', '◇', 'ढ़', 'প', 'শ', 'W', 'మ', 'ş', 'ā', 'с', 'и', '\\ue205', 'S', 'u', 'ر', 'ɨ', 'ي', 'ؤ', 'ಕ', 'ς', 'ల', 'ೆ', '්', '±', 'ऱ', 'ន', '…', '¡', '™', 'ί', 'ත', '中', 'ê', 'ว', 'ŋ', 'ِ', '॥', 'X', 'ィ', '\\uf0a7', '≈', 'i', 'к', '代', 'ා', 'ை', 'ন', 'ا', 'Θ', 'ক', 'Ṭ', '♦', 'k', '大', 'о', '鲁', 'ண', 'λ', 'ḍ', 'l', '❏', 'ò', 'L', 'ট', 'ৰ', 'ī', 'ǔ', 'B', 'ॅ', 'ν', 'క', 'ස', 'т', 'ி', '€', 'ל', '\\u200c', 'ব', '~', 'ල', 'Τ', '›', 'ප', '·', '̯', 'ε', 'ĺ', 'ز', '\\uf146', 'd', 'A', 'د', 'ច', 'й', '்', '©', 'а', 'ಯ', 'ế', 'क़', 'త', '»', 'ಿ', 'න', 'י', 'ව', 'Ò', '\\\\', 'ô', 'E', '洞', 'ֶ', '\\u200e', 't', 'ɪ', 'ช', 'л', '¶', '੍', 'ᐃ', 'য', 'ல', '˝', '\\ue88c', '공', 'Ι', 'ச', 'τ', 'ळ', 'b', 'נ', 'ה', 'آ', 'F', 'Ž', 'ឃ', 'h', 'م'}\n"
          ]
        }
      ],
      "source": [
        "# for i in 'शख्स':\n",
        "  # print(i)\n",
        "\n",
        "\n",
        "# hindi_sentences = [\n",
        "#     'आस्ट्रेलिया के पाल हेनली के साथ जोड़ी बनाने वाले पेस मियामी में क्वार्टरफाइनल तक ही पहुंच सके क्योंकि इस दौर में उन्हें भूपति और नोल्स ने हराया था।',\n",
        "#     'और जो शख्स (अपने आमाल का) बदला दुनिया ही में चाहता है तो ख़ुदा के पास दुनिया व आख़िरत दोनों का अज्र मौजूद है और ख़ुदा तो हर शख्स की सुनता और सबको देखता है',\n",
        "#     'जैव-मंडल में कीड़ों का मूल्य बहुत है, क्योंकि प्रजातियों की समृद्धि के मामले में उनकी संख्या अन्य जीव समूहों से ज़्यादा है।',\n",
        "#     'आस्ट्रेलिया के खिलाफ वनडे टीम की कमान मिताली को',\n",
        "#     '8 सितम्‍बर, 2016 को माननीय राष्ट्रपति की स्‍वीकृति मिलने के बाद 101वां संविधान संशोधन अधिनियम, 2016 अस्तित्व में आया'\n",
        "# ]\n",
        "\n",
        "# Define the provided vocabulary set\n",
        "hindi_vocabulary_set = set(hindi_vocabulary)\n",
        "\n",
        "# Combine all sentences into one string\n",
        "combined_sentences = ''.join(hindi_sentences)\n",
        "\n",
        "# Find all unique characters in the combined sentences\n",
        "unique_chars_in_sentences = set(combined_sentences)\n",
        "\n",
        "# Find characters in the sentences that are not in the vocabulary\n",
        "missing_chars = unique_chars_in_sentences - hindi_vocabulary_set\n",
        "\n",
        "print(\"Characters in the sentences not in the vocabulary:\", missing_chars)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dXmG9YJcTN3d"
      },
      "outputs": [],
      "source": [
        "index_to_hindi = {k:v for k,v in enumerate(hindi_vocabulary)}\n",
        "hindi_to_index = {v:k for k,v in enumerate(hindi_vocabulary)}\n",
        "index_to_english = {k:v for k,v in enumerate(english_vocabulary)}\n",
        "english_to_index = {v:k for k,v in enumerate(english_vocabulary)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rdyjQgdKTN3d"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, Dropout\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, max_seq_len):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "    def call(self, inputs):\n",
        "        pos = np.arange(self.max_seq_len)[:, np.newaxis]\n",
        "        i = np.arange(self.d_model)[np.newaxis, :]\n",
        "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(self.d_model))\n",
        "        angle_rads = pos * angle_rates\n",
        "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "        pos_encoding = angle_rads[np.newaxis, ...]\n",
        "        return tf.constant(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "class SentenceEmbedding(tf.keras.Model):\n",
        "    def __init__(self, max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n",
        "        super(SentenceEmbedding, self).__init__()\n",
        "        self.vocab_size = len(language_to_index)\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.embedding = Embedding(input_dim=self.vocab_size, output_dim=d_model)\n",
        "        self.language_to_index = language_to_index\n",
        "        self.position_encoder = PositionalEncoding(d_model, max_sequence_length)\n",
        "        self.dropout = Dropout(0.1)\n",
        "        self.START_TOKEN = START_TOKEN\n",
        "        self.END_TOKEN = END_TOKEN\n",
        "        self.PADDING_TOKEN = PADDING_TOKEN\n",
        "\n",
        "    def batch_tokenize(self, batch, start_token, end_token):\n",
        "        def tokenize(sentence, start_token, end_token):\n",
        "            sentence_word_indices = [self.language_to_index[token] for token in list(sentence)]\n",
        "            if start_token:\n",
        "                sentence_word_indices.insert(0, self.language_to_index[self.START_TOKEN])\n",
        "            if end_token:\n",
        "                sentence_word_indices.append(self.language_to_index[self.END_TOKEN])\n",
        "            sentence_word_indices.extend([self.language_to_index[self.PADDING_TOKEN]] * (self.max_sequence_length - len(sentence_word_indices)))\n",
        "            return tf.convert_to_tensor(sentence_word_indices, dtype=tf.int32)\n",
        "\n",
        "        tokenized = [tokenize(sentence, start_token, end_token) for sentence in batch]\n",
        "        return tf.stack(tokenized)\n",
        "\n",
        "    def call(self, x, start_token, end_token):\n",
        "        x = self.batch_tokenize(x, start_token, end_token)\n",
        "        x = self.embedding(x)\n",
        "        pos = self.position_encoder(x)\n",
        "        x = self.dropout(x + pos)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cOLH9UaTN3e",
        "outputId": "13692a82-ad74-4b52-9132-6443bd4b8592"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[[ 0.          1.          0.         ...  1.          0.\n",
            "    1.        ]\n",
            "  [ 0.81260556  0.5216928   0.83729273 ...  0.9880216  -0.04947659\n",
            "    0.96890825]\n",
            "  [ 0.8734833  -0.45281655  0.9058869  ...  0.99145526  0.01492784\n",
            "    0.9527636 ]\n",
            "  ...\n",
            "  [ 0.92680717 -0.37553754 -0.47029194 ...  0.99964774  0.02560204\n",
            "    0.99967223]\n",
            "  [ 0.18475212 -0.98278517  0.45737407 ...  0.9996449   0.02570567\n",
            "    0.99966955]\n",
            "  [-0.7271632  -0.6864646   0.9914194  ...  0.999642    0.02580929\n",
            "    0.99966687]]\n",
            "\n",
            " [[ 0.          1.          0.         ...  1.          0.\n",
            "    1.        ]\n",
            "  [ 0.8147846   0.5685391   0.80294305 ...  1.0359268  -0.02027452\n",
            "    0.9509402 ]\n",
            "  [ 0.880432   -0.4347563   0.95185125 ...  0.9880216  -0.04937293\n",
            "    0.96890825]\n",
            "  ...\n",
            "  [ 0.92680717 -0.37553754 -0.47029194 ...  0.99964774  0.02560204\n",
            "    0.99967223]\n",
            "  [ 0.18475212 -0.98278517  0.45737407 ...  0.9996449   0.02570567\n",
            "    0.99966955]\n",
            "  [-0.7271632  -0.6864646   0.9914194  ...  0.999642    0.02580929\n",
            "    0.99966687]]\n",
            "\n",
            " [[ 0.          1.          0.         ...  1.          0.\n",
            "    1.        ]\n",
            "  [ 0.8233422   0.52831453  0.8483957  ...  1.0352166  -0.02354738\n",
            "    1.0255268 ]\n",
            "  [ 0.880432   -0.4347563   0.95185125 ...  0.9880216  -0.04937293\n",
            "    0.96890825]\n",
            "  ...\n",
            "  [ 0.92680717 -0.37553754 -0.47029194 ...  0.99964774  0.02560204\n",
            "    0.99967223]\n",
            "  [ 0.18475212 -0.98278517  0.45737407 ...  0.9996449   0.02570567\n",
            "    0.99966955]\n",
            "  [-0.7271632  -0.6864646   0.9914194  ...  0.999642    0.02580929\n",
            "    0.99966687]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 0.          1.          0.         ...  1.          0.\n",
            "    1.        ]\n",
            "  [ 0.81018263  0.5646158   0.8681743  ...  1.04832     0.0275083\n",
            "    1.0162146 ]\n",
            "  [ 0.9301939  -0.4039211   0.94640225 ...  1.0211445   0.00354391\n",
            "    0.9922772 ]\n",
            "  ...\n",
            "  [ 0.92680717 -0.37553754 -0.47029194 ...  0.99964774  0.02560204\n",
            "    0.99967223]\n",
            "  [ 0.18475212 -0.98278517  0.45737407 ...  0.9996449   0.02570567\n",
            "    0.99966955]\n",
            "  [-0.7271632  -0.6864646   0.9914194  ...  0.999642    0.02580929\n",
            "    0.99966687]]\n",
            "\n",
            " [[ 0.          1.          0.         ...  1.          0.\n",
            "    1.        ]\n",
            "  [ 0.88999456  0.5868228   0.8665945  ...  0.95667654 -0.01323181\n",
            "    1.0465804 ]\n",
            "  [ 0.880432   -0.4347563   0.95185125 ...  0.9880216  -0.04937293\n",
            "    0.96890825]\n",
            "  ...\n",
            "  [ 0.92680717 -0.37553754 -0.47029194 ...  0.99964774  0.02560204\n",
            "    0.99967223]\n",
            "  [ 0.18475212 -0.98278517  0.45737407 ...  0.9996449   0.02570567\n",
            "    0.99966955]\n",
            "  [-0.7271632  -0.6864646   0.9914194  ...  0.999642    0.02580929\n",
            "    0.99966687]]\n",
            "\n",
            " [[ 0.          1.          0.         ...  1.          0.\n",
            "    1.        ]\n",
            "  [ 0.81018263  0.5646158   0.8681743  ...  1.04832     0.0275083\n",
            "    1.0162146 ]\n",
            "  [ 0.9258274  -0.39043388  0.96043336 ...  1.0445123   0.00934294\n",
            "    0.9853323 ]\n",
            "  ...\n",
            "  [ 0.92680717 -0.37553754 -0.47029194 ...  0.99964774  0.02560204\n",
            "    0.99967223]\n",
            "  [ 0.18475212 -0.98278517  0.45737407 ...  0.9996449   0.02570567\n",
            "    0.99966955]\n",
            "  [-0.7271632  -0.6864646   0.9914194  ...  0.999642    0.02580929\n",
            "    0.99966687]]], shape=(10, 250, 512), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "d_model = 512\n",
        "\n",
        "sentence_embedding = SentenceEmbedding(\n",
        "    max_sequence_length=max_sequence_length,\n",
        "    d_model=d_model,\n",
        "    language_to_index=english_to_index,\n",
        "    START_TOKEN=START_TOKEN,\n",
        "    END_TOKEN=END_TOKEN,\n",
        "    PADDING_TOKEN=PADDING_TOKEN\n",
        ")\n",
        "\n",
        "output = sentence_embedding(english_sentences[:10], start_token=True, end_token=True)\n",
        "\n",
        "print(len(output[0][0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJAxtWozZ5Hq",
        "outputId": "4a1632a1-9f5a-4b30-8bb7-9025254ac0e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[70 46 53 61 43 60 43 56 13  1 54 39 43 57 13  1 61 46 53  1 61 39 57  1\n",
            " 54 39 56 58 52 43 56 47 52 45  1 39 59 57 58 56 39 50 47 39  8 57  1 54\n",
            " 39 59 50  1 46 39 52 50 43 63 13  1 41 53 59 50 42  1 53 52 50 63  1 45\n",
            " 53  1 39 57  1 44 39 56  1 39 57  1 58 46 43  1 55 59 39 56 58 43 56 44\n",
            " 47 52 39 50 57  1 61 46 43 56 43  1 58 46 43 63  1 50 53 57 58  1 58 53\n",
            "  1 40 46 59 54 39 58 46 47  1 39 52 42  1 49 52 53 61 50 43 57 70 70 70\n",
            " 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70\n",
            " 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70\n",
            " 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70\n",
            " 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70\n",
            " 70 70 70 70 70 70 70 70 70 70], shape=(250,), dtype=int32)\n",
            "tf.Tensor(110, shape=(), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "def is_valid_tokens(sentence, vocab):\n",
        "    for token in list(set(sentence)):\n",
        "        if token not in vocab:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def is_valid_length(sentence, max_sequence_length):\n",
        "    return len(list(sentence)) < (max_sequence_length - 1) # need to re-add the end token so leaving 1 space\n",
        "\n",
        "valid_english_sentences = []\n",
        "for index in range(len(english_sentences)):\n",
        "    english_sentence =  english_sentences[index]\n",
        "    if  is_valid_length(english_sentence, max_sequence_length) \\\n",
        "      and is_valid_tokens(english_sentence, english_vocabulary):\n",
        "        valid_english_sentences.append(english_sentence)\n",
        "\n",
        "\n",
        "def batch_tokenize(language_to_index, batch, start_token, end_token, START_TOKEN, END_TOKEN, PADDING_TOKEN, max_sequence_length):\n",
        "    def tokenize(sentence, start_token, end_token):\n",
        "        sentence_word_indices = [language_to_index[token] for token in list(sentence)]\n",
        "        if start_token:\n",
        "            sentence_word_indices.insert(0, language_to_index[START_TOKEN])\n",
        "        if end_token:\n",
        "            sentence_word_indices.append(language_to_index[END_TOKEN])\n",
        "        sentence_word_indices.extend([language_to_index[PADDING_TOKEN]] * (max_sequence_length - len(sentence_word_indices)))\n",
        "        return tf.convert_to_tensor(sentence_word_indices, dtype=tf.int32)\n",
        "\n",
        "    tokenized = [tokenize(sentence, start_token, end_token) for sentence in batch]\n",
        "    return tf.stack(tokenized)\n",
        "\n",
        "x = batch_tokenize(language_to_index = english_to_index, batch = valid_english_sentences[:10], start_token = True, end_token = True,\n",
        "                     START_TOKEN= START_TOKEN, END_TOKEN= END_TOKEN, PADDING_TOKEN=PADDING_TOKEN, max_sequence_length= max_sequence_length)\n",
        "\n",
        "print(x)\n",
        "# print(valid_english_sentences)\n",
        "# count_70 = tf.math.count_nonzero(res[0] == 70)\n",
        "\n",
        "# print(count_70)\n"
<<<<<<< HEAD
=======
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJAxtWozZ5Hq",
        "outputId": "774e142f-98af-4f31-de1b-ac823d503497"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[70 46 53 ... 70 70 70]\n",
            " [70 61 46 ... 70 70 70]\n",
            " [70 58 46 ... 70 70 70]\n",
            " ...\n",
            " [70 39 57 ... 70 70 70]\n",
            " [70 48 46 ... 70 70 70]\n",
            " [70 39 56 ... 70 70 70]], shape=(10, 250), dtype=int32)\n"
          ]
        }
>>>>>>> eefedf4ecc41ec268fad2a52ffef4af3e9d73412
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": null,
=======
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Layer\n",
        "import math\n",
        "\n",
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    d_k = tf.shape(q)[-1]\n",
        "    scaled = tf.matmul(q, k, transpose_b=True) / tf.math.sqrt(tf.cast(d_k, tf.float32))\n",
        "    if mask is not None:\n",
        "        scaled += mask\n",
        "    attention = tf.nn.softmax(scaled, axis=-1)\n",
        "    values = tf.matmul(attention, v)\n",
        "    return values, attention\n",
        "\n",
        "class MultiHeadAttention(Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.qkv_layer = Dense(3 * d_model)\n",
        "        self.linear_layer = Dense(d_model)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        sequence_length = tf.shape(x)[1]\n",
        "        qkv = self.qkv_layer(x)\n",
        "        qkv = tf.reshape(qkv, (batch_size, sequence_length, self.num_heads, 3 * self.head_dim))\n",
        "        qkv = tf.transpose(qkv, perm=[0, 2, 1, 3])\n",
        "        q, k, v = tf.split(qkv, num_or_size_splits=3, axis=-1)\n",
        "        values, attention = scaled_dot_product(q, k, v, mask)\n",
        "        values = tf.transpose(values, perm=[0, 2, 1, 3])\n",
        "        values = tf.reshape(values, (batch_size, sequence_length, self.num_heads * self.head_dim))\n",
        "        out = self.linear_layer(values)\n",
        "        return out\n",
        "\n",
        "multihead_attention = MultiHeadAttention(d_model= d_model, num_heads= 8)\n",
        "\n",
        "x = multihead_attention(x= output)\n",
        "x"
      ],
>>>>>>> eefedf4ecc41ec268fad2a52ffef4af3e9d73412
      "metadata": {
        "id": "6HJV6kdiT3XN",
        "outputId": "8cf4e69f-0643-4ea8-bb20-7b8c3a05c5d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10, 250, 512), dtype=float32, numpy=\n",
              "array([[[ 0.47113872,  0.07759048, -0.15139236, ..., -0.27391303,\n",
              "         -0.22008754,  0.12190286],\n",
              "        [ 0.47305018,  0.07121822, -0.15115435, ..., -0.27165136,\n",
              "         -0.22114435,  0.12639491],\n",
              "        [ 0.4716777 ,  0.06702181, -0.15389189, ..., -0.27139845,\n",
              "         -0.22057194,  0.1296442 ],\n",
              "        ...,\n",
              "        [ 0.48015267,  0.069378  , -0.15162373, ..., -0.26199   ,\n",
              "         -0.19092515,  0.15157498],\n",
              "        [ 0.47974178,  0.06841743, -0.15316162, ..., -0.26531988,\n",
              "         -0.18917821,  0.1564349 ],\n",
              "        [ 0.47824448,  0.06805169, -0.15425922, ..., -0.2684981 ,\n",
              "         -0.18758845,  0.15522857]],\n",
              "\n",
              "       [[ 0.47049412,  0.07677332, -0.15001275, ..., -0.2742098 ,\n",
              "         -0.22179905,  0.12206203],\n",
              "        [ 0.4709833 ,  0.07130327, -0.1495182 , ..., -0.27238777,\n",
              "         -0.22140792,  0.12672192],\n",
              "        [ 0.4722253 ,  0.06499142, -0.15166529, ..., -0.27149862,\n",
              "         -0.22334903,  0.1310081 ],\n",
              "        ...,\n",
              "        [ 0.47871417,  0.06888346, -0.14943892, ..., -0.26251754,\n",
              "         -0.19262032,  0.15215398],\n",
              "        [ 0.4783341 ,  0.06795494, -0.15092729, ..., -0.26585045,\n",
              "         -0.1909245 ,  0.15696727],\n",
              "        [ 0.47680053,  0.06766003, -0.15204038, ..., -0.26898408,\n",
              "         -0.18939526,  0.15566547]],\n",
              "\n",
              "       [[ 0.47001243,  0.07686383, -0.15055218, ..., -0.27223697,\n",
              "         -0.22060977,  0.12360706],\n",
              "        [ 0.4703365 ,  0.06980568, -0.15067986, ..., -0.27022803,\n",
              "         -0.22118561,  0.12800524],\n",
              "        [ 0.47192997,  0.06514418, -0.15221262, ..., -0.26945052,\n",
              "         -0.22208303,  0.13254453],\n",
              "        ...,\n",
              "        [ 0.47908142,  0.06871025, -0.15019515, ..., -0.26038602,\n",
              "         -0.19124356,  0.15335089],\n",
              "        [ 0.47868055,  0.06778657, -0.151685  , ..., -0.26378733,\n",
              "         -0.18958344,  0.15829273],\n",
              "        [ 0.4771179 ,  0.06752065, -0.15282908, ..., -0.26706398,\n",
              "         -0.18808018,  0.15703964]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[ 0.47135773,  0.07570019, -0.15100944, ..., -0.2753347 ,\n",
              "         -0.22058627,  0.12420281],\n",
              "        [ 0.4719447 ,  0.07128672, -0.1504475 , ..., -0.2739005 ,\n",
              "         -0.22171722,  0.12890556],\n",
              "        [ 0.47067222,  0.06584802, -0.15257657, ..., -0.27252728,\n",
              "         -0.22120692,  0.13193668],\n",
              "        ...,\n",
              "        [ 0.48015594,  0.06681751, -0.15084979, ..., -0.26320443,\n",
              "         -0.19068295,  0.15443729],\n",
              "        [ 0.4798649 ,  0.06568372, -0.15231352, ..., -0.2665125 ,\n",
              "         -0.18894379,  0.15925728],\n",
              "        [ 0.47847068,  0.06524464, -0.15340526, ..., -0.26972464,\n",
              "         -0.18749648,  0.15798952]],\n",
              "\n",
              "       [[ 0.4685365 ,  0.0780619 , -0.1523697 , ..., -0.2719873 ,\n",
              "         -0.22129993,  0.12286483],\n",
              "        [ 0.46855956,  0.07198395, -0.15252171, ..., -0.26957476,\n",
              "         -0.22372383,  0.12718971],\n",
              "        [ 0.47056004,  0.06619425, -0.15409723, ..., -0.26920205,\n",
              "         -0.22286768,  0.13168074],\n",
              "        ...,\n",
              "        [ 0.477215  ,  0.06888781, -0.15161663, ..., -0.2600042 ,\n",
              "         -0.19181976,  0.15279084],\n",
              "        [ 0.4768363 ,  0.06792532, -0.1530955 , ..., -0.2633641 ,\n",
              "         -0.19007486,  0.15764935],\n",
              "        [ 0.47533938,  0.06762554, -0.15420347, ..., -0.26659024,\n",
              "         -0.18852842,  0.15637405]],\n",
              "\n",
              "       [[ 0.4720878 ,  0.07577635, -0.152169  , ..., -0.27285358,\n",
              "         -0.22197789,  0.12305605],\n",
              "        [ 0.47272605,  0.07135919, -0.1514192 , ..., -0.2713542 ,\n",
              "         -0.22305617,  0.1276198 ],\n",
              "        [ 0.47259307,  0.06631397, -0.1540587 , ..., -0.26947755,\n",
              "         -0.22440287,  0.13026917],\n",
              "        ...,\n",
              "        [ 0.48097235,  0.06714789, -0.15196723, ..., -0.26123244,\n",
              "         -0.19270651,  0.15317304],\n",
              "        [ 0.4808032 ,  0.06620353, -0.1535253 , ..., -0.26455423,\n",
              "         -0.19106455,  0.15808856],\n",
              "        [ 0.47940573,  0.06588778, -0.15467723, ..., -0.2677382 ,\n",
              "         -0.18957148,  0.15688476]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalization(tf.keras.layers.Layer):\n",
        "    def __init__(self, parameters_shape, eps=1e-5):\n",
        "        super(LayerNormalization, self).__init__()\n",
        "        self.parameters_shape = parameters_shape\n",
        "        self.eps = eps\n",
        "        self.gamma = self.add_weight(\n",
        "            shape=parameters_shape,\n",
        "            initializer=\"ones\",\n",
        "            trainable=True,\n",
        "            name=\"gamma\"\n",
        "        )\n",
        "        self.beta = self.add_weight(\n",
        "            shape=parameters_shape,\n",
        "            initializer=\"zeros\",\n",
        "            trainable=True,\n",
        "            name=\"beta\"\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        dims = [-i for i in range(1, len(self.parameters_shape) + 1)]\n",
        "        mean = tf.reduce_mean(inputs, axis=dims, keepdims=True)\n",
        "        var = tf.reduce_mean(tf.square(inputs - mean), axis=dims, keepdims=True)\n",
        "        std = tf.sqrt(var + self.eps)\n",
        "        y = (inputs - mean) / std\n",
        "        out = self.gamma * y + self.beta\n",
        "        return out\n",
        "\n"
      ],
      "metadata": {
        "id": "VyuN9ZL8xXyo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
